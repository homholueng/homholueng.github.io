{"meta":{"title":"live in passion","subtitle":"","description":"description?","author":"Homho Lueng","url":"https://homholueng.github.io"},"pages":[{"title":"Categories","date":"2018-10-25T05:31:47.000Z","updated":"2019-04-20T07:30:41.784Z","comments":true,"path":"categories/index.html","permalink":"https://homholueng.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-10-25T05:29:32.000Z","updated":"2019-04-20T07:30:41.784Z","comments":true,"path":"tags/index.html","permalink":"https://homholueng.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"python asyncio High-level APIs","slug":"python-asyncio","date":"2019-11-25T15:24:50.000Z","updated":"2019-11-27T02:45:39.980Z","comments":true,"path":"2019/11/25/python-asyncio/","link":"","permalink":"https://homholueng.github.io/2019/11/25/python-asyncio/","excerpt":"","text":"asyncio 是 Python 官方提供的用于编写并发程序的库，特别是在 Python 3.6 之后，官方通过添加 async/await 关键字来支持了原生的协程，这使得我们不需要再使用传统的生成器来编写协程，而在 Python 3.7 之后，官方对 asyncio 库进行了调整，提供了更为抽象的高层接口，使得这个库的易用程度大大提高，通过这些上层接口，我们能够： 并发的执行 Python 协程，病能够很好的对其进行管理 进行网络 IO 和进程间通信 控制子进程 通过队列来执行分布式任务 同步并发代码 Croutines and Tasks在 Python 3.6 之后，用 async/await 修饰的函数我们称之为协程，如下所示： 12345import asyncioasync def main(): print('hello') await asyncio.sleep(1) print('world') 上面的程序在打印了 hello 之后，会调用 asyncio.sleep 函数睡眠一秒，此时 main 会将执行权交出去，执行流程会回到当前运行循环中，让下一个准备好的协程执行。需要注意的是，一旦我们的程序决定使用协程来实现，我们就不应该在协程中调用会阻塞当前线程的函数，不然会阻塞整个运行循环，就失去了并发程序间互相协作的意义，这就是为什么我们在这里低啊用 asyncio.sleep 而不是 time.sleep 的原因，前者会将控制权交还给运行循环。 Run a coroutine我们有三种方式来执行一个协程： asyncio.run()，一般通过这个函数来运行最顶层的协程 await 关键字，通过这个关键字我们能够在协程中调用其他的协程 asyncio.create_task()，使用这个函数能够以 task 的形式执行多个协程，使用这个方式来执行协程的意义在于，我们能够获得一个 Task 对象，通过这个对象我们能够实现对协程的一些控制行为 12345678910111213141516import asyncioimport timeasync def say_after(delay, what): await asyncio.sleep(delay) print(what)async def main(): print(f\"started at &#123;time.strftime('%X')&#125;\") await say_after(1, 'hello') await say_after(2, 'world') print(f\"finished at &#123;time.strftime('%X')&#125;\")asyncio.run(main()) 上述代码中的 main 协程也可以替换成下面这种实现方式： 123456789101112131415async def main(): task1 = asyncio.create_task( say_after(1, 'hello')) task2 = asyncio.create_task( say_after(2, 'world')) print(f\"started at &#123;time.strftime('%X')&#125;\") # Wait until both tasks are completed (should take # around 2 seconds.) await task1 await task2 print(f\"finished at &#123;time.strftime('%X')&#125;\") Awaitables如果一个对象能够对其使用 await 关键字，那么该对象就是一个 awaitable 的对象。在 asyncio 中，主要有下面三种 awaitable 对象： croutines：使用 async def 关键字定义的函数或是调用协程函数返回的对象 tasks：调用 asyncio.create_task 返回的 Task 对象，通常使用 Task 对象来同时调度多个协程 futures：这是一个底层实现中使用的对象，其表示一个异步操作最终会产生的结果；一般来说，使用高层 API 时我们都不会接触到这种对象 Running Tasks Concurrently如果要同时执行多个协程，asyncio 提供了一个便捷函数 gather 供我们使用：awaitable asyncio.gather(*aws, return_exceptions=False)。其会按照 aws 中传入的协程的顺序来并发的执行他们，如果 aws 中传入的对象是 awaitable 的，那么 gather 就会将其作为一个 Task 对象进行调度。 12345678910111213141516171819202122232425262728293031import asyncioasync def factorial(name, number): f = 1 for i in range(2, number + 1): print(f\"Task &#123;name&#125;: Compute factorial(&#123;i&#125;)...\") await asyncio.sleep(1) f *= i print(f\"Task &#123;name&#125;: factorial(&#123;number&#125;) = &#123;f&#125;\")async def main(): # Schedule three calls *concurrently*: await asyncio.gather( factorial(\"A\", 2), factorial(\"B\", 3), factorial(\"C\", 4), )asyncio.run(main())# Expected output:## Task A: Compute factorial(2)...# Task B: Compute factorial(2)...# Task C: Compute factorial(2)...# Task A: factorial(2) = 2# Task B: Compute factorial(3)...# Task C: Compute factorial(3)...# Task B: factorial(3) = 6# Task C: Compute factorial(4)...# Task C: factorial(4) = 24 如果 return_exceptions 参数为 True，则协程中抛出的异常会被当做结果返回，否则 gather 函数会抛出异常。 1234567891011121314151617181920212223simport asyncioasync def coro1(): raise Exception()async def coro_n(num): print('coro', num) await asyncio.sleep(num) return numasync def main(): results = await asyncio.gather(coro1(), coro_n(2), coro_n(3), return_exceptions=True) print(results)asyncio.run(main())# Expected output:# coro 2# coro 3# [Exception(), 2, 3] 如果 return_exceptions 为 False，则会抛出异常： 12345678910111213&lt;ipython-input-3-cf6ac787bf10&gt; in main() 13 14 async def main():---&gt; 15 results = await asyncio.gather(coro1(), coro_n(2), coro_n(3), return_exceptions=False) 16 print(results) 17 &lt;ipython-input-3-cf6ac787bf10&gt; in coro1() 3 4 async def coro1():----&gt; 5 raise Exception() 6 7 Shielding From Cancellation我们可以使用 shield 函数来防止一个协程被 calcel() 调用影响： 1res = await shield(something()) 但是如果正在执行 something 的协程被取消了，虽然此时 something 本身没有被取消，但是这条 await 语句还是会抛出 CancelledError。 Timeoutsasyncio.wait_for(aw, timeout, *) 等待 aw 协程在超时时间内完成，否则抛出 TimeoutError 异常： 12345678910111213async def eternity(): # Sleep for one hour await asyncio.sleep(3600) print('yay!')async def main(): # Wait for at most 1 second try: await asyncio.wait_for(eternity(), timeout=1.0) except asyncio.TimeoutError: print('timeout!')asyncio.run(main()) Waiting Primitivesasyncio.wait(aws, *, timeout=None, return_when=ALL_COMPLETED) 会执行 aws 并阻塞到 return_when 参数中指定的条件满足位置。 1done, pending = await asyncio.wait(aws) return_when 有以下选项： FIRST_COMPLETED：函数将在任意一个 future 完成或被取消后返回。 FIRST_EXCEPTION：函数将在任意一个 future 抛出异常后返回，如果没有任何异常抛出，其等同于 ALL_COMPLETED。 ALL_COMPLETED：函数将在所有 future 完成或被取消后返回。 Scheduling From Other Threadsasyncio.run_coroutine_threadsafe(coro, loop) 能够让我们在另一个线程中执行协程： 12345678# Create a coroutinecoro = asyncio.sleep(1, result=3)# Submit the coroutine to a given loopfuture = asyncio.run_coroutine_threadsafe(coro, loop)# Wait for the result with an optional timeout argumentassert future.result(timeout) == 3 Introspection asyncio.current_task(loop=None)：返回当前正在执行的 Task 实例，如果当前没有任务在执行则返回 None asyncio.all_tasks(loop=None)：返回 loop 中尚未执行完成的 Task 集合 如果 loop 参数为 None，函数内部会使用 get_running_loop() 来获取当前运行循环。 StreamsStreams 提供了高层可用 async/await 关键字操作网络连接的借口，下面是用 Streams 编写的 TCP 回显客户端和 TCP 回显服务器 Client12345678910111213141516import asyncioasync def tcp_echo_client(message): reader, writer = await asyncio.open_connection( '127.0.0.1', 8888) print(f'Send: &#123;message!r&#125;') writer.write(message.encode()) data = await reader.read(100) print(f'Received: &#123;data.decode()!r&#125;') print('Close the connection') writer.close()asyncio.run(tcp_echo_client('Hello World!')) Server123456789101112131415161718192021222324252627import asyncioasync def handle_echo(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info('peername') print(f\"Received &#123;message!r&#125; from &#123;addr!r&#125;\") print(f\"Send: &#123;message!r&#125;\") writer.write(data) await writer.drain() print(\"Close the connection\") writer.close()async def main(): server = await asyncio.start_server( handle_echo, '127.0.0.1', 8888) addr = server.sockets[0].getsockname() print(f'Serving on &#123;addr&#125;') async with server: await server.serve_forever()asyncio.run(main()) Synchronization Primitives虽然并发程序设计很大程度上涉及了代码之间协作的模式，但是总有一些情况下我们还是要对代码的执行进行同步，因为运行循环的行为不是我们能够控制的，这就需要借助各种同步原语提供的能力了。 Lock互斥锁，使用方式如下： 12345678910111213141516171819import asyncioasync def lock_competitor(n, lock): print(f\"competitor&#123;n&#125; try to get lock\") async with lock: print(f\"competitor&#123;n&#125; get the lock\") print(f\"competitor sleep for &#123;n&#125; seconds...\") await asyncio.sleep(n) print(f\"competitor&#123;n&#125; wake up!\") print(f\"competitor&#123;n&#125; give up the lock\")async def main(): lock = asyncio.Lock() await asyncio.gather(*[lock_competitor(n, lock) for n in range(1, 5)])asyncio.run(main()) 当然，也可以采用传统的模式来获取互斥锁，但还是推荐使用 async with 关键字： 12345678lock = asyncio.Lock()# ... laterawait lock.acquire()try: # access shared statefinally: lock.release() 注意，互斥锁是公平的，这意味这先尝试获取锁的协程会在锁可用后先得到锁。 Event事件能够用于通知多个 asyncio 任务某件事情已经发生了： 1234567891011121314151617181920async def waiter(event): print('waiting for it ...') await event.wait() print('... got it!')async def main(): # Create an Event object. event = asyncio.Event() # Spawn a Task to wait until 'event' is set. waiter_task = asyncio.create_task(waiter(event)) # Sleep for 1 second and set the event. await asyncio.sleep(1) event.set() # Wait until the waiter task is finished. await waiter_taskasyncio.run(main()) ConditionCondition 可以用于让多个任务等待某些事件的发生，一旦事件发生后这些任务会尝试去互斥的访问某些共享的资源。Condition 就像是 Lock 和 Event 的结合。但是，多个 Condition 对象能够绑定同一个锁，这就能够让对不同事件感兴趣却需要访问同一个共享资源的任务间进行同步操作。 12345cond = asyncio.Condition()# ... laterasync with cond: await cond.wait() Semaphore即传统的信号量，当内置的计数器等于 0 时，则当前执行的任务会让出调度权： 12345sem = asyncio.Semaphore(10)# ... laterasync with sem: # work with shared resource BoundedSemaphore特殊版本的 Semaphore，若调用 release() 后其内置计数器的值大于初始值，则会抛出 ValueError。 Subprocessesasyncio 还提供了 API 来让我们创建和管理子进程，下面是一个使用子进程来执行命令的例子： 1234567891011121314151617import asyncioasync def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() print(f'[&#123;cmd!r&#125; exited with &#123;proc.returncode&#125;]') if stdout: print(f'[stdout]\\n&#123;stdout.decode()&#125;') if stderr: print(f'[stderr]\\n&#123;stderr.decode()&#125;')asyncio.run(run('ls /zzz'))","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 21 - 类元编程","slug":"fluent-python-meta-class","date":"2019-11-03T12:38:12.000Z","updated":"2019-11-03T12:56:40.388Z","comments":true,"path":"2019/11/03/fluent-python-meta-class/","link":"","permalink":"https://homholueng.github.io/2019/11/03/fluent-python-meta-class/","excerpt":"","text":"21.1 类工厂函数其实我们经常使用的 collections.namedtuple 就是一个类工厂函数，我们把类名和几个属性名传给这个函数，它就会创建一个 tuple 的子类。 123456789101112131415161718192021222324252627282930def record_factory(cls_name, field_names): try: field_names = field_names.replace(',', ' ').split() except AttributeError: pass field_names = tuple(field_names) def __init__(self, *args, **kwargs): attrs = dict(zip(self.__slots__, args)) attrs.update(kwargs) for name, value in attrs.items(): setattr(self, name, value) def __iter__(self): for name in self.__slots__: yield getattr(self, name) def __repr__(self): values = ', '.join('&#123;&#125;=&#123;!r&#125;'.format(*i) for i in zip(self.__slots__, self)) return '&#123;&#125;(&#123;&#125;)'.format(self.__class__.__Name__, values) cls_attrs = dict( __slots__=field_names, __init__=__init__, __iter__=__iter__, __repr__=__repr__ ) return type(cls_name, (object,), cls_attrs) 通常，我们把 type 视作函数，因为我们像函数那样使用它，例如，调用 type(my_object) 获取对象所属的类——作用与 my_object.__class__ 相同。然而，type 是一个类。当成类使用时，传入三个参数可以新建一个类: 1MyClass = type('MyClass', (MySuperClass, MyMixin), &#123;'x': 42 'x2': lambda self : self.x * 2&#125;) type 的三个参数分别是 name、bases 和 dict。最后一个参数是一个映射，指定新类的属性名和值。上述代码的作用与下述代码相同: 12345class MyClass(MySUperClass, MyMixin): x = 42 def x2(self): return self.x * 2 21.2 定制描述符的类装饰器类装饰器与函数装饰器非常类似，是参数为类对象的函数，返回原来的类或修改后的类。 1234567def entity(cls): for key, attr in cls.__dict__.items(): if isintance(attr, Validated): type_name = type(attr).__name__ attr.storage_name = '_&#123;&#125;#&#123;&#125;'.format(type_name, key) return cls 类装饰器有个重大缺点: 只对直接依附的类有效。这意味着，被装饰的类的子类可能继承也可能不继承装饰器所做的改动，具体情况视改动的方式而定。 21.3 导入时和运行时比较导入模块时，解释器会执行顶层的 def 语句，可是这么做有什么作用呢?解释器会编译函数的定义体(首次导入模块时)，把函数对象绑定到对应的全局名称上，但是显然解释器不会执行函数的定义体。 对类来说，情况就不同了:在导入时，解释器会执行每个类的定义体，甚至会执行嵌套类的定义体。执行类定义体的结果是，定义了类的属性和方法，并构建了类对象。 21.4 元类基础知识元类是制造类的工厂，不过不是函数，而是类。 根据 Python 的对象模型，类是对象，因此类肯定是另外某个类的实例。 1234567891011In [1]: 'spam'.__class__ Out[1]: strIn [2]: str.__class__ Out[2]: typeIn [3]: str.__class__ Out[3]: typeIn [4]: type.__class__Out[4]: type 为了避免无限回溯，type 是其自身的实例，如最后一行所示。 除了 type，标准库中还有一些别的元类，例如 ABCMeta。 123456789101112In [1]: import collectionsIn [2]: collections.Iterable.__class__Out[2]: abc.ABCMetaIn [3]: import abcIn [4]: abc.ABCMeta.__class__ Out[4]: typeIn [5]: abc.ABCMeta.__mro__Out[5]: (abc.ABCMeta, type, object) 向上追溯，ABCMeta 最终所属的类也是 type。所有类都直接或间接地是 type 的实例，不过只有元类同时也是 type 的子类。若想理解元类，一定要知道这种关系:元类(如 ABCMeta)从 type 类继承了构建类的能力。 我们要抓住的重点是，所有类都是 type 的实例，但是元类还是 type 的子类，因此可以作为制造类的工厂。具体来说，元类可以通过实现 __init__ 方法定制实例。元类的 __init__ 方法可以做到类装饰器能做的任何事情，但是作用更大。元类的 __init__ 方法有四个参数： self：要初始化的类对象（一般改名为 cls） name：类名 bases：基类列表 dic：类的属性名和值 21.5 定制描述符的元类1234567891011class EntityMeta(type): def __init__(cls, name, bases, attr_dict): super().__init__(name, bases, attr_dict) for key, attr in attr_dict.items(): if isinstance(attr, Validated): type_name = type(attr).__name__ attr.storage_name = '_&#123;&#125;#&#123;&#125;'.format(type_name, key)class Entity(metaclass=EntityMeta): \"\"\"带有验证字段的业务实体\"\"\" 因为元类会影响使用期作为元类的类及所有子类的初始化，所以我们可以在元类中定义一些行为，这样就能够作用在整个类继承链条的类上。 21.6 元类的特殊方法 __prepare__在某些应用中，可能需要知道类的属性定义的顺序。如前所述，type 构造方法及元类的 __new__ 和 __init__ 方法都会收到要计算的类的定义体，形式是名称到属性的映像。然而在默认情况下，那个映射是字典;也就是说，元类或类装饰器获得映射时，属性在类定义体中的顺序已经丢失了。 这个问题的解决办法是，使用 Python 3 引入的特殊方法 __prepare__。 这个特殊方法只在元类中有用，而且必须声明为类方法(即要使用 @classmethod 装饰器定义)。解释器调用元类的 __new__ 方法之前会先调用 __prepare__ 方法，使用类定义体中的属性创建映射。 __prepare__ 方法的第一个参数是元类，随后两个参数分别是要构建的类的名称和基类组成的元组，返回值必须是映射。元类构建新类时，__prepare__ 方法返回的映射会传给 __new__ 方法的最后一个参数，然后再传给 __init__ 方法。 12345678910111213141516import collectionsclass EntityMeta(type): @classmethod def __prepare__(cls, name, bases): return collections.OrderedDict() def __init__(cls, name, bases, attr_dict): super().__init__(name, bases, attr_dict) cls._field_names = [] for key, attr in attr_dict.items(): if isinstance(attr, Validated): type_name = type(attr).__name__ attr.storage_name = '&#123;&#125;#&#123;&#125;'.format(type_name, key) cls._field_names.append(key) 在上述例子中，由于 __prepare__ 方法返回的是一个有序的字典，所以在 __init__ 中的 for 循环里我们能够根据属性定义的顺序来进行遍历。 在现实世界中，框架和库会使用元类协助程序员执行很多任务，例如: 验证属性 一次把装饰器依附到多个方法上 序列化对象或转换数据 对象关系映射 基于对象的持久存储 动态转换使用其他语言编写的类结构 21.7 类作为对象cls.__bases__：由类的基类组成的元组。cls.__qualname__：Python 3.3 新引入的属性，其值是类或函数的限定名称，即从模块的全局作用域到类的点分路径。cls.__subclasses__()：这个方法返回一个列表，包含类的直接子类。cls.mro()：构建类时，如果需要获取储存在类属性 __mro__ 中的超类元组，解释器会调用这个方法。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 20 - 属性描述符","slug":"fluent-python-attribute-descriptor","date":"2019-11-03T12:37:52.000Z","updated":"2019-11-03T12:56:40.386Z","comments":true,"path":"2019/11/03/fluent-python-attribute-descriptor/","link":"","permalink":"https://homholueng.github.io/2019/11/03/fluent-python-attribute-descriptor/","excerpt":"","text":"描述符是对多个属性运用相同存取逻辑的一种方式，是实现了特定协议的类，这个协议包括 __get__、__set__ 和 __delete__ 方法。 20.1 描述符示例：验证属性20.1.1 一个简单的描述符实现了 __get__、__set__ 或 __delete__ 方法的类是描述符。描述符的用法是，创建一个实例，作为另一个类的类属性。 在学习使用描述符前，需要先清除以下概念： 描述符类：实现描述符协议的类 托管类：把描述符实例声明为类属性的类 描述符实例：描述符类的各个实例，声明为托管类的类属性 托管实例：托管类的实例 存储属性：托管实例中存储自身托管属性的属性 托管属性：托管类中由描述符实例处理的公开属性 1234567891011121314151617181920212223class Quantity: def __init__(self, storage_name): self.storage_name = storage_name def __set__(self, instance, value): if value &gt; 0: instance.__dict__[self.storage_name] = value else: raise ValueError('value must be &gt; 0') class LineItem: weight = Quantity('weight') price = Quantity('price') def __init__(self, description, weight, price): self.description = description self.weight = weight self.price = price def subtotal(self): return self.weight * self.price 编写 __set__ 方法时，要记住 self 和 instance 参数的意思：self 是描述符实例，instance 是托管实例。 20.1.3 一种新型描述符我们可以将描述符定义中通用的部分提取出来，将一些可能有开发者自定义的部分放开让子类来实现，采用模板方法的设计模式来进行抽象和简化： 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import abcclass AutoStorage: __counter = 0 def __init__(self): cls = self.__class__ prefix = cls.__name__ index = cls.__counter__ self.storage_name = '_&#123;&#125;#&#123;&#125;'.format(prefix, index) cls.__counter += 1 def __get__(self, instance, owner): if instance is None: return self return getattr(instance, self.storage_name) def __set__(self, instance, value): setattr(instance, self.storage_name, value)class Validated(abc.ABC, AutoStorage): def __set__(self, instance, value): value = self.validate(instance, value) super().__set__(instance, value) @abc.abstractmethod def validate(self, instance, value): \"\"\"return validated value or raise ValueError\"\"\" class Quantity(Validated): def validate(self, instance, value): if value &lt;= 0: raise ValueError('value must be &gt; 0') return valueclass BonBlank(Validated): def validate(self, isinstance, value): value = value.strip() if len(value) == 0: raise ValueError('value cannot be empty or blank') return value 上述的两个例子演示了描述符的典型用途：管理数据属性。这种描述符也叫覆盖型描述符，因为描述符的 __set__ 方法使用托管实例中的同名属性覆盖(即插手接管)了要设置的属性。 20.2 覆盖型与非覆盖型描述符对比20.2.1 覆盖型描述符实现 __set__ 方法的描述符属于覆盖型描述符，因为虽然描述符是类属性，但是实现 __set__ 方法的话，会覆盖对实例属性的赋值操作。 20.2.2 没有 __get__ 方法的覆盖型描述符此时，只有写操作由描述符处理。通过实例读取描述符会返回描述符对象本身，因为没有处理读操作的 __get__ 方法。如果直接通过实例的 __dict__ 属性创建同名实例属性，以后再设置那个属性时，仍会由 __set__ 方法插手接管，但是读取那个属性的话，就会直接从实例中返回新赋予的值，而不会返回描述符对象。 20.2.3 非覆盖型描述符没有实现 __set__ 方法的描述符是非覆盖型描述符。如果设置了同名的实例属性，描述符会被遮盖，致使描述符无法处理那个实例的那个属性。 20.3 方法是描述符在类中定义的函数属于绑定方法(bound method)，因为用户定义的函数都有 __get__ 方法，所以依附到类上时，就相当于描述符。 描述符一样，通过托管类访问时，函数的 __get__ 方法会返回自身的引用。但是，通过实例访问时，函数的 __get__ 方法返回的是绑定方法对象：一种可调用的对象，里面包装着函数，并把托管实例(例如 obj)绑定给函数的第一个参数(即 self)。 下面我们来测试一下： 12345678import collectionsclass Text(collections.UserString): def __repr__(self): return 'Text(&#123;!r&#125;)'.format(self.data) def reverse(self): return self[::-1] 1234567891011121314151617181920&gt;&gt;&gt; word = Text(&apos;forward&apos;) &gt;&gt;&gt; wordText(&apos;forward&apos;)&gt;&gt;&gt; word.reverse()Text(&apos;drawrof&apos;)&gt;&gt;&gt; Text.reverse(Text(&apos;backward&apos;))Text(&apos;drawkcab&apos;)&gt;&gt;&gt; type(Text.reverse), type(word.reverse)(&lt;class &apos;function&apos;&gt;, &lt;class &apos;method&apos;&gt;)&gt;&gt;&gt; list(map(Text.reverse, [&apos;repaid&apos;, (10, 20, 30), Text(&apos;stressed&apos;)])) [&apos;diaper&apos;, (30, 20, 10), Text(&apos;desserts&apos;)]&gt;&gt;&gt; Text.reverse.__get__(word)&lt;bound method Text.reverse of Text(&apos;forward&apos;)&gt;&gt;&gt;&gt; Text.reverse.__get__(None, Text)&lt;function Text.reverse at 0x101244e18&gt;&gt;&gt;&gt; word.reverse&lt;bound method Text.reverse of Text(&apos;forward&apos;)&gt;&gt;&gt;&gt; word.reverse.__self__ # 绑定方法对象有个 __self__ 属性，其值是调用这个方法的实例引用。Text(&apos;forward&apos;)&gt;&gt;&gt; word.reverse.__func__ is Text.reverseTrue 20.4 描述符用法建议 使用特性来创建只读属性：创建只读属性最简单的方式是使用特性，没必要再自定义一个描述符 只读描述符必须有 __set__ 方法：如果使用描述符类实现只读属性，要记住，__get__ 和 __set__ 两个方法必须都定义，否则，实例的同名属性会遮盖描述符。 用于验证的描述符可以只有 __set__ 方法：对仅用于验证的描述符来说，__set__ 方法应该检查 value 参数获得的值，如果有效，使用描述符实例的名称为键，直接在实例的 __dict__ 属性中设置。 仅有 __get__ 方法的描述符可以实现高效缓存：如果只编写了 __get__ 方法，那么创建的是非覆盖型描述符。这种描述符可用于执行某些耗费资源的计算，然后为实例设置同名属性，缓存结果。同名实例属性会遮盖描述符，因此后续访问会直接从实例的 __dict__ 属性中获取值，而不会再触发描述符的 __get__ 方法。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 19 - 动态属性和特性","slug":"fluent-python-dynamic-attribute-and-property","date":"2019-10-17T02:24:43.000Z","updated":"2019-10-17T02:24:43.790Z","comments":true,"path":"2019/10/17/fluent-python-dynamic-attribute-and-property/","link":"","permalink":"https://homholueng.github.io/2019/10/17/fluent-python-dynamic-attribute-and-property/","excerpt":"","text":"特性至关重要的地方在于，特性的存在使得开发者可以非常安全并且确定可行地将公共数据属性作为类的公共接口的一部分开放出来。 19.1.3 使用 __new__ 方法以灵活的方式创建对象我们通常把 __init__ 称为构造方法，这是从其他语言借鉴过来的术语。其实，用于构建实例的是特殊方法 __new__：这是个类方法（使用特殊方式处理，因此不必使用 @classmethod 装饰器），必须返回一个实例。返回的实例会作为第一个参数（即 self）传给 __init__ 方法。因为调用 __init__ 方法时要传入实例，而且禁止返回任何值，所以 __init__ 方法其实是“初始化方法”。真正的构造方法是 __new__。我们几乎不需要自己编写 __new__ 方法，因为从 object 类继承的实现已经足够了。 刚才说明的过程，即从 __new__ 方法到 __init__ 方法，是最常见的，但不是唯一的。__new__ 方法也可以返回其他类的实例，此时，解释器不会调用 __init__ 方法。 19.1.4 shelve 模块标准库中有个 shelve（架子）模块，这名字听起来怪怪的，可是如果知道 pickle（泡菜）是 Python 对象序列化格式的名字，还是在那个格式与对象之间相互转换的某个模块的名字，就会觉得以 shelve 命名是合理的。泡菜坛子摆放在架子上，因此 shelve 模块提供了 pickle 存储方式。 shelve.open 高阶函数返回一个 shelve.Shelf 实例，这是简单的键 值对象数据库，背后由 dbm 模块支持，具有下述特点： shelve.Shelf 是 abc.MutableMapping 的子类，因此提供了处理映射类型的重要方法。 shelve.Shelf 类还提供了几个管理 I/O 的方法，如 sync 和 close；它也是一个上下文管理器。 只要把新值赋予键，就会保存键和值。 键必须是字符串。 值必须是 pickle 模块能处理的对象。 123456import shelvedb = shelve.open(DB_NAME)db[key] = SomeObject()obj = db[key] 19.1.5 使用特性获取链接的记录防止实例属性覆盖类属性在使用特性时，如果我们要使用对象的类属性，为了防止对象的实例属性覆盖掉我们对雷属性的访问，建议按照以下的方式访问类属性： 12345class Obj: @property def attr(self): self.__class__.some_attr 使用特性覆盖类的实例属性我们可以使用特性覆盖我们实例中的某些属性，但是，当我们要在类中访问实例的属性时，就需要按照如下方式进行访问： 12345class Obj: @property def attr(self): self.__dict__['attr'] 19.2 使用特性验证属性19.2.2 能验证值的特性下面所展示的类使用 property 来向外暴露内部的属性，并且防止用户将属性设置为非法的值： 1234567891011121314151617181920class LineItem: def __init__(self, description, weight, price): self.description = description self.weight = weight # 在这里就使用了 setter 来赋值 self.price = price def subtoal(self): return self.weight * self.price @property def weight(self): return self.__weight # 真正存储值的地方 @weight.setter def weight(self, value): if value &gt; 0: self.__weight = value else: raise ValueError('value must be &gt; 0') 19.3 特性全解析虽然内置的 property 经常用作装饰器，但它其实是一个类。property 构造方法的完整签名如下： 1property(fget=None, fset=None, fdel=None, doc=None) 经典的特性设置方式其实是这样的： 123456789class LineItem: def set_weight(self, value): pass def get_weight(self): pass weight = property(get_weight, set_weight) 而且，我们知道实例属性会覆盖类属性，虽然特性是类属性，但是实例属性并不会覆盖类的特性。 虽然特性是个强大的功能，不过有时更适合使用简单的或底层的替代方案。 19.6 处理属性的重要属性和函数19.6.1 影响属性处理方式的特殊属性 __class__：对象所属类的引用（即 obj.__class__ 与 type(obj) 的作用相同）。Python 的某些特殊方法，例如 __getattr__，只在对象的类中寻找，而不在实例中寻找。 __dict__：一个映射，存储对象或类的可写属性。有 __dict__ 属性的对象，任何时候都能随意设置新属性。如果类有 __slots__ 属性，它的实例可能没有 __dict__ 属性。 __slot__：类可以定义这个这属性，限制实例能有哪些属性。 19.6.2 处理属性的内置函数 dir：列出对象的大多数属性。dir 函数的目的是交互式使用，因此没有提供完整的属性列表，只列出一组“重要的”属性名。 19.6.3 处理属性的特殊方法 __delattr__(self, name)：只要使用 del 语句删除属性，就会调用这个方法。 __dir__(self)：把对象传给 dir 函数时调用，列出属性。 __getattr__(self, name)：仅当获取指定的属性失败，搜索过 obj、Class 和超类之后调用。 __getattribute__(self, name)：尝试获取指定的属性时总会调用这个方法，不过，寻找的属性是特殊属性或特殊方法时除外。 __setattr__(self, name, value)：尝试设置指定的属性时总会调用这个方法。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 18 - 使用 asyncio 包处理并发","slug":"fluent-python-asyncio","date":"2019-10-08T13:23:35.000Z","updated":"2019-10-08T13:23:35.204Z","comments":true,"path":"2019/10/08/fluent-python-asyncio/","link":"","permalink":"https://homholueng.github.io/2019/10/08/fluent-python-asyncio/","excerpt":"","text":"并发是指同时处理多件事 并行是指同时做多件事 二者不同，但是有联系 一个关于结构，一个关于执行 并发用于指定方案，用来解决可能（但未必）并行的问题 —— Rob Pike（Go 语言创造者之一） 18.1 线程与协程对比在编写使用线程来编写程序时，我们需要使用锁来保护程序中重要的部分。而协程默认会做好全方位保护，以防止中断。我们必须显式产出才能让程序的余下部分运行。对协程来说，无需保留锁，在多个线程之间同步操作，协程自身就会同步，因为在任意时刻只有一个协程运行。 18.1.1 asyncio.Future：故意不阻塞asyncio.Future 类的 .result() 方法没有参数，因此不能指定超时时间。此外，如果调用 .result() 方法时期物还没运行完毕，那么 .result() 方法不会阻塞去等待结果，而是抛出 asyncio.InvalidStateError` 异常。 然而，获取 asyncio.Future 对象的结果通常使用 yield from，从中产出结果。使用 yield from 处理期物，等待期物运行完毕这一步无需我们关心，而且不会阻塞事件循环，因为在 asyncio 包中，yield from 的作用是把控制权还给事件循环。 18.1.2 从期物、任务和协程中产出在 asyncio 包中，期物和协程关系紧密，因为可以使用 yield from 从 asyncio.Future 对象中产出结果。这意味着，如果 foo 是协程函数（调用后返回协程对象），抑或是返回 Future 或 Task 实例的普通函数，那么可以这样写：res = yield from foo()。这是 asyncio 包的 API 中很多地方可以互换协程与期物的原因之一。 为了执行这些操作，必须排定协程的运行时间，然后使用 asyncio.Task 对象包装协程。对协程来说，获取 Task 对象有两种主要方式： asyncio.async(coro_or_future, *, loop=None)：这个函数统一了协程和期物：第一个参数可以是二者中的任何一个。如果是 Future 或 Task 对象，那就原封不动地返回。如果是协程，那么 async 函数会调用 loop.create_task(...) 方法创建 Task 对象。loop= 关键字参数是可选的，用于传入事件循环；如果没有传入，那么 async 函数会通过调用 asyncio.get_event_loop() 函数获取循环对象。 BaseEventLoop.create_task(coro)：这个方法排定协程的执行时间，返回一个 asyncio.Task 对象。 在 asyncio 包的文档中，“18.5.3. Tasks and coroutines”一节(https://docs.python.org/3/library/asyncio-task.html)说明了协程、期物和任务之间的关系。 18.2 使用 asyncio 和 aiohttp 包下载asyncio.wait(...) 协程的参数是一个由期物或协程构成的可迭代对象；wait 会分别把各个协程包装进一个 Task 对象。最终的结果是，wait 处理的所有对象都通过某种方式变成 Future 类的实例。wait 是协程函数，因此返回的是一个协程或生成器对象。 wait_coro 运行结束后返回一个元组，第一个元素是一系列结束的期物，第二个元素是一系列未结束的期物。wait 函数有两个关键字参数，如果设定了可能会返回未结束的期物；这两个参数是 timeout 和 return_when。详情参见 asyncio.wait 函数的文档(https://docs.python.org/3/library/asyncio-task.html#asyncio.wait)。 使用 asyncio 包时，我们编写的异步代码中包含由 asyncio 本身驱动的协程（即委派生成器），而生成器最终把职责委托给 asyncio 包或第三方库（如 aiohttp）中的协程。这种处理方式相当于架起了管道，让 asyncio 事件循环（通过我们编写的协程）驱动 执行低层异步 I/O 操作的库函数。 18.3 避免阻塞型调用有两种方法能避免阻塞型调用中止整个应用程序的进程： 在单独的线程中运行各个阻塞型操作 把每个阻塞型操作转换成非阻塞的异步调用使用 多个线程是可以的，但是各个操作系统线程（Python 使用的是这种线程）消耗的内存达兆字节（具体的量取决于操作系统种类）。如果要处理几千个连接，而每个连接都使用一个线程的话，我们负担不起。 为了降低内存的消耗，通常使用回调来实现异步调用。这是一种底层概念，类似于所有并发机制中最古老、最原始的那种——硬件中断。使用回调时，我们不等待响应，而是注册一个函数，在发生某件事时调用。这样，所有调用都是非阻塞的。 18.4 改进 asyncio 下载脚本asyncio.Semaphore 对象维护着一个内部计数器，若在对象上调用 .acquire() 协程方法，计数器则递减；若在对象上调用 .release() 协程方法，计数器则递增。计数器的初始值在实例化 Semaphore 时设定： 1semaphore = asyncio.Semaphore(concur_req) 如果计数器大于零，那么调用 .acquire() 方法不会阻塞；可是，如果计数器为零，那么 .acquire() 方法会阻塞调用这个方法的协程，直到其他协程在同一个 Semaphore 对象上调用 .release() 方法，让计数器递增。 使用 with 语句能够完成上述两种操作： 12with (yield from semaphore): ... 18.4.2 使用Executor对象， 防止阻塞事件循环asyncio 的事件循环在背后维护着一个 ThreadPoolExecutor 对象，我们可以调用 run_in_executor 方法，把可调用的对象发给它执行，避免某些方法的调用阻塞整个事件循环。 12loop = asyncio.get_event_loop()loop.run_in_executor(None, io_function, *args, **kwargs) # 第一个参数是 Executor 实例；如果设为 None，使用事件循环的默认 ThreadPoolExecutor 实例 18.5 从回调到期物和协程Python 中的回调地狱： 123456789101112def stage1(response1): request2 = step1(response1) api_call2(request2, stage2)def stage2(response2): request3 = step2(response2) api_call3(request3, stage3)def stage3(response3): step3(response3)api_call1(request1, stage1) 上述例子中组织代码的方式导致代码难以阅读，也更难编写：每个函数做一部分工作，设置下一个回调，然后返回，让事件循环继续运行。这样，所有本地的上下文都会丢失。执行下一个回调时（例如 stage2），就无法获取 request2 的值。如果需要那个值，那就必须依靠闭包，或者把它存储在外部数据结构中，以便在处理过程的不同 阶段使用。 在这个问题上，协程能发挥很大的作用。在协程中，如果要连续执行 3 个异步操作，只需使用 yield 3 次，让事件循环继续运行。3 次操作都在同一个函数定义体中，像是顺序代码，能在处理过程中使用局部变量保留整个任务的上下文： 123456789101112131415@asyncio.coroutine def three_stages(request1): response1 = yield from api_call1(request1) # 第一步 request2 = step1(response1) response2 = yield from api_call2(request2) # 第二步 request3 = step2(response2) response3 = yield from api_call3(request3) # 第三步 step3(response3)loop.create_task(three_stages(request1))# 必须显式调度执行 如果这时候还需要为每一步可能发生的错误编写处理逻辑，那么第二种方式带来的好处就更为明显。传统的回调方式需要传入错误处理函数，导致代码阅读难度更大；而使用协程，我们只需要简单的使用 try ··· except 块来处理即可。 这么做比陷入回调地狱好多了，但是我不会把这种方式称为协程天堂，毕竟我们还要付出代价。我们不能使用常规的函数，必须使用协程，而且要习惯 yield from。另外，我们必 须使用事件循环显式排定协程的执行时间，或者在其他排定了执行时间的协程中使用 yield from 表达式把它激活。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 17 - 使用期物处理并发","slug":"fluent-python-future-in-parallel","date":"2019-10-06T09:15:18.000Z","updated":"2019-10-08T13:22:59.941Z","comments":true,"path":"2019/10/06/fluent-python-future-in-parallel/","link":"","permalink":"https://homholueng.github.io/2019/10/06/fluent-python-future-in-parallel/","excerpt":"","text":"17.1.2 concurrent.futures 模块concurrent.futures 模块的主要特色是 ThreadPoolExecutor 和 ProcessPoolExecutor 类，这两个类实现的接口能分别在不同的线程或进程中执行可调用的对象。这两个类在内部维护着一个工作线程或进程池，以及要执行的任务队列。 1234from concurrent import futureswith futures.ThredPoolExecutor(worker_count) as executor: res = executor.map(download, address_list) 在上面代码展示的例子中，executor.__exist__ 方法糊调用 executor.shutdown(wait=True) 方法，它会在所有线程都执行完毕前阻塞主线程。 17.1.3 期物从 Python 3.4 起，标准库中有两个名为 Future 的类：concurrent.futures.Future 和 asyncio.Future。这两个类的作用相同：两个 Future 类的实例都表示可能已经完成或者尚未完成的 延迟计算。这与 Twisted 引擎中的 Deferred 类、Tornado 框架中的 Future 类，以及多个 JavaScript 库中的 Promise 对象类似。 期物封装待完成的操作，可以放入队列，完成的状态可以查询，得到结果（或抛出异常）后可以获取结果（或异常）。 我们要记住一件事：通常情况下自己不应该创建期物，而只能由并发框架（concurrent.futures 或 asyncio）实例化。原因很简单：期物表示终将发生的事情，而确定某件事会发生的唯一方式是执行的时间已经排定。 上述两种期物所拥有的下列方法比较在日常使用中会经常用到： .done()：这个方法不阻塞，返回值是布尔值，指明期物链接的可调用对象是否已经执行。 .add_done_callback()：这个方法只有一个参数，类型是可调用的对象，期物运行结束后会调用指定的可调用对象。 .result()：在期物运行结束后调用的话，这个方法 在两个 Future 类中的作用相同：返回可调用对象的结果，或者重新抛出执行可调用的对象时抛出的异常。可是，如果期物没有运行结束，result 方法在两个 Future 类中的行为相差很大： concurrency.futures.Future：result 的调用会阻塞调用方所在的线程，直到有结果可返回。此时，result 方法可以接收可选的 timeout 参数，如果在指定的时间内期物没有运行完毕，会抛出 TimeoutError 异常。 asyncio.Future：result 方法不支持设定超时时间，在这个库中获取期物的结果最好使用 yield from 结构。 17.2 阻塞型I/O和GILCPython 解释器本身就不是线程安全的，因此有全局解释器锁（GIL）， 一次只允许使用一个线程执行 Python 字节码。因此，一个 Python 进程通常不能同时使用多个 CPU 核心。 然而，标准库中所有执行阻塞型 I/O 操作的函数，在等待操作系统返回结果时都会释放 GIL。这意味着在 Python 语言这个层次上可以使用多线程，而 I/O 密集型 Python 程序能从中受益：一个 Python 线程等待网络响应时，阻塞型 I/O 函数会释放 GIL，再运行一个线程。 17.3 使用 concurrent.futures 模块启动进程ProcessPoolExecutor 和 ThreadPoolExecutor 类都实现了通用的 Executor 接口，如果需要做 CPU 密集型处理，使用这个模块能绕开 GIL，利用所有可用的 CPU 核心。 如果使用 Python 处理 CPU 密集型工作，应该试试 PyPy（http://pypy.org）。 17.4 实验 Executor.map 方法1234567891011121314151617181920212223242526from time import sleep, strftimefrom concurrent import futuresdef display(*args): print(strftime('[%H:%M:%S]'), end=' ') print(*args)def loiter(n): msg = '&#123;&#125;loiter(&#123;&#125;): doing nothing for &#123;&#125;s...' display(msg.format('\\t' * n, n, n)) sleep(n) msg = '&#123;&#125;loiter(&#123;&#125;): done.' display(msg.format('\\t' * n, n)) return n * 10def main(): display('Script starting.') executor = futures.ThreadPoolExecutor(max_workers=3) results = executor.map(loiter, range(5)) display('results:', results) display('Waiting for individual results:') for i, result in enumerate(results): display('result &#123;&#125;: &#123;&#125;'.format(i, result))main() 上述程序输出如下： 123456789101112131415161718[16:57:09] Script starting.[16:57:09] loiter(0): doing nothing for 0s...[16:57:09] loiter(0): done.[16:57:09] loiter(1): doing nothing for 1s...[16:57:09] loiter(2): doing nothing for 2s...[16:57:09] results: &lt;generator object Executor.map.&lt;locals&gt;.result_iterator at 0x10d21f258&gt;[16:57:09] loiter(3): doing nothing for 3s...[16:57:09] Waiting for individual results:[16:57:09] result 0: 0[16:57:10] loiter(1): done.[16:57:10] loiter(4): doing nothing for 4s...[16:57:10] result 1: 10[16:57:11] loiter(2): done.[16:57:11] result 2: 20[16:57:12] loiter(3): done.[16:57:12] result 3: 30[16:57:14] loiter(4): done.[16:57:14] result 4: 40 Executor.map 函数易于使用，不过有个特性需要注意：这个函数返回结果的顺序与调用开始的顺序一致。如果第一个调用生成结果用时 10 秒，而其他调用只用 1 秒，在获取 map 方法返回的生成器产出的第一个结果时代码会阻塞 10 秒。在此之后，获取后续结果时不会阻塞，因为后续的调用已经结束。 executor.submit 和 futures.as_completed 这个组合比 executor.map 更灵活，因为 submit 方法能处理不同的可调用对 象和参数，而 executor.map 只能处理参数不同的同一个可调用对象。此外，传给 futures.as_completed 函数的期物集合可以来 自多个 Executor 实例 17.5 显示下载进度并处理错误使用 TQDM 包 （https://github.com/noamraph/tqdm）可以实现的文本动画进度条。 12345import timefrom tqdm import tqdmfor i in tqdm(range(1000)): time.sleep(.01) 随后，进度条会显示在下方： 149%|███████████████████████████████████████████████████████████████▊ | 491/1000 [00:05&lt;00:05, 87.69it/s]","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"通识 - 暗网","slug":"dark-web","date":"2019-09-30T09:41:00.000Z","updated":"2019-09-30T09:41:38.939Z","comments":true,"path":"2019/09/30/dark-web/","link":"","permalink":"https://homholueng.github.io/2019/09/30/dark-web/","excerpt":"","text":"","categories":[{"name":"General Knowledge","slug":"General-Knowledge","permalink":"https://homholueng.github.io/categories/General-Knowledge/"}],"tags":[{"name":"Dark Web","slug":"Dark-Web","permalink":"https://homholueng.github.io/tags/Dark-Web/"}]},{"title":"Chapter 16 - 协程","slug":"fluent-python-coroutine","date":"2019-09-05T12:14:56.000Z","updated":"2019-09-30T09:35:49.645Z","comments":true,"path":"2019/09/05/fluent-python-coroutine/","link":"","permalink":"https://homholueng.github.io/2019/09/05/fluent-python-coroutine/","excerpt":"","text":"字典为动词 “to yield” 给出了两个释义：产出和让步。对于 Python 生成器中的 yield 来说，这两个含义都成立。yield item 这行代码会产出一个值，提供给 next(...) 的调用方；此外，还会作出让步，暂停执行生成器，让调用方继续工作，直到需要使用另一个值时再调用 next()。调用方会从生成器中拉取值。 yield 关键字甚至还可以不接收或传出数据。不管数据如何流动，yield 都是一种流程控制工具，使用它可以实现协作式多任务：协程可以把控制器让步给中心调度程序，从而激活其他的协程。 从根本上把 yield 视作控制流程的方式，这样就好理解协程了。 16.1 生成器如何进化成协程协程的底层架构在 “PEP 342—Coroutines via Enhanced Generators”（https://www.python.org/dev/peps/pep-0342/)中定义，并在 Python 2.5（2006年）实现了。自此之后，yield 关键字可以在表达式中使用，而且生成器 API 中增加了 .send(value) 方法。生成器的调用方可以使用 .send(...) 方法发送数据，发送的数据会成为生成器函数中 yield 表达式的值。因此，生成器可以作为协程使用。协程是指一个过程，这个过程与调用方协作，产出由调用方提供的值。 除了 .send(...) 方法，PEP 342 还添加了 .throw(...) 和 .close() 方法：前者的作用是让调用方抛出异常，在生成器中处理；后者的作用是终止生成器。 协程最近的演进来自 Python 3.3（2012 年）实现的 “PEP 380—Syntax for Delegating to a Subgenerator”（https://www.python.org/dev/peps/pep0380/)。PEP 380 对生成器函数的句法做了两处改动，以便更好地作为协程使用。 现在，生成器可以返回一个值；以前，如果在生成器中给 return 语句提供值，会抛出 SyntaxError 异常。 新引入了 yield from 句法，使用它可以把复杂的生成器重构成小型的嵌套生成器，省去了之前把生成器的工作委托给子生成器所需的大量样板代码。 16.2 用作协程的生成器的基本行为下面是一个最简单的协程的演示： 12345678910111213141516171819202122In [1]: def simple_coroutine(): ...: print('-&gt; coroutine started') ...: x = yield ...: print('-&gt; coroutine received:', x) ...:In [2]: my_coro = simple_coroutine()In [3]: my_coroOut[3]: &lt;generator object simple_coroutine at 0x106cfeaf0&gt;In [4]: next(my_coro)-&gt; coroutine startedIn [5]: my_coro.send(42)-&gt; coroutine received: 42---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-5-7c96f97a77cb&gt; in &lt;module&gt;----&gt; 1 my_coro.send(42)StopIteration: 首先，协程使用生成器函数定义，即定义体中含有 yield 关键字，与创建生成器的方式一下，调用函数得到生成器对象；然后调用 next 函数，因为生成器还没启动，没有在 yield 处暂停。调用 send 发送数据后，yield 表达式会计算出发送过去的值，协程恢复执行，一直运行到下一个 yield 表达式或者终止，实例中控制权流到了协程定义体的末尾，导致生成器像往常一样抛出了 StopIteration 异常。 协程会处于一下四个状态中的其中一个，使用 inspect.getgeneratorstate 能够获取协程的状态： GEN_CREATED：等待开始执行。 GEN_RUNNING：解释器正在执行。 GEN_SUSPENDED：在 yield 表达式处暂停。 GEN_CLOSED：执行结束。 123456789101112131415161718192021222324252627In [6]: import inspectIn [7]: inspect.getgeneratorstate(my_coro)Out[7]: 'GEN_CLOSED'In [8]: my_coro = simple_coroutine()In [9]: inspect.getgeneratorstate(my_coro)Out[9]: 'GEN_CREATED'In [10]: next(my_coro)-&gt; coroutine startedIn [11]: inspect.getgeneratorstate(my_coro)Out[11]: 'GEN_SUSPENDED'In [12]: my_coro.send(42)-&gt; coroutine received: 42---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-12-7c96f97a77cb&gt; in &lt;module&gt;----&gt; 1 my_coro.send(42)StopIteration:In [13]: inspect.getgeneratorstate(my_coro)Out[13]: 'GEN_CLOSED' 因为 send 方法的参数会成为暂停的 yield 表达式的值，所以，仅当协程处于暂停状态时才能调用 send 方法。 如果创建协程对象后立即把 None 之外的值发给它，会出现下述错误： 123456789In [14]: my_coro = simple_coroutine()In [15]: my_coro.send(42)---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-15-7c96f97a77cb&gt; in &lt;module&gt;----&gt; 1 my_coro.send(42)TypeError: can't send non-None value to a just-started generator 最先调用 next(my_coro) 函数这一步通常称为“预激”（prime）协程 （即，让协程向前执行到第一个 yield 表达式，准备好作为活跃的协程使用）。 16.4 预激协程的装饰器如果不预激，那么协程没什么用，为了简化协程的用法，有时会使用一个预激装饰器。 12345678910111213141516171819202122232425262728from functools import wrapsdef coroutine(func): @wraps(func) def primer(*args, **kwargs): gen = func(*args, **kwargs) next(gen) return gen return primerIn [2]: @coroutine ...: def simple_coroutine(): ...: print('-&gt; coroutine started') ...: x = yield ...: print('-&gt; coroutine received:', x) ...:In [3]: gen =simple_coroutine()-&gt; coroutine startedIn [4]: gen.send(42)-&gt; coroutine received: 42---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-4-72d0e54668c5&gt; in &lt;module&gt;----&gt; 1 gen.send(42)StopIteration: 16.5 终止协程和异常处理协程中未处理的异常会向上冒泡，传给 next 函数或 send 方法的调用方（即触发协程的对象）。 这个特性允许我们发送某个哨符值，让协程退出。内置的 None 和 Ellipsis 等常量经常用作哨符值。Ellipsis 的优点是，数据流中不太常有这个值。 从 Python 2.5 开始，客户代码可以在生成器对象上调用两个方法，显式地把异常发给协程。 这两个方法是 throw 和 close： generator.throw(exc_type[, exc_value[, traceback]])：致使生成器在暂停的 yield 表达式处抛出指定的异常。如果生成器处理了抛出的异常，代码会向前执行到下一个 yield 表达式，而产出的值会成为调用 generator.throw 方法得到的返回值。如果生成器没有处理抛出的异常，异常会向上冒泡，传到调用方的上下文中。 generator.close()：致使生成器在暂停的 yield 表达式处抛出 GeneratorExit 异常。 如果生成器没有处理这个异常，或者抛出了 StopIteration 异常（通常是指运行到结尾），调用方不会报错。如果收到 GeneratorExit 异常，生成器一定不能产出值，否则解释器会抛出 RuntimeError 异常。 生成器抛出的其他异常会向上冒泡，传给调用方。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class DemoException(Exception): passdef demo_exc_handling(): print('-&gt; coroutine started') while True: try: x = yield except DemoException: print('*** DemoException handled. Continuing...') else: print('-&gt; coroutine received: &#123;!r&#125;'.format(x)) raise RuntimeError('This line should never run.')In [2]: exc_coro = demo_exc_handling()In [3]: next(exc_coro)-&gt; coroutine startedIn [4]: exc_coro.send(11)-&gt; coroutine received: 11In [5]: exc_coro.send(22)-&gt; coroutine received: 22In [6]: exc_coro.throw(DemoException())*** DemoException handled. Continuing...In [8]: inspect.getgeneratorstate(exc_coro)Out[8]: 'GEN_SUSPENDED'In [9]: exc_coro.throw(ZeroDivisionError())---------------------------------------------------------------------------ZeroDivisionError Traceback (most recent call last)&lt;ipython-input-9-e03abf340dfc&gt; in &lt;module&gt;----&gt; 1 exc_coro.throw(ZeroDivisionError())&lt;ipython-input-1-cc1425411a81&gt; in demo_exc_handling() 6 while True: 7 try:----&gt; 8 x = yield 9 except DemoException: 10 print('*** DemoException handled. Continuing...')ZeroDivisionError:In [11]: inspect.getgeneratorstate(exc_coro)Out[11]: 'GEN_CLOSED'In [12]: exc_coro = demo_exc_handling()In [13]: next(exc_coro)-&gt; coroutine startedIn [14]: exc_coro.send(22)-&gt; coroutine received: 22In [15]: exc_coro.close()In [16]: inspect.getgeneratorstate(exc_coro)Out[16]: 'GEN_CLOSED' 16.6 让协程返回值下面是 averager 协程的不同版本，这一版会返回结果： 12345678910111213141516171819202122232425262728293031323334from collections import namedtupleResult = namedtuple('Result', 'count average')def averager(): total = 0.0 count = 0 average = None while True: term = yield if term is None: break total += term count += 1 average = total / count return Result(count, average)In [2]: coro_avg = averager()In [3]: next(coro_avg)In [4]: coro_avg.send(10)In [5]: coro_avg.send(30)In [6]: coro_avg.send(35)In [7]: coro_avg.send(None)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-7-a9c80bbec98f&gt; in &lt;module&gt;----&gt; 1 coro_avg.send(None)StopIteration: Result(count=3, average=25.0) 发送 None 会终止循环，导致协程结束，返回结果。一如既往，生成器对象会抛出 StopIteration 异常。异常对象的 value 属性保存着返回的值。 12345678910111213141516In [8]: coro_avg = averager()In [9]: next(coro_avg)In [10]: coro_avg.send(30)In [11]: coro_avg.send(10)In [12]: try: ...: coro_avg.send(None) ...: except StopIteration as exc: ...: result = exc.value ...:In [13]: resultOut[13]: Result(count=2, average=20.0) 获取协程的返回值虽然要绕个圈子，但这是 PEP 380 定义的方式，当我们意识到这一点之后就说得通了：yield from 结构会在内部自动捕获 StopIteration 异常。这种处理方式与 for 循环处理 StopIteration 异常的方式一样：循环机制使用用户易于理解的方式处理异常。对 yield from 结构来说，解释器不仅会捕获 StopIteration 异常，还会把 value 属性的值变成 yield from 表达式的值。可惜，我们无法在控制台中使用交互的方式测试这种行为，因为在函数外部使用 yield from（以及 yield）会导致句法出错。 16.7 使用yield from首先要知道，yield from 是全新的语言结构。它的作用比 yield 多很 多，因此人们认为继续使用那个关键字多少会引起误解。在其他语言中，类似的结构使用 await 关键字，这个名称好多了，因为它传达了至关重要的一点：在生成器 gen 中使用 yield from subgen() 时，subgen 会获得控制权，把产出的值传给 gen 的调用方，即调用方可以直接控制 subgen。与此同时，gen 会阻塞，等待 subgen 终止。 yield from 的主要功能是打开双向通道，把最外层的调用方与最内层的子生成器连接起来，这样二者可以直接发送和产出值，还可以直接传入异常，而不用在位于中间的协程中添加大量处理异常的样板代码。有了这个结构，协程可以通过以前不可能的方式委托职责。 若想使用 yield from 结构，就要大幅改动代码。为了说明需要改动的部分，PEP 380 使用了一些专门的术语： 委派生成器：包含 yield from &lt;iterable&gt; 表达式的生成器函数。 子生成器：从 yield from 表达式中 &lt;iterable&gt; 部分获取的生成器。 调用方：指代调用委派生成器的客户端代码。 下面的代码使用 yield from 计算平均值并输出统计报告： 1234567891011121314151617181920212223242526272829303132333435363738394041424344from collections import namedtupleResult = namedtuple('Result', 'count average')# 子生成器def averager(): total = 0.0 count = 0 average = None while True: term = yield if term is None: break total += term count += 1 average = total / count return Result(count, average)# 委派生成器def grouper(results, key): while True: results[key] = yield from averager()# 客户端代码，即调用方def main(data): results = &#123;&#125; for key, values in data.items(): group = grouper(results, key) next(group) for value in values: group.send(value) group.send(None) print(results)data = &#123; 'girls;kg': [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5], 'girls;m': [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43], 'boys;kg': [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3], 'boys;m': [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],&#125;if __name__ == '__main__': main(data) 在上述的例子中，grouper 发送的每个值都会经由 yield from 处理，通过管道传给 averager 实例。grouper 会在 yield from 表达式处暂停，等待 averager 实例处理客户端发来的值。averager 实例运行完毕后，返回的值绑定到 results[key] 上。while 循环会不断创建 averager 实例，处理更多的值。 而在 main 中，我们把各个 value 传给 grouper。传入的值最终到达 averager 函数中 term = yield 那一行；grouper 永远不知道传入的值是什么。 最后，把 None 传入 grouper，导致当前的 averager 实例终止，也让 grouper 继续运行，再创建一个 averager 实例，处理下一组值。 整个过程如下： 外层 for 循环每次迭代会新建一个 grouper 实例，赋值给 group 变量；group 是委派生成器。 调用 next(group)，预激委派生成器 grouper，此时进入 while True 循环，调用子生成器 averager 后，在 yield from 表达式处暂停。 内层 for 循环调用 group.send(value)，直接把值传给子生成器 averager。同时，当前的 grouper 实例（group）在 yield from 表达式处暂停。 内层循环结束后，group 实例依旧在 yield from 表达式处暂停， 因此，grouper 函数定义体中为 results[key] 赋值的语句还没有执行。 如果外层 for 循环的末尾没有 group.send(None)，那么 averager 子生成器永远不会终止，委派生成器 group 永远不会再次激活，因此永远不会为 results[key] 赋值。 外层 for 循环重新迭代时会新建一个 grouper 实例，然后绑定到 group 变量上。前一个 grouper 实例（以及它创建的尚未终止的 averager 子生成器实例）被垃圾回收程序回收。 main，grouper，averager 的关系就如下图所示： 16.8 yield from 的意义PEP 380 中对 yield from 行为的说明如下： 子生成器产出的值都直接传给委派生成器的调用方（即客户端代码）。 使用 send() 方法发给委派生成器的值都直接传给子生成器。如果 发送的值是 None，那么会调用子生成器的 __next__() 方法。如果发送的值不是 None，那么会调用子生成器的 send() 方法。如 果调用的方法抛出 StopIteration 异常，那么委派生成器恢复运行。任何其他异常都会向上冒泡，传给委派生成器。 生成器退出时，生成器（或子生成器）中的 return expr 表达式会触发 StopIteration(expr) 异常抛出。 yield from 表达式的值是子生成器终止时传给 StopIteration 异常的第一个参数。 传入委派生成器的异常，除了 GeneratorExit 之外都传给子生成器的 throw() 方法。如果调用 throw() 方法时抛出 StopIteration 异常，委派生成器恢复运行。StopIteration 之外的异常会向上冒泡，传给委派生成器。 如果把 GeneratorExit 异常传入委派生成器，或者在委派生成器 上调用 close() 方法，那么在子生成器上调用 close() 方法，如果它有的话。如果调用 close() 方法导致异常抛出，那么异常会向上冒泡，传给委派生成器；否则，委派生成器抛出 GeneratorExit 异常。 在不考虑在委派生成器上调用 throw 和 close 及子生成器不抛出异常的情况下，RESULT = yield from EXPR 等价于下列代码： 123456789101112131415_i = iter(EXPR) # 子生成器，因为需要处理可迭代对象，所以调用了 iter 方法，对生成器对象调用 iter 会返回生成器自身try: _y = next(_i) # 子生成器产出的值except StopIteration as _e: _r = _e.value # 结果else: while 1: _s = yield _y # 客户端发送过来的值 try: _y = _i.send(_s) except StopIteration as _e: _r = _e.value breakRESULT = _r 但是，现实情况要复杂一些，因为要处理客户对 .throw(...) 和 .close() 方法的调用，而这两个方法执行的操作必须传入子生成器。 此外，子生成器可能只是纯粹的迭代器，不支持 .throw(...) 和 .close() 方法，因此 yield from 结构的逻辑必须处理这种情况。。如果子生成器实现了这两个方法，而在子生成器内部，这两个方法都会触发异常抛出，这种情况也必须由 yield from 机制处理。调用方可能会无缘无故地让子生成器自己抛出异常，实现 yield from 结构时也必须处理这种情况。最后，为了优化，如果调用方调用 next(...) 函数或 .send(None) 方法，都要转交职责，在子生成器上调用 next(...) 函数；仅当调用方发送的值不是 None 时，才使用子生成器的 .send(...) 方法。 为了方便对比，下面列出 PEP 380 中扩充 yield from 表达式的完整伪代码： 123456789101112131415161718192021222324252627282930313233343536373839_i = iter(EXPR)try: _y = next(_i)except StopIteration as _e: _r = _e.valueelse: while 1: try: _s = yield _y except GeneratorExit as _e: # 这一部分用于关闭委派生成器和子生成器。因为子生成器可以是任 何可迭代的对象，所以可能没有 close 方法。 try: _m = _i.close except AttributeError: pass else: _m() raise _e except BaseException as _e: # 这一部分处理调用方通过 .throw(...) 方法传入的异常。同样，子生成器可以是迭代器，从而没有 throw 方法可调用——这种情况会导 致委派生成器抛出异常。 _x = sys.exc_info() try: _m = _i.throw except AttributeError: raise _e else: # 如果子生成器有 throw 方法，调用它并传入调用方发来的异常。子生成器可能会处理传入的异常（然后继续循环）；可能抛出 StopIteration 异常（从中获取结果，赋值给 _r，循环结束）；还可能不处理，而是抛出相同的或不同的异常，向上冒泡，传给委派生成器。 try: _y = m(*_x) except StopIteration as _e: _r = _e.value break else: # 如果产出值时没有异常…… try: if _s is None: _y = next(_i) # 如果调用方最后发送的值是 None，在子生成器上调用 next 函数， 否则调用 send 方法。 else: _y = _i.send(_s) except StopIteration as _e: _r = _e.value breakRESULT = _r 在伪代码的顶部，有行代码（_y = next(_i)）揭示了一个重要的细节：要预激子生成器。这表明，用于自动预激的装饰器与 yield from 结构不兼容。 16.9 使用协程做离散事件仿真离散事件仿真（Discrete Event Simulation，DES）是一种把系统建模成一系列事件的仿真类型。在离散事件仿真中，仿真“钟”向前推进的量不是固定的，而是直接推进到下一个事件模型的模拟时间。假如我们抽象模拟出租车的运营过程，其中一个事件是乘客上车，下一个事件则是乘客下车。 离散事件仿真能够使用多线程或在单个线程中使用面向事件的编程技 术（例如事件循环驱动的回调或协程）实现。可以说，为了实现连续仿真，在多个线程中处理实时并行的操作更自然。而协程恰好为实现离散事件仿真提供了合理的抽象。 在仿真领域，进程这个术语指代模型中某个实体的活动，与操作系统中的进程无关。仿真系统中的一个进程可以使用操作系统中的一个进程实现，但是通常会使用一个线程或一个协程实现。 小结 生成器有三种不同的代码编写风格：有传统的拉取式（迭代器）、推送式（计算平均值的示例）、还有任务时。 使用协程做面向事件编程时，协程会不断把控制权让步给主循环，激活并向前运行其他协程，从而执行各个并发活动。这是一种协作式多任务：协程显式自主地把控制权让步给中央调度程序。而多线程实现的是抢占式多任务。调度程序可以在任何时刻暂停线程（即使在执行一个语句的过程中），把控制权让给其他线程。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 15 - 上下文管理器和 else 块","slug":"fluent-python-context-and-else","date":"2019-08-25T09:32:39.000Z","updated":"2019-09-30T09:35:49.644Z","comments":true,"path":"2019/08/25/fluent-python-context-and-else/","link":"","permalink":"https://homholueng.github.io/2019/08/25/fluent-python-context-and-else/","excerpt":"","text":"15.1 先做这个，再做那个：if 语句之外的 else 块for/else、while/else 和 try/else 的语义关系紧密，不过与 if/else 差别很大： for： 仅当 for 循环运行完毕时（即 for 循环没有被 break 语句中止）才运行 else 块。 while：仅当 while 循环因为条件为假值而退出时（即 while 循环没有被 break 语句中止）才运行 else 块。 try：仅当 try 块中没有异常抛出时才运行 else 块。（注意，else 中抛出的异常不会由前面的 except 子句处理） 在所有情况下，如果异常或者 return、break 或 continue 语句导致 控制权跳到了复合语句的主块之外，else 子句也会被跳过。 15.2 上下文管理器和 with 块上下文管理器对象存在的目的是管理 with 语句，就像迭代器的存在是为了管理 for 语句一样。 上下文管理器协议包含 __enter__ 和 __exit__ 两个方法。with 语句开始运行时，会在上下文管理器对象上调用 __enter__ 方法。with 语句运行结束后，会在上下文管理器对象上调用 __exit__ 方法，以此扮演 finally 子句的角色。 不管控制流程以哪种方式退出 with 块，都会在上下文管理器对象上调用 __exit__ 方法，而不是在 __enter__ 方法返回的对象上调用。 下面的例子使用一个精心制作的上下文管理器执行操作，以此强调上下文管理器与 __enter__ 方法返回的对象之间的区别： 12345678910111213141516171819202122232425262728In [5]: class LookingGlass: ...: ...: def __enter__(self): ...: import sys ...: self.original_write = sys.stdout.write ...: sys.stdout.write = self.reverse_write ...: return 'JABBERWOCKY' ...: ...: def reverse_write(self, text): ...: self.original_write(text[::-1]) ...: ...: def __exit__(self, exc_type, exc_value, traceback): ...: import sys ...: sys.stdout.write = self.original_write ...: if exc_type is ZeroDivisionError: ...: print('Please DO NOT divide by zero!') ...: return True ...:In [6]: with LookingGlass() as what: ...: print('Alice, Kitty and Snowdrop') ...: print(what) ...:pordwonS dna yttiK ,ecilAYKCOWREBBAJIn [7]: whatOut[7]: 'JABBERWOCKY' 如果一切正常，Python 调用 __exit__ 方法时传入的参数是 None, None, None；如果抛出了异常，这三个参数是异常数据。 exc_type：异常类。 exc_value：异常实例。有时会有参数传给异常构造方法，例如错误消息，这些参数可以使用 exc_value.args 获取。 traceback：traceback 对象。 如果 __exit__ 方法返回 None，或者 True 之外的值，with 块中的任何异常都会向上冒泡。 上下文管理器的具体工作方式参见下面的例子。在这个示例中，我们在 with 块之外使用 LookingGlass 类，因此可以手动调用 __enter__ 和 __exit__ 方法： 1234567891011121314In [8]: manager = LookingGlass()In [9]: monster = manager.__enter__()In [10]: monster == 'JABBERWOCKY'Out[10]: eurTIn [11]: monsterOut[11]: 'YKCOWREBBAJ'In [13]: manager.__exit__(None, None, None)In [14]: monsterOut[14]: 'JABBERWOCKY' 15.3 contextlib 模块中的实用工具closing如果对象提供了 close() 方法，但没有实现 __enter__/__exit__ 协议，那么可以使用这个函数构建上下文管理器。 123456from contextlib import closingfrom urllib.request import urlopenwith closing(urlopen('http://www.python.org')) as page: for line in page: print(line) suppress构建临时忽略指定异常的上下文管理器。 1234567from contextlib import suppresswith suppress(FileNotFoundError): os.remove('somefile.tmp')with suppress(FileNotFoundError): os.remove('someotherfile.tmp') 上面的代码和下面的代码是等价的： 123456789try: os.remove('somefile.tmp')except FileNotFoundError: passtry: os.remove('someotherfile.tmp')except FileNotFoundError: pass @contextmanager这个装饰器把简单的生成器函数变成上下文管理器，这样就不用创建类去实现管理器协议了。 123456789101112131415from contextlib import contextmanager@contextmanagerdef managed_resource(*args, **kwds): # Code to acquire resource, e.g.: resource = acquire_resource(*args, **kwds) try: yield resource finally: # Code to release resource, e.g.: release_resource(resource)&gt;&gt;&gt; with managed_resource(timeout=3600) as resource:... # Resource is released at the end of this block,... # even if code in the block raises an exception ContextDecorator这是个基类，用于定义基于类的上下文管理器。这种上下文管理器也能用于装饰函数，在受管理的上下文中运行整个函数。 1234567891011121314151617181920212223242526from contextlib import ContextDecoratorclass mycontext(ContextDecorator): def __enter__(self): print('Starting') return self def __exit__(self, *exc): print('Finishing') return False&gt;&gt;&gt; @mycontext()... def function():... print('The bit in the middle')...&gt;&gt;&gt; function()StartingThe bit in the middleFinishing&gt;&gt;&gt; with mycontext():... print('The bit in the middle')...StartingThe bit in the middleFinishing ExitStack这个上下文管理器能进入多个上下文管理器。with 块结束时，ExitStack 按照后进先出的顺序调用栈中各个上下文管理器的 __exit__ 方法。如果事先不知道 with 块要进入多少个上下文管理器，可以使用这个类。例如，同时打开任意一个文件列表中的所有文件。 12345with ExitStack() as stack: files = [stack.enter_context(open(fname)) for fname in filenames] # All opened files will automatically be closed at the end of # the with statement, even if attempts to open files later # in the list raise an exception 15.4 使用 @contextmanager其实，contextlib.contextmanager 装饰器会把函数包装成实现 __enter__ 和 __exit__ 方法的类。 这个类的 __enter__ 方法有如下作用： 调用生成器函数，保存生成器对象（这里把它称为 gen）。 调用 next(gen)，执行到 yield 关键字所在的位置。 返回 next(gen) 产出的值，以便把产出的值绑定到 with/as 语句中的目标变量上。 with 块终止时，__exit__ 方法会做一下几件事： 检查有没有把异常传给 exc_type；如果有，调用 gen.throw(exception)，在生成器函数定义体中包含 yield 关键字的那一行抛出异常。 否则，用 next(gen)，继续执行生成器函数定义体中 yield 语句之后的代码。 使用 @contextmanager 装饰器时，要把 yield 语句放在 try/finally 语句中（或者放在 with 语句中），这是无法避免的，因为我们永远不知道上下文管理器的用户会在 with 块中做什么。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 14 - 可迭代的对象、迭代器和生成器","slug":"fluent-python-iter","date":"2019-08-25T08:21:37.000Z","updated":"2019-08-27T13:35:34.287Z","comments":true,"path":"2019/08/25/fluent-python-iter/","link":"","permalink":"https://homholueng.github.io/2019/08/25/fluent-python-iter/","excerpt":"","text":"14.1 Sentence 类第1版：单词序列我们要实现一个 Sentence 类，以此打开探索可迭代对象的旅程。我们向这个类的构造方法传入包含一些文本的字符串，然后可以逐个单词迭代。 12345678910111213141516171819import reimport reprlibRE_WORD = re.compile('\\w+')class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __getitem__(self, index): return self.words[index] def __len__(self): return len(self.words) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) 注意，reprlib.repr 这个实用函数用于生成大型数据结构的简略字符串表示。 序列可以迭代的原因：iter函数解释器需要迭代对象 x 时，会自动调用 iter(x)。 内置的 iter 函数有以下作用。 检查对象是否实现了 __iter__ 方法，如果实现了就调用它，获取一个迭代器。 如果没有实现 __iter__ 方法，但是实现了 __getitem__ 方法， Python 会创建一个迭代器，尝试按顺序（从索引 0 开始）获取元素。 如果尝试失败，Python 抛出 TypeError 异常，通常会提示“C object is not iterable”（C 对象不可迭代），其中 C 是目标对象所属的类。 任何 Python 序列都可迭代的原因是，它们都实现了 __getitem__ 方法。其实，标准的序列也都实现了 __iter__ 方法，因此你也应该这么做。之所以对 __getitem__ 方法做特殊处理，是为了向后兼容，而未来可能不会再这么做。 从 Python 3.4 开始，检查对象 x 能否迭代，最准确的方法是：调用 iter(x) 函数，如果不可迭代，再处理 TypeError 异常。 14.2 可迭代的对象与迭代器的对比可迭代对象： 实现了能返回迭代器的 __iter__ 方法的对象 实现了 __getitem__ 方法，且参数是从 0 开始的索引的对象 Python 会从可迭代对象中获取迭代器来进行迭代操作。 例如下面的例子，背后是有迭代器的： 123s = 'ABC'for char in s: print(char) 如果没有 for 语句的话，则需要使用 while 来模拟： 12345678s = 'ABC'it = iter(s)while True: try: print(next(it)) except StopIteration: del it break 标准的迭代器接口有两个方法： __next__：返回下一个可用的元素 __iter__：返回 self，以便在可使用迭代对象的地方使用迭代器 这个接口在 collections.abc.Iterator 抽象基类中制定，该接口使用了 __subclasshook__ 来判断某个类是否是其子类： 123456789class Iterator(Iterable): @classmethod def __subclasshook__(cls, C): if (any(\"__next__\" in B.__dict__ for B in C.__mro__)) and (any(\"__iter__\" in B.__dict__ for B in C.__mro__)): return True return NotImplemented __subclasshook__(subclass)必须定义为类方法。该方法检查 subclass 是否是该抽象基类的子类。该方法必须返回 True, False 或是 NotImplemented。如果返回 True，subclass 就会被认为是这个抽象基类的子类。如果返回 False，无论正常情况是否应该认为是其子类，统一视为不是。如果返回 NotImplemented，子类检查会按照正常机制继续执行。 14.3 Sentence 类第2版：典型的迭代器在这一个版本中，我们根据《设计模式：可复用面向对象软件的基础》一书中给出的模型来实现典型的迭代器设计模式，但这并不符合 Python 的习惯做法。 12345678910111213141516171819202122232425262728293031323334import reimport reprlibRE_WORD = re.compile('\\w+')class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return SentenceIterator(self.words)class SentenceIterator: def __init__(self, words): self.words = words self.index = 0 def __next__(self): try: word = self.words[self.index] except IndexError: raise StopIteration() self.index += 1 return word def __iter__(self): return self 这一版的工作量很大（对于懒惰的 Python 程序员来说的确如此）。 把 Sentence 变成迭代器：BAD IDEA ！构建可迭代的对象和迭代器时经常会出现错误，原因是混淆了二者。要知道，可迭代对象有个 __iter__ 方法，每次都实例化一个新的迭代器；而迭代器要实现 __next__ 方法，返回单个元素，此外还要实现 __iter__ 方法，返回迭代器本身。 切忌将 Sentence 类实现为迭代器！因为可迭代对象必须能够返回多个独立的迭代器，而每个迭代器要能维护自身内部的状态！ 14.4 Sentence 类第3版：生成器函数实现相同的功能，Python 习惯的方式是用生成器函数代替 SentenceIterator 类： 12345678910111213141516171819import reimport reprlibRE_WORD = re.compile('\\w+')class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): for word in self.words: yield word return 只要 Python 函数的定义体中有 yield 关键字，该函数就是生成器函数。调用生成器函数时，会返回一个生成器对象。也就是说，生成器函数是生成器工厂。 下面一个特别简单的函数说明生成器的行为： 12345678910111213141516171819202122232425262728293031323334353637In [3]: def gen123(): ...: yield 1 ...: yield 2 ...: yield 3 ...:In [4]: gen123Out[4]: &lt;function __main__.gen123()&gt;In [8]: gen123()Out[8]: &lt;generator object gen123 at 0x10287ff10&gt;In [10]: for i in gen123(): ...: print(i) ...:123In [11]: g = gen123()In [12]: next(g)Out[12]: 1In [13]: next(g)Out[13]: 2In [14]: next(g)Out[14]: 3In [15]: next(g)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-15-e734f8aca5ac&gt; in &lt;module&gt;----&gt; 1 next(g)StopIteration: 生成器函数会创建一个生成器对象，包装生成器函数的定义体。把生成器传给 next 函数时，生成器函数会向前，执行函数定义体中的下一个 yield 语句，返回产出的值，并在函数定义体的当前位置暂停。 这一版 Sentence 类比前一版简短多了，但是还不够懒惰。如今，人们认为惰性是好的特质，至少在编程语言和 API 中是如此。惰性实现是指尽可能延后生成值。这样做能节省内存，而且或许还可以避免做无用的处理。 14.5 Sentence 类第4版：惰性实现目前实现的几版 Sentence 类都不具有惰性，因为 __init__ 方法在一开始就构建好了文本中的单词列表，然后将其绑定到 self.words 属性上。 为了解决这个问题，我们可以使用 re.finditer 函数，re.finditer 函数是 re.findall 函数的惰性版本，返回的不是列表，而是一个生成器，按需生成 re.MatchObject 实例。 1234567891011121314151617import reimport reprlibRE_WORD = re.compile('\\w+')class Sentence: def __init__(self, text): self.text = text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): for match in RE_WORD.finditer(self.text): yield match.group() 14.6 Sentence 类第5版：生成器表达式生成器表达式可以理解为列表推导的惰性版本：不会迫切地构建列表，而是返回一个生成器，按需惰性生成元素。也就是说，如果列表推导是制造列表的工厂，那么生成器表达式就是制造生成器的工厂。 下面是列表推导和生成器表达式的对比： 1234567891011121314151617181920212223242526272829In [1]: def gen_AB(): ...: print('start') ...: yield 'A' ...: print('continue') ...: yield 'B' ...: print('end.') ...:In [2]: res1 = [x for x in gen_AB()]startcontinueend.In [4]: res1Out[4]: ['A', 'B']In [5]: res2 = (x for x in gen_AB())In [6]: res2Out[6]: &lt;generator object &lt;genexpr&gt; at 0x1027dbca8&gt;In [8]: for i in res2: ...: print(i) ...:startAcontinueBend. 使用生成器表达式，我们能够进一步减少 Sentence 实现的代码： 12345678910111213141516import reimport reprlibRE_WORD = re.compile('\\w+')class Sentence: def __init__(self, text): self.text = text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return (match.group() for match in RE_WORD.finditer(self.text)) 生成器表达式是语法糖：完全可以替换成生成器函数，不过有时使用生成器表达式更便利。 14.7 何时使用生成器表达式生成器表达式是创建生成器的简洁句法，这样无需先定义函数再调用。不过，生成器函数灵活得多，可以使用多个语句实现复杂的逻辑，也可以作为协程使用。 选择使用哪种句法很容易判断：如果生成器表达式要分成多行写，推荐定义生成器函数，以便提高可读性。此外，生成器函数有名称，因此可以重用。 14.9 标准库中的生成器函数过滤第一组是用于过滤的生成器函数：从输入的可迭代对象中产出元素的子集，而且不修改元素本身。 模块 函数 说明 itertools compress(it, selector_it) 并行处理两个可迭代对象；如果 selector_it 中的元素是真值，产出 it 中对应的元素 itertools dropwhile(predicate, it) 处理 it，跳过 predicate 的计算结果为真值的元素，然后产出剩下的各个元素（不再进一步检查） （内置） filter 把 it 中的各个元素传给 predicate，如果 返回真值，那么产出对应的元素，如果 predicate 是 None，那么只产出真值元素 itertools filterfalse(predicate, it) 与 filter 函数的作用类似，不过 predicate 的逻辑是相反的：predicate 返回假值时产出对应的元素 itertools islice(it, stop) 或 islice(it, start, stop, step=1) 产出 it 的切片，作用类似于 s[:stop] 或 s[start:stop:step]，不过 it 可以是任何可迭代的对象，而且这个函数实现的是惰性操作 itertools takewhile(predicate, it) predicate 返回真值时产出对应的元素，然后立即停止，不再继续检查 1234567891011121314151617181920212223242526272829In [1]: import itertoolsIn [2]: def vowel(c): ...: return c.lower() in 'aeiou' ...:In [3]: list(filter(vowel, 'Aardvark'))Out[3]: ['A', 'a', 'a']In [4]: list(itertools.filterfalse(vowel, 'Aardvark'))Out[4]: ['r', 'd', 'v', 'r', 'k']In [5]: list(itertools.dropwhile(vowel, 'Aardvark'))Out[5]: ['r', 'd', 'v', 'a', 'r', 'k']In [6]: list(itertools.takewhile(vowel, 'Aardvark'))Out[6]: ['A', 'a']In [7]: list(itertools.compress('Aardvark', [1, 0, 1, 1, 0, 1]))Out[7]: ['A', 'r', 'd', 'a']In [8]: list(itertools.islice('Aardvark', 4))Out[8]: ['A', 'a', 'r', 'd']In [9]: list(itertools.islice('Aardvark', 4, 7))Out[9]: ['v', 'a', 'r']In [10]: list(itertools.islice('Aardvark', 1, 7, 2))Out[10]: ['a', 'd', 'a'] 映射下一组是用于映射的生成器函数：在输入的单个可迭代对象（map 和 starmap 函数处理多个可迭代的对象）中的各个元素上做计算，然后返回结果。 模块 函数 说明 itertools accumulate(it, [func]) 产出累积的总和；如果提供了 func，那么把前两个元素传给它，然后把计算结果和下一个元素传给它，以此类推，最后产出结果 （内置） enumerate(iterable, start=0) 产出由两个元素组成的元组，结构是 (index, item) ，其中 index 从 start 开始计数，item 则从 iterable 中获取 内置 map(func, it1, [it2, …, itN]) 把 it 中的各个元素传给func，产出结果；如果传入 N 个可迭代的对象，那么 func 必须能接受 N 个参数，而且要并行处理各个可迭代的对象 itertools starmap(func, it) 把 it 中的各个元素传给 func，产出结果；输入的可迭代对象应该产出可迭代的元素 iit，然后以 func(*iit) 这种形式调用 func 1234567891011121314151617181920In [11]: sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]In [12]: import itertoolsIn [13]: list(itertools.accumulate(sample))Out[13]: [5, 9, 11, 19, 26, 32, 35, 35, 44, 45]In [14]: list(itertools.accumulate(sample, min))Out[14]: [5, 4, 2, 2, 2, 2, 2, 0, 0, 0]In [15]: list(itertools.accumulate(sample, max))Out[15]: [5, 5, 5, 8, 8, 8, 8, 8, 9, 9]In [16]: import operatorIn [17]: list(itertools.accumulate(sample, operator.mul))Out[17]: [5, 20, 40, 320, 2240, 13440, 40320, 0, 0, 0]In [18]: list(itertools.starmap(operator.mul, enumerate('albatroz', 1)))Out[18]: ['a', 'll', 'bbb', 'aaaa', 'ttttt', 'rrrrrr', 'ooooooo', 'zzzzzzzz'] 合并接下来这一组是用于合并的生成器函数，这些函数都从输入的多个可迭代对象中产出元素。 模块 函数 说明 itertools chain(it1, …, itN) 先产出 it1 中的所有元素，然后产出 it2 中的所有元素，以此类推，无缝连接在一起 itertools chain.from_iterable(it) 产出 it 生成的各个可迭代对象中的元素，一个接一个，无缝连接在一起；it 应该产出可迭代的元素，例如可迭代的对象列表 itertools product(it1, …, itN, repeat=1) 计算笛卡儿积：从输入的各个可迭代对象中获取元素，合并成由 N 个元素组成的元组，与嵌套的 for 循环效果一样；repeat 指明重复处理多少次输入的可迭代对象 （内置） zip(it1, …, itN) 并行从输入的各个可迭代对象中获取元素，产出由 N 个元素组成的元组，只要有一个可迭代的对象到头了，就默默地停止 itertools zip_longest(it1, …, itN, fillvalue=None) 并行从输入的各个可迭代对象中获取元素，产出由 N 个元素组成的元组，等到最长的可迭代对象到头后才停止，空缺的值使用 fillvalue 填充 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879In [1]: import itertoolsIn [2]: list(itertools.chain('ABC', range(2)))Out[2]: ['A', 'B', 'C', 0, 1]In [3]: list(itertools.chain(enumerate('ABC')))Out[3]: [(0, 'A'), (1, 'B'), (2, 'C')]In [4]: list(itertools.chain.from_iterable(enumerate('ABC')))Out[4]: [0, 'A', 1, 'B', 2, 'C']In [5]: list(zip('ABC', range(5)))Out[5]: [('A', 0), ('B', 1), ('C', 2)]In [6]: list(zip('ABC', range(5), [10, 20, 30, 40]))Out[6]: [('A', 0, 10), ('B', 1, 20), ('C', 2, 30)]In [8]: list(itertools.zip_longest('ABC', range(5)))Out[8]: [('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)]In [9]: list(itertools.zip_longest('ABC', range(5), fillvalue='?'))Out[9]: [('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)]In [10]: list(itertools.product('ABC', range(2)))Out[10]: [('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)]In [11]: list(itertools.product('ABC'))Out[11]: [('A',), ('B',), ('C',)]In [12]: list(itertools.product('ABC', repeat=2))Out[12]:[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]In [13]: list(itertools.product('ABC', range(2), repeat=2))Out[13]:[('A', 0, 'A', 0), ('A', 0, 'A', 1), ('A', 0, 'B', 0), ('A', 0, 'B', 1), ('A', 0, 'C', 0), ('A', 0, 'C', 1), ('A', 1, 'A', 0), ('A', 1, 'A', 1), ('A', 1, 'B', 0), ('A', 1, 'B', 1), ('A', 1, 'C', 0), ('A', 1, 'C', 1), ('B', 0, 'A', 0), ('B', 0, 'A', 1), ('B', 0, 'B', 0), ('B', 0, 'B', 1), ('B', 0, 'C', 0), ('B', 0, 'C', 1), ('B', 1, 'A', 0), ('B', 1, 'A', 1), ('B', 1, 'B', 0), ('B', 1, 'B', 1), ('B', 1, 'C', 0), ('B', 1, 'C', 1), ('C', 0, 'A', 0), ('C', 0, 'A', 1), ('C', 0, 'B', 0), ('C', 0, 'B', 1), ('C', 0, 'C', 0), ('C', 0, 'C', 1), ('C', 1, 'A', 0), ('C', 1, 'A', 1), ('C', 1, 'B', 0), ('C', 1, 'B', 1), ('C', 1, 'C', 0), ('C', 1, 'C', 1)] 扩展有些生成器函数会从一个元素中产出多个值，扩展输入的可迭代对象。 模块 函数 说明 itertools combinations(it, out_len) 把 it 产出的 out_len 个元素组合在一起，然后产出 itertools combinations_with_replacement(it, out_len) 把 it 产出的 out_len 个元素组合在一起，然后产出，包含相同元素的组合 itertools count(start=0, step=1) 从 start 开始不断产出数字，按 step 指定的步幅增加 itertools cycle(it) 从 it 中产出各个元素，存储各个元素的副本，然后按顺序重复不断地产出各个元素 itertools permutations(it, out_len=None) 把 out_len 个 it 产出的元素排列在一起，然后产出这些排列；out_len 的默认值等于 len(list(it)) itertools repeat(item, [times]) 重复不断地产出指定的元素，除非提供 times，指定次数 12345678910In [1]: import itertoolsIn [2]: list(itertools.combinations('ABC', 2))Out[2]: [('A', 'B'), ('A', 'C'), ('B', 'C')]In [3]: list(itertools.combinations_with_replacement('ABC', 2))Out[3]: [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]In [4]: list(itertools.permutations('ABC', 2))Out[4]: [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] 重排列 模块 函数 说明 itertools groupby(it, key=None) 产出由两个元素组成的元素，形式为 (key, group) ，其中 key 是分组标准，group 是生成器，用于产出分组里的元素 （内置） reversed(seq) 从后向前，倒序产出 seq 中的元素；seq 必须是序 列，或者是实现了 __reversed__ 特殊方法的对象 itertools tee(it, n=2) 产出一个由 n 个生成器组成的元组，每个生成器 用于单独产出输入的可迭代对象中的元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354In [1]: import itertoolsIn [2]: list(itertools.combinations('ABC', 2))Out[2]: [('A', 'B'), ('A', 'C'), ('B', 'C')]In [3]: list(itertools.combinations_with_replacement('ABC', 2))Out[3]: [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]In [4]: list(itertools.permutations('ABC', 2))Out[4]: [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]In [5]: import itertoolsIn [6]: list(itertools.groupby('LLLLAAGGG'))Out[6]:[('L', &lt;itertools._grouper at 0x10eaaf7b8&gt;), ('A', &lt;itertools._grouper at 0x10eaaf7f0&gt;), ('G', &lt;itertools._grouper at 0x10eaaf8d0&gt;)]In [7]: for char, group in itertools.groupby('LLLLAAGGG'): ...: print(char, '-&gt;', list(group)) ...:L -&gt; ['L', 'L', 'L', 'L']A -&gt; ['A', 'A']G -&gt; ['G', 'G', 'G']In [8]: animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear', 'bat', 'dolphin', 'shark', 'lion']In [9]: animals.sort(key=len)In [10]: animalsOut[10]: ['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark', 'giraffe', 'dolphin']In [11]: for length, group in itertools.groupby(animals, len): ...: print(length, '-&gt;', list(group)) ...:3 -&gt; ['rat', 'bat']4 -&gt; ['duck', 'bear', 'lion']5 -&gt; ['eagle', 'shark']7 -&gt; ['giraffe', 'dolphin']In [12]: for length, group in itertools.groupby(reversed(animals), len): ...: print(length, '-&gt;', list(group)) ...:7 -&gt; ['dolphin', 'giraffe']5 -&gt; ['shark', 'eagle']4 -&gt; ['lion', 'bear', 'duck']3 -&gt; ['bat', 'rat']In [15]: list(zip(*itertools.tee('ABC')))Out[15]: [('A', 'A'), ('B', 'B'), ('C', 'C')]In [16]: list(zip(*itertools.tee('ABC', 3)))Out[16]: [('A', 'A', 'A'), ('B', 'B', 'B'), ('C', 'C', 'C')] 14.10 Python 3.3中新出现的句法：yield from如果生成器函数需要产出另一个生成器生成的值，传统的解决方法是使用嵌套的 for 循环，例如下面自己实现的 chain 的例子： 1234def chain(*iterables): for it in iterables: for i in it: yield i 在 PEP380 中引入了一个新的句法：yield from，它能够替代我们内层的 for 循环： 123def chain(*iterables): for it in iterables: yield from it yield from 不仅仅是语法糖，其还会创建通道，把内层生成器直接与外层生成器的客户端联系起来。把生成器当成协程使用时，这个通道特别重要，不仅能为客户端代码生成值，还能使用客户端代码提供的值。 14.12 深入分析 iter 函数如前所述，在 Python 中迭代对象 x 时会调用 iter(x)。 可是，iter 函数还有一个鲜为人知的用法：传入两个参数，使用常规的函数或任何可调用的对象创建迭代器。这样使用时，第一个参数必须是可调用的对象，用于不断调用（没有参数），产出各个值；第二个值是哨符，这是个标记值，当可调用的对象返回这个值时，触发迭代器抛出 StopIteration 异常，而不产出哨符。 12345678910111213141516171819202122232425In [21]: def d6(): ...: return randint(1, 6) ...:In [22]:In [22]: def d6(): ...: return randint(1, 6) ...:In [23]: d6_iter = iter(d6, 1)In [24]: d6_iterOut[24]: &lt;callable_iterator at 0x10ee55b70&gt;In [25]: for roll in iter(d6, 1): ...: print(roll) ...:4255645 内置函数 iter 的文档中有个实用的例子。这段代码逐行读取文件，直到遇到空行或者到达文件末尾为止： 123with open('mydata.txt') as fp: for line in iter(fp.readline, '\\n'): process_line(line)","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"思维导图与高效学习","slug":"effective-way-mind-map","date":"2019-08-16T17:20:13.000Z","updated":"2019-08-20T13:36:55.996Z","comments":true,"path":"2019/08/17/effective-way-mind-map/","link":"","permalink":"https://homholueng.github.io/2019/08/17/effective-way-mind-map/","excerpt":"","text":"相信我们在生活中会经常使用思维导图来进行规划，总结，事项整理等，思维导图由 Tony Buzan 创建，是一种风靡全球的工具。可用于生活，和工作中；他能够辅助我们思考，发散，并将思维和逻辑具象化和形式化；本文是《思维导图与高效学习术》课程的总结，所以，本文也会尝试使用思维导图来进行课程知识的总结和归纳。 思维导图的特点 C：从一张纸的中心开始思考 R：放射状往外有层次逻辑扩散（先放后收） O：采用关键词重点思考技术 C：运用图像与颜色让思考活络容易记忆 思维导图的形式 全图思维导图：由图像和线条组成，适合用在吸引眼球以及内容需要被记住的场景；但是需要注意，一般需要有人解说，并且需要控制内容的数量。 全文思维导图：由文字和线条组成，适合用在信息量多，重点需要被精准传递的场景；职场上用的一般比较多。 图文思维导图：有图像，文字与线条组成，既能透过文字传达内容，又能运用图像表示出重点与亮点；是目前全球最多人使用的形式。 思维导图的结构 中心主题：中心主题（Central Image），位于导图的中央，代表导图的题目；主题能够帮助我们防止思考跑题。 主干：从中心主题延伸出来的第一层线条，代表大项，是大方向的思考，称之为主干（Main Branch）；辅助大脑快速定位到思考的大方向。 支干：在主干后面所有细细的线条是第三个结构：支干（Sub-Branch），支干上方的信息是经过整理的重要细节内容。 思维导图的要素 文字：传递信息，表达重点；一般使用的都是关键字。 线条：穿针引线，组织架构；呈现思维的脉络。 图像：强调重点，带动情感；吸引人的注意力。 思维导图的应用思维导图能够用于很多场景，但我们可以将其划分为两大类别： 输入（Taking）：从外界撷取信息到大脑中。 笔记 会议 谈话摘录 整理想法 输出（Making）：组织运用自己大脑内的想法及数据。 工作规划 汇报构思 头脑风暴 问题分析 思维导图思考技术思维导图在使用时有两个比较关键的思考技术：Key Word 及 BOIs，能够帮助我们尽可能的发挥思维导图的价值。 Key Word 技术关键词可以勾起我们大脑想到关键词所代表的意义、相关重要内容、当时学习时的情景等。关键词通常是名词或动词；以一个字、词或凝聚概念为主。 通过拆分关键词技术，在思维导图中我们能够创建更多的活口，因为每一个关键词都是思考的起点与刺激点。也就是说，关键词技术的价值是在思考时，帮助我们快速提升想法的数量，扩增想法的广度。 而在输入和输出的应用上，关键词拆解的程度有所不同，在输出时，想法的数量会影响最终收敛结果的品质，所以请严格遵守一个关键词，也就是一词一线元组；再输入的应用上，可以视情况而定给出弹性，对于没有保留活口的整理需求，可以适度的结合关键词，也就是弹性使用一线二词。 下面给出一个拆解关键词的实例，假设我们需要给公司的老客户进行一些回馈，而其中一项是赠送生日礼物，那么通过关键字拆解技术，我们能够拆解成礼物和生日两个关键字，这使得我们留出了一个思考的活口和空间，即在礼物和生日之间的一个思考空间，通过这个思考空间，我们能够发散出生日礼物，情人节礼物，春节礼物等更多的细项目： BOIs 系统思考BOIs 是 Basic Ordering Ideas 的缩写，透过加入更细的分类阶层，让想法更全更周延，其最大的价值在找出遗漏的思维，从跳脱信息接收与梳理的层次，更进一步主动思考其他的可能性，让整个思考过程更完整。 BOIs 的快速启动步骤如下： 想法出现之后，先找大类，即我们的主干，这被称之为主干技术。 思考大类与想法之间是否有更细的功能性位阶。 锻炼 BOIs 技术的方法是：先画一条空白线。因为大脑追求完成的倾向，会让我们想要填满它，进而刺激我们去思考。 这个技术能够帮助我们提升思维的完整性和缜密度，但是要注意，在运用时不要将小事搞得过于复杂，我们应该积极锻炼我们的思考能力，但是在应用时要视场景而定，避免用力过猛。 下面给出一个 BOIs 的实例，在我们上一节使用 Key Word 技术拆分的基础上，我们进行进一步的思考。首先，我们在礼物（大类）与生日（想法）之间画出一条白线，然后填满它，这就是我们的第一个功能性位阶段，最后，我们会通过这条白线的刺激，发散出其他更多的位阶（节日，对象，种类，产品等）： 最佳实践一般我们会按照以下步骤来绘制思维导图： 定中心主题 内容整理分类 重点加图像 应用思维导图整理思维导图笔记4步骤： 快速浏览，区分大类 细读内容，圈关键词 组织架构，画思维导图 检查，说图（根据导图向自己或别人讲述笔记的内容） 其他要点 根据思维导图面向的对象不同，我们绘制思维导图的风格可能也不一样；例如在面向自己的情况下（笔记，总结），我们可能会以我们自己的思维方式和逻辑来进行绘制，且绘制风格和用词可能会更加偏向自己；而在面向他人的情况下，我们可能需要考虑图的形式（图文，全图，全文），用词和用语，便于他人快速理解。 在大量的信息面前，思维导图能够帮助我们将信息进行压缩，并提升对信息的价值。 实践下面是通过使用思维导图及相关关键技术对课程的内容进行的归纳和总结：","categories":[{"name":"Effective Way","slug":"Effective-Way","permalink":"https://homholueng.github.io/categories/Effective-Way/"}],"tags":[{"name":"Effective","slug":"Effective","permalink":"https://homholueng.github.io/tags/Effective/"},{"name":"Mind Map","slug":"Mind-Map","permalink":"https://homholueng.github.io/tags/Mind-Map/"}]},{"title":"Redis 使用技巧 - Pipelining","slug":"redis-skill-pipelining","date":"2019-08-11T12:25:35.000Z","updated":"2019-08-20T13:36:55.995Z","comments":true,"path":"2019/08/11/redis-skill-pipelining/","link":"","permalink":"https://homholueng.github.io/2019/08/11/redis-skill-pipelining/","excerpt":"","text":"请求/相应以及 RTTRedis 是基于 CS 模型的 TCP 服务器，所以我们每次操作都会遵循如下步骤完成： 客户端将查询请求发送给服务器，服务器中套接字中读取客户端的请求，客户端会等待服务器的相应，而这个过程通常是阻塞的。 服务器处理完成后，将响应返回给客户端。 假设客户端需要向服务器发送四条请求，那么请求和响应顺序应该是这样的： Client: INCR X Server: 1 Client: INCR X Server: 2 Client: INCR X Server: 3 Client: INCR X Server: 4 从客户端发起请求的那一刻开始计算，直到客户端收到来自服务器的响应，这中间消耗的时间我们称其为 RTT（Round Trip Time）。当客户端需要在一行（例如往同一个列表中添加多个元素，或是操作同一个数据库中的多个 key 值）上进行多次操作时，RTT 会对性能造成不小的影响。假设我们拥有一台性能强大的服务器，这台服务器能够在一秒内处理 100K 个请求，那么在 RTT 为 250 毫秒的情况下，这个性能强大的服务器一秒内也只能处理 4 个请求。 Redis PipeliningRedis 提供的 Pipelining 特性允许我们将多个请求一次性发送给服务器，然后再将所有的响应一次性读取回来。 1234$ (printf \"PING\\r\\nPING\\r\\nPING\\r\\n\"; sleep 1) | nc localhost 6379+PONG+PONG+PONG 以上小节的例子来看，使用的 Pipelining 之后，请求和相应的顺序应该是这样的(注意，多条命令是合并在一个请求中发送出去的，处理结果也是在一个响应中返回的)： Client: INCR X Client: INCR X Client: INCR X Client: INCR X Server: 1 Server: 2 Server: 3 Server: 4 如果客户端使用了 Pipelining 这个特性，那么服务器就必须将响应的内容加入到队列中，直到所有操作都处理完成后再一次性返回。当然，这个队列是在内存中的，如果你需要使用 Pipelining 来执行大量的操作，最好是将这些操作进行分批 Pipelining，避免服务器在存储响应内容时消耗过多的内存。 其实不仅仅是 RTT 的问题Pipelining 这个特性其实不仅仅是解决了 RTT 开销的问题，还防止了过多 socket I/O 带来的消耗，因为多条操作指令能够在一次 read() 系统调用中读取出来，多条响应内容也能够在一次 write() 系统调用中完成写入。 从官方文档给出的资料可以看出，随着 Pipelining 指令数量的增长，服务器的 qps 也成近似指数关系的增长，最后稳定在大概十倍的位置： 实际测试下面是一段使用 Python 代码进行的测试： 123456789101112131415161718192021222324import redisimport timeTIMES = 10000def bench(func, desc): start = time.time() func() print('&#123;desc&#125; &#123;cost&#125; milliseconds'.format(desc=desc, cost=time.time() - start)) def without_pipeling(): r = redis.Redis(host='localhost', port=6379) for _ in range(TIMES): r.ping()def with_pipelining(): r = redis.Redis(host='localhost', port=6379) with r.pipeline(transaction=False) as p: for _ in range(TIMES): p.ping()if __name__ == '__main__': bench(without_pipeling, 'without_pipeling') bench(with_pipelining, 'with_pipelining') 测试结果如下： 12without_pipeling 1.24247217178 millisecondswith_pipelining 0.0302491188049 milliseconds","categories":[{"name":"Redis","slug":"Redis","permalink":"https://homholueng.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://homholueng.github.io/tags/Redis/"}]},{"title":"Chapter 13 - 正确的重载运算符","slug":"fluent-python-override-op-in-right-way","date":"2019-08-10T16:38:36.000Z","updated":"2019-08-20T13:36:55.995Z","comments":true,"path":"2019/08/11/fluent-python-override-op-in-right-way/","link":"","permalink":"https://homholueng.github.io/2019/08/11/fluent-python-override-op-in-right-way/","excerpt":"","text":"13.1 运算符重载基础Python 在重载运算符上施加了一些限制，做好了灵活性、可用性和安全性的平衡： 不能重载内置类型的运算符 不能新建运算符，只能重载现有的 某些运算符不能重载：is，and，or 和 not 13.2 一元运算符支持一元运算符很简单，只需实现相应的特殊方法。这些特殊方法只有一个参数，self。然后，使用符合所在类的逻辑实现。不过，要遵守运算符的一个基本规则：始终返回一个新对象。也就是说，不能修改 self，要创建并返回合适类型的新实例。 为了支持涉及不同类型的运算，Python 为中缀运算符特殊方法提供了特殊的分派机制。对表达式 a + b 来说，解释器会执行以下几步操作： 如果 a 有 __add__ 方法，而且返回值不是 NotImplemented，调用 a.__add__(b)，然后返回结果。 如果 a 没有 __add__ 方法，或者调用 __add__ 方法返回 NotImplemented，检查 b 有没有 __radd__ 方法，如果有，而且没有 返回 NotImplemented，调用 b.__radd__(a)，然后返回结果。 如果 b 没有 __radd__ 方法，或者调用 __radd__ 方法返回 NotImplemented，抛出 TypeError，并在错误消息中指明操作数类型 不支持。 在重载运算符时，如果我们在进行处理的过程中抛出了异常，那么我们不应该直接让该异常抛出，因为这样有可能阻断 Python 继续尝试调用其他运算符的流程，所以我们应该将错误捕获，并返回 Notimplemented： 123456def __add__(self, other): try: pairs = itertools.zip_longest(self, other, fillvalue=0.0) return Vector(a + b for a, b in pairs) except TypeError: return NotImplemented 如果操作数的类型不同，我们要检测出不能处理的操作数。可以使用两种方式处理这个问题：一种是鸭子类型，直接尝试执行运算，如果有问题，捕获 TypeError 异常；另一种是显式使用 isinstance 测试，但是不能测试具体类，而要测试例如 numbers.Real 这样的抽象基类。 13.6 增量赋值运算符如果一个类没有实现就地运算符，增量赋值运算符只是语法糖：a += b 的作用与 a = a + b 完全一样。 然而，如果实现了就地运算符方法，例如 __iadd__，计算 a += b 的结果时会调用就地运算符方法。这种运算符的名称表明，它们会就地修改左操作数，而不会创建新对象作为结果。 对于不可变类型，一定不能实现就地特殊方法。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Go 语言圣经 - 6. 方法","slug":"gopl-method","date":"2019-08-07T12:02:28.000Z","updated":"2019-08-07T12:02:28.177Z","comments":true,"path":"2019/08/07/gopl-method/","link":"","permalink":"https://homholueng.github.io/2019/08/07/gopl-method/","excerpt":"","text":"6.1. 方法声明在函数声明时，在其名字之前放上一个变量，即是一个方法。这个附加的参数会将该函数附加到这种类型上，即相当于为这种类型定义了一个独占的方法。 123456789101112131415package geometryimport \"math\"type Point struct&#123; X, Y float64 &#125;// traditional functionfunc Distance(p, q Point) float64 &#123; return math.Hypot(q.X-p.X, q.Y-p.Y)&#125;// same thing, but as a method of the Point typefunc (p Point) Distance(q Point) float64 &#123; return math.Hypot(q.X-p.X, q.Y-p.Y)&#125; 上面的代码里那个附加的参数 p，叫做方法的接收器（receiver），早期的面向对象语言留下的遗产将调用一个方法称为 “向一个对象发送消息”。 在 Go 语言中，我们并不会像其它语言那样用 this 或者 self 作为接收器；我们可以任意的选择接收器的名字。由于接收器的名字经常会被使用到，所以保持其在方法间传递时的一致性和简短性是不错的主意。这里的建议是可以使用其类型的第一个字母，比如这里使用了 Point 的首字母 p。 而每种类型都有其方法的命名空间，让我们来定义一个 Path 类型，这个 Path 代表一个线段的集合，并且也给这个 Path 定义一个叫 Distance 的方法。 1234567891011type path []Pointfunc (path Path) Distance() float64 &#123; sum := 0.0 for i := range path &#123; if i &gt; 0 &#123; sum += path[i-1].Distance(path[i]) &#125; &#125; return sum&#125; 在能够给任意类型定义方法这一点上，Go 和很多其它的面向对象的语言不太一样。因此在 Go 语言里，我们为一些简单的数值、字符串、slice、map 来定义一些附加行为很方便。我们可以给同一个包内的任意命名类型定义方法，只要这个命名类型的底层类型不是指针或者 interface。 6.2. 基于指针对象的方法当调用一个函数时，会对其每一个参数值进行拷贝，如果一个函数需要更新一个变量，或者函数的其中一个参数实在太大我们希望能够避免进行这种默认的拷贝，这种情况下我们就需要用到指针了。对应到我们这里用来更新接收器的对象的方法，当这个接受者变量本身比较大时，我们就可以用其指针而不是对象来声明方法，如下： 1234func (p *Point) ScaleBy(factor float64) &#123; p.X *= factor p.Y *= factor&#125; 这个方法的名字是 (*Point).ScaleBy。在现实的程序里，一般会约定如果 Point 这个类有一个指针作为接收器的方法，那么所有 Point 的方法都必须有一个指针接收器，即使是那些并不需要这个指针接收器的函数。 只有类型（Point）和指向他们的指针 (*Point)，才可能是出现在接收器声明里的两种接收器。此外，为了避免歧义，在声明方法时，如果一个类型名本身是一个指针的话，是不允许其出现在接收器中的，比如下面这个例子： 12type P *intfunc (P) f() &#123; /* ... */ &#125; // compile error: invalid receiver type 想要调用指针类型方法 (*Point).ScaleBy，只要提供一个 Point 类型的指针即可，像下面这样。 123456789r := &amp;Point&#123;1, 2&#125;r.ScaleBy(2)p := Point&#123;1, 2&#125;pptr := &amp;ppptr.ScaleBy(2)p := Point&#123;1, 2&#125;(&amp;p).ScaleBy(2) 但是，go 语言本身在这种地方会帮到我们。如果接收器 p 是一个 Point 类型的变量，并且其方法需要一个 Point 指针作为接收器，我们可以用下面这种简短的写法： 1p.ScaleBy(2) 编译器会隐式地帮我们用 &amp;p 去调用 ScaleBy 这个方法。这种简写方法只适用于 “变量”，包括 struct 里的字段比如 p.X，以及 array 和 slice 内的元素比如 perim[0]。我们不能通过一个无法取到地址的接收器来调用指针方法，比如临时变量的内存地址就无法获取得到： 1Point&#123;1, 2&#125;.ScaleBy(2) // compile error: can't take address of Point literal go 的编译器会隐式为我们 解引用 或 取地址。、 6.2.1. Nil 也是一个合法的接收器类型就像一些函数允许 nil 指针作为参数一样，方法理论上也可以用 nil 指针作为其接收器，尤其当 nil 对于对象来说是合法的零值时，比如 map 或者 slice。 当你定义一个允许 nil 作为接收器值的方法的类型时，在类型前面的注释中指出 nil 变量代表的意义是很有必要的。 6.3. 通过嵌入结构体来扩展类型来看看 ColoredPoint 这个类型： 12345678import \"image/color\"type Point struct&#123; X, Y float64 &#125;type ColoredPoint struct &#123; Point Color color.RGBA&#125; 我们通过类型为 Point 的匿名成员将 Point 类的方法也引入了 ColoredPoint 中。这种方式可以使我们定义字段特别多的复杂类型，我们可以将字段先按小类型分组，然后定义小类型的方法，之后再把它们组合起来。 在类型中内嵌的匿名字段也可能是一个命名类型的指针，这种情况下字段和方法会被间接地引入到当前的类型中。 方法只能在命名类型（像 Point）或者指向类型的指针上定义，但是多亏了内嵌，有些时候我们给匿名 struct 类型来定义方法也有了手段。 下面是一个小 trick。这个例子展示了简单的 cache： 1234567891011121314var cache = struct &#123; sync.Mutex mapping map[string]string&#125;&#123; mapping: make(map[string]string),&#125;func Lookup(key string) string &#123; cache.Lock() v := cache.mapping[key] cache.Unlock() return v&#125; 6.4. 方法值和方法表达式我们经常选择一个方法，并且在同一个表达式里执行，比如常见的 p.Distance() 形式，实际上将其分成两步来执行也是可能的。 1234567p := Point&#123;1, 2&#125;q := Point&#123;4, 6&#125;distanceFromP := p.Distance // method valuefmt.Println(distanceFromP(q)) // \"5\"var origin Point // &#123;0, 0&#125;fmt.Println(distanceFromP(origin)) // \"2.23606797749979\", sqrt(5) 我们将 distanceFromP 称为 Distance 绑定了接受者 p 上的方法值。 和方法 “值” 相关的还有方法表达式： 12345p := Point&#123;1, 2&#125;q := Point&#123;4, 6&#125;distance := Point.Distance // method expressionfmt.Println(distance(p, q)) // \"5\" 6.6. 封装封装提供了三方面的优点。首先，因为调用方不能直接修改对象的变量值，其只需要关注少量的语句并且只要弄懂少量变量的可能的值即可。 第二，隐藏实现的细节，可以防止调用方依赖那些可能变化的具体实现，这样使设计包的程序员在不破坏对外的 api 情况下能得到更大的自由。 把 bytes.Buffer 这个类型作为例子来考虑。这个类型在做短字符串叠加的时候很常用，所以在设计的时候可以做一些预先的优化，比如提前预留一部分空间，来避免反复的内存分配。又因为 Buffer 是一个 struct 类型，这些额外的空间可以用附加的字节数组来保存，且放在一个小写字母开头的字段中。这样在外部的调用方只能看到性能的提升，但并不会看得到这个附加变量。 12345type Buffer struct &#123; buf []byte initial [64]byte /* ... */&#125; 封装的第三个优点也是最重要的优点，是阻止了外部调用方对对象内部的值任意地进行修改。 封装并不总是理想的。 虽然封装在有些情况是必要的，但有时候我们也需要暴露一些内部内容，比如：time.Duration 将其表现暴露为一个 int64 数字的纳秒，使得我们可以用一般的数值操作来对时间进行对比，甚至可以定义这种类型的常量： 12const day = 24 * time.Hourfmt.Println(day.Seconds()) // \"86400\"","categories":[{"name":"Go 语言圣经","slug":"Go-语言圣经","permalink":"https://homholueng.github.io/categories/Go-语言圣经/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://homholueng.github.io/tags/Go/"}]},{"title":"Go 语言圣经 - 5. 函数","slug":"gopl-function","date":"2019-08-05T13:33:25.000Z","updated":"2019-08-05T13:33:25.129Z","comments":true,"path":"2019/08/05/gopl-function/","link":"","permalink":"https://homholueng.github.io/2019/08/05/gopl-function/","excerpt":"","text":"5.1. 函数声明函数声明包括函数名、形式参数列表、返回值列表（可省略）以及函数体。 123func name(parameter-list) (result-list) &#123; body&#125; 返回值也可以像形式参数一样被命名。在这种情况下，每个返回值被声明成一个局部变量，并根据该返回值的类型，将其初始化为该类型的零值。 如果一个函数在声明时，包含返回值列表，该函数必须以 return 语句结尾，除非函数明显无法运行到结尾处。 如果一组形参或返回值有相同的类型，我们不必为每个形参都写出参数类型。下面 2 个声明是等价的： 12func f(i, j, k int, s, t string) &#123; /* ... */ &#125;func f(i int, j int, k int, s string, t string) &#123; /* ... */ &#125; 函数的类型被称为函数的签名。如果两个函数形式参数列表和返回值列表中的变量类型一一对应，那么这两个函数被认为有相同的类型或签名。形参和返回值的变量名不影响函数签名，也不影响它们是否可以以省略参数类型的形式表示。 实参通过值的方式传递，因此函数的形参是实参的拷贝。对形参进行修改不会影响实参。但是，如果实参包括引用类型，如指针，slice (切片)、map、function、channel 等类型，实参可能会由于函数的间接引用被修改。 你可能会偶尔遇到没有函数体的函数声明，这表示该函数不是以 Go 实现的。这样的声明定义了函数签名。 123package mathfunc Sin(x float64) float //implemented in assembly language 5.3. 多返回值调用多返回值函数时，返回给调用者的是一组值，调用者必须显式的将这些值分配给变量: 1links, err := findLinks(url) 如果某个值不被使用，可以将其分配给 blank identifier: 1links, _ := findLinks(url) // errors ignored 当你调用接受多参数的函数时，可以将一个返回多参数的函数调用作为该函数的参数。虽然这很少出现在实际生产代码中，但这个特性在 debug 时很方便，我们只需要一条语句就可以输出所有的返回值。下面的代码是等价的： 123log.Println(findLinks(url))links, err := findLinks(url)log.Println(links, err) 如果一个函数所有的返回值都有显式的变量名，那么该函数的 return 语句可以省略操作数。这称之为 bare return。 当一个函数有多处 return 语句以及许多返回值时，bare return 可以减少代码的重复，但是使得代码难以被理解。 5.4. 错误通常，导致失败的原因不止一种，尤其是对 I/O 操作而言，用户需要了解更多的错误信息。因此，额外的返回值不再是简单的布尔类型，而是 error 类型，内置的 error 是接口类型。 通常，当函数返回 non-nil 的 error 时，其他的返回值是未定义的（undefined），这些未定义的返回值应该被忽略。然而，有少部分函数在发生错误时，仍然会返回一些有用的返回值。比如，当读取文件发生错误时，Read 函数会返回可以读取的字节数以及错误信息。 5.4.1. 错误处理策略当一次函数调用返回错误时，调用者应该选择合适的方式处理错误。根据情况的不同，有很多处理方式，让我们来看看常用的五种方式。 1首先，也是最常用的方式是传播错误。这意味着函数中某个子程序的失败，会变成该函数的失败。 1234resp, err := http.Get(url)if err != nil &#123; return nil, err&#125; 当对 html.Parse 的调用失败时，findLinks 不会直接返回 html.Parse 的错误，因为缺少两条重要信息：1、发生错误时的解析器（html parser）；2、发生错误的 url。因此，findLinks 构造了一个新的错误信息，既包含了这两项，也包括了底层的解析出错的信息。 12345doc, err := html.Parse(resp.Body)resp.Body.Close()if err != nil &#123; return nil, fmt.Errorf(\"parsing %s as HTML: %v\", url,err)&#125; 我们使用该函数添加额外的前缀上下文信息到原始错误信息。当错误最终由 main 函数处理时，错误信息应提供清晰的从原因到后果的因果链，就像美国宇航局事故调查时做的那样： 1genesis: crashed: no parachute: G-switch failed: bad relay orientation 由于错误信息经常是以链式组合在一起的，所以错误信息中应避免大写和换行符。 编写错误信息时，我们要确保错误信息对问题细节的描述是详尽的。尤其是要注意错误信息表达的一致性，即相同的函数或同包内的同一组函数返回的错误在构成和处理方式上是相似的。 以 os 包为例，os 包确保文件操作（如 os.Open、Read、Write、Close）返回的每个错误的描述不仅仅包含错误的原因（如无权限，文件目录不存在）也包含文件名，这样调用者在构造新的错误信息时无需再添加这些信息。 2如果错误的发生是偶然性的，或由不可预知的问题导致的。一个明智的选择是重新尝试失败的操作。在重试时，我们需要限制重试的时间间隔或重试的次数，防止无限制的重试。 3如果错误发生后，程序无法继续运行，我们就可以采用第三种策略：输出错误信息并结束程序。需要注意的是，这种策略只应在 main 中执行。 12345// (In function main.)if err := WaitForServer(url); err != nil &#123; fmt.Fprintf(os.Stderr, \"Site is down: %v\\n\", err) os.Exit(1)&#125; 调用 log.Fatalf 可以更简洁的代码达到与上文相同的效果。log 中的所有函数，都默认会在错误信息之前输出时间信息。 123if err := WaitForServer(url); err != nil &#123; log.Fatalf(\"Site is down: %v\\n\", err)&#125; 4第四种策略：有时，我们只需要输出错误信息就足够了，不需要中断程序的运行。我们可以通过 log 包提供函数 123if err := Ping(); err != nil &#123; log.Printf(\"ping failed: %v; networking disabled\", err)&#125; 或者标准错误流输出错误信息。 123if err := Ping(); err != nil &#123; fmt.Fprintf(os.Stderr, \"ping failed: %v; networking disabled\\n\", err)&#125; log 包中的所有函数会为没有换行符的字符串增加换行符。 5第五种，也是最后一种策略：我们可以直接忽略掉错误。 5.4.2. 文件结尾错误（EOF）io 包保证任何由文件结束引起的读取失败都返回同一个错误 —— io.EOF，该错误在 io 包中定义： 123456package ioimport &quot;errors&quot;// EOF is the error returned by Read when no more input is available.var EOF = errors.New(&quot;EOF&quot;) 5.5. 函数值在 Go 中，函数被看作第一类值（first-class values）：函数像其他值一样，拥有类型，可以被赋值给其他变量，传递给函数，从函数返回。 123456789101112func square(n int) int &#123; return n * n &#125;func negative(n int) int &#123; return -n &#125;func product(m, n int) int &#123; return m * n &#125;f := squarefmt.Println(f(3)) // \"9\"f = negativefmt.Println(f(3)) // \"-3\"fmt.Printf(\"%T\\n\", f) // \"func(int) int\"f = product // compile error: can't assign func(int, int) int to func(int) int 函数类型的零值是 nil，函数值可以与 nil 比较。 但是函数值之间是不可比较的，也不能用函数值作为 map 的 key。 5.6. 匿名函数拥有函数名的函数只能在包级语法块中被声明，通过函数字面量（function literal），我们可绕过这一限制，在任何表达式中表示一个函数值。 函数字面量允许我们在使用函数时，再定义它。通过这种技巧，我们可以改写之前对 strings.Map 的调用： 1strings.Map(func(r rune) rune &#123;return r + 1&#125;, \"HAL-9000\") 更为重要的是，通过这种方式定义的函数可以访问完整的词法环境（lexical environment），这意味着在函数中定义的内部函数可以引用该函数的变量，如下例所示： 1234567891011121314func squares() func() int &#123; var x int return func() int &#123; x++ return x * x &#125;&#125;func main() &#123; f := squares() fmt.Println(f()) // \"1\" fmt.Println(f()) // \"4\" fmt.Println(f()) // \"9\" fmt.Println(f()) // \"16\"&#125; squares 的例子证明，函数值不仅仅是一串代码，还记录了状态。在 squares 中定义的匿名内部函数可以访问和更新 squares 中的局部变量，这意味着匿名函数和 squares 中，存在变量引用。这就是函数值属于引用类型和函数值不可比较的原因。Go 使用闭包（closures）技术实现函数值，Go 程序员也把函数值叫做闭包。 当匿名函数需要被递归调用时，我们必须首先声明一个变量，再将匿名函数赋值给这个变量。如果不分成两步，函数字面量无法与特定变量绑定，我们也无法递归调用该匿名函数。 123456var visitAll func(items []string)visitAll = func(items []string) &#123; // ... visitAll(items) // ...&#125; 5.6.1. 警告：捕获迭代变量 本节，将介绍 Go 词法作用域的一个陷阱。请务必仔细的阅读，弄清楚发生问题的原因。即使是经验丰富的程序员也会在这个问题上犯错误。 考虑这样一个问题：你被要求首先创建一些目录，再将目录删除。在下面的例子中我们用函数值来完成删除操作。下面的示例代码需要引入 os 包。为了使代码简单，我们忽略了所有的异常处理。 123456789101112var rmdirs []func()for _, d := range tempDirs() &#123; dir := d // NOTE: necessary! os.MkdirAll(dir, 0755) // creates parent directories too rmdirs = append(rmdirs, func() &#123; os.RemoveAll(dir) &#125;)&#125;// ...do some work…for _, rmdir := range rmdirs &#123; rmdir() // clean up&#125; 你可能会感到困惑，为什么要在循环体中用循环变量 d 赋值一个新的局部变量，而不是像下面的代码一样直接使用循环变量 dir。 问题的原因在于循环变量的作用域。在上面的程序中，for 循环语句引入了新的词法块，循环变量 dir 在这个词法块中被声明。在该循环中生成的所有函数值都共享相同的循环变量。需要注意，函数值中记录的是循环变量的内存地址，而不是循环变量某一时刻的值。以 dir 为例，后续的迭代会不断更新 dir 的值，当删除操作执行时，for 循环已完成，dir 中存储的值等于最后一次迭代的值。这意味着，每次对 os.RemoveAll 的调用删除的都是相同的目录。 通常，为了解决这个问题，我们会引入一个与循环变量同名的局部变量，作为循环变量的副本。比如下面的变量 dir，虽然这看起来很奇怪，但却很有用。 1234for _, dir := range tempDirs() &#123; dir := dir // declares inner dir, initialized to outer dir // ...&#125; 这个问题不仅存在基于 range 的循环，在三段式的 for 循环中，也会存在这样的问题。 5.7. 可变参数参数数量可变的函数称为可变参数函数。 在声明可变参数函数时，需要在参数列表的最后一个参数类型之前加上省略符号 ...，这表示该函数会接收任意数量的该类型参数。 1234567func sum(vals...int) int &#123; total := 0 for _, val := range vals &#123; total += val &#125; return total&#125; 在函数体中，vals 被看作是类型为 []int 的切片。sum 可以接收任意数量的 int 型参数： 1fmt.Println(sum(1, 2, 3, 4)) 在上面的代码中，调用者隐式的创建一个数组，并将原始参数复制到数组中，再把数组的一个切片作为参数传给被调用函数。如果原始参数已经是切片类型，我们该如何传递给 sum？只需在最后一个参数后加上省略符。下面的代码功能与上个例子相同。 12values := []int&#123;1, 2, 3, 4&#125;fmt.Println(sum(values...)) // \"10\" 5.8. Deferred 函数你只需要在调用普通函数或方法前加上关键字 defer，就完成了 defer 所需要的语法。 当执行到该条语句时，函数和参数表达式得到计算，但直到包含该 defer 语句的函数执行完毕时，defer 后的函数才会被执行，不论包含 defer 语句的函数是通过 return 正常结束，还是由于 panic 导致的异常结束。 你可以在一个函数中执行多条 defer 语句，它们的执行顺序与声明顺序相反。 defer 语句经常被用于处理成对的操作，如打开、关闭、连接、断开连接、加锁、释放锁。通过 defer 机制，不论函数逻辑多复杂，都能保证在任何执行路径下，资源被释放。 调试复杂程序时，defer 机制也常被用于记录何时进入和退出函数。下例中的 bigSlowOperation 函数，直接调用 trace 记录函数的被调情况。需要注意一点：不要忘记 defer 语句后的圆括号，否则本该在进入时执行的操作会在退出时执行，而本该在退出时执行的，永远不会被执行。 12345678910111213func bigSlowOperation() &#123; defer trace(\"bigSlowOperation\")() // don't forget the extra parentheses time.Sleep(10 * time.Second)&#125;func trace(msg string) func() &#123; start := time.Now() log.Printf(\"enter: %s\", msg) return func() &#123; log.Printf(\"exit %s (%s)\", msg, time.Since(start)) &#125;&#125; 我们知道，defer 语句中的函数会在 return 语句更新返回值变量后再执行，又因为在函数中定义的匿名函数可以访问该函数包括返回值变量在内的所有变量，所以，对匿名函数采用 defer 机制，可以使其观察函数的返回值。 被延迟执行的匿名函数甚至可以修改函数返回给调用者的返回值： 12345func triple(x int) (result int) &#123; defer func() &#123; result += x &#125;() return double(x)&#125;fmt.Println(triple(4)) // \"12\" 在循环体中的 defer 语句需要特别注意，因为只有在函数执行完毕后，这些被延迟的函数才会执行。下面的代码会导致系统的文件描述符耗尽，因为在所有文件都被处理之前，没有文件会被关闭。 12345678for _, filename := range filenames &#123; f, err := os.Open(filename) if err != nil &#123; return err &#125; defer f.Close() // NOTE: risky; could run out of file descriptors // ...process f…&#125; 5.9. Panic 异常Go 的类型系统会在编译时捕获很多错误，但有些错误只能在运行时检查，如数组访问越界、空指针引用等。这些运行时错误会引起 painc 异常。 一般而言，当 panic 异常发生时，程序会中断运行，并立即执行在该 goroutine 中被延迟的函数（defer 机制）。随后，程序崩溃并输出日志信息。日志信息包括 panic value 和函数调用的堆栈跟踪信息。panic value 通常是某种错误信息。对于每个 goroutine，日志信息中都会有与之相对的，发生 panic 时的函数调用堆栈跟踪信息。 不是所有的 panic 异常都来自运行时，直接调用内置的 panic 函数也会引发 panic 异常；panic 函数接受任何值作为参数。当某些不应该发生的场景发生时，我们就应该调用 panic。比如，当程序到达了某条逻辑上不可能到达的路径： 12345678switch s := suit(drawCard()); s &#123; case \"Spades\": // ... case \"Hearts\": // ... case \"Diamonds\": // ... case \"Clubs\": // ... default: panic(fmt.Sprintf(\"invalid suit %q\", s)) // Joker?&#125; 虽然 Go 的 panic 机制类似于其他语言的异常，但 panic 的适用场景有一些不同。由于 panic 会引起程序的崩溃，因此 panic 一般用于严重错误，如程序内部的逻辑不一致。勤奋的程序员认为任何崩溃都表明代码中存在漏洞，所以对于大部分漏洞，我们应该使用 Go 提供的错误机制，而不是 panic，尽量避免程序的崩溃。在健壮的程序中，任何可以预料到的错误，如不正确的输入、错误的配置或是失败的 I/O 操作都应该被优雅的处理，最好的处理方式，就是使用 Go 的错误机制。但是在某些场景下，我们希望程序一定不能抛出错误，否则就引发 panic，go 的标准库中为了满足这种情况，许多函数都提供了 Must 前缀的包装版本： 123456789package regexpfunc Compile(expr string) (*Regexp, error) &#123; /* ... */ &#125;func MustCompile(expr string) *Regexp &#123; re, err := Compile(expr) if err != nil &#123; panic(err) &#125; return re&#125; 为了方便诊断问题，runtime 包允许程序员输出堆栈信息。在下面的例子中，我们通过在 main 函数中延迟调用 printStack 输出堆栈信息。 123456789func main() &#123; defer printStack() f(3) // will panic&#125;func printStack() &#123; var buf [4096]byte n := runtime.Stack(buf[:], false) os.Stdout.Write(buf[:n])&#125; 5.10. Recover 捕获异常通常来说，不应该对 panic 异常做任何处理，但有时，也许我们可以从异常中恢复，至少我们可以在程序崩溃前，做一些操作。举个例子，当 web 服务器遇到不可预料的严重问题时，在崩溃前应该将所有的连接关闭；如果不做任何处理，会使得客户端一直处于等待状态。如果 web 服务器还在开发阶段，服务器甚至可以将异常信息反馈到客户端，帮助调试。 如果在 deferred 函数中调用了内置函数 recover，并且定义该 defer 语句的函数发生了 panic 异常，recover 会使程序从 panic 中恢复，并返回 panic value。导致 panic 异常的函数不会继续运行，但能正常返回。在未发生 panic 时调用 recover，recover 会返回 nil。 1234567891011121314func panicFunc(x int) int &#123; return 1 / x&#125;func main() &#123; defer func() &#123; if p := recover(); p != nil &#123; err = fmt.Errorf(\"internal error: %v\", p) &#125; &#125;() fmt.Println(panicFunc(0))&#125; 不加区分的恢复所有的 panic 异常，不是可取的做法；因为在 panic 之后，无法保证包级变量的状态仍然和我们预期一致。比如，对数据结构的一次重要更新没有被完整完成、文件或者网络连接没有被关闭、获得的锁没有被释放。此外，如果写日志时产生的 panic 被不加区分的恢复，可能会导致漏洞被忽略。 虽然把对 panic 的处理都集中在一个包下，有助于简化对复杂和不可以预料问题的处理，但作为被广泛遵守的规范，你不应该试图去恢复其他包引起的 panic。公有的 API 应该将函数的运行失败作为 error 返回，而不是 panic。同样的，你也不应该恢复一个由他人开发的函数引起的 panic，比如说调用者传入的回调函数，因为你无法确保这样做是安全的。","categories":[{"name":"Go 语言圣经","slug":"Go-语言圣经","permalink":"https://homholueng.github.io/categories/Go-语言圣经/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://homholueng.github.io/tags/Go/"}]},{"title":"Go 语言圣经 - 4. 复合数据类型","slug":"gopl-complex-type","date":"2019-07-31T15:15:15.000Z","updated":"2019-08-05T12:04:36.081Z","comments":true,"path":"2019/07/31/gopl-complex-type/","link":"","permalink":"https://homholueng.github.io/2019/07/31/gopl-complex-type/","excerpt":"","text":"4.1. 数组数组是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。因为数组的长度是固定的，因此在Go语言中很少直接使用数组。 默认情况下，数组的每个元素都被初始化为元素类型对应的零值： 1var a = [3]int // array of 3 intergers with 0 value 我们也可以使用数组字面值语法用一组值来初始化数组： 123var q [3]int = [3]int&#123;1, 2, 3&#125;var r [3]int = [3]int&#123;1, 2&#125;fmt.Println(r[2]) // 0 在数组字面值中，如果在数组的长度位置出现的是 ... 省略号，则表示数组的长度是根据初始化值的个数来计算： 1q := [...]int&#123;1, 2, 3&#125; 数组的长度是数组类型的一个组成部分，因此 [3]int 和 [4]int 是两种不同的数组类型。 数组也可以指定一个索引和对应值列表的方式初始化，在这种形式的数组字面值形式中，初始化索引的顺序是无关紧要的，而且没用到的索引可以省略，和前面提到的规则一样，未指定初始值的元素将用零值初始化。 123456789101112type Currency intconst ( USD Currency = iota // 美元 EUR // 欧元 GBP // 英镑 RMB // 人民币)symbol := [...]string&#123;USD: \"$\", EUR: \"€\", GBP: \"￡\", RMB: \"￥\"&#125;fmt.Println(RMB, symbol[RMB]) // \"3 ￥\" 如果一个数组的元素类型是可以相互比较的，那么数组类型也是可以相互比较的，这时候我们可以直接通过 == 比较运算符来比较两个数组，只有当两个数组的所有元素都是相等的时候数组才是相等的。 注意，GO 并不会隐式的将传递给函数参数的数组转换成为指针，如果函数需要接收一个数组并修改其内容，则需要显示的将其参数类型声明为数组指针。 因为数组的类型包含了僵化的长度信息，并且不同长度的数组也被认作不同的类型，所以 GO 中很少直接使用数组，而是使用 slice。 4.2. SliceSlice（切片）代表变长的序列，序列中每个元素都有相同的类型。一个 slice 类型一般写作 []T，其中 T 代表 slice 中元素的类型；slice 的语法和数组很像，只是没有固定长度而已。 数组和 slice 之间有着紧密的联系。一个 slice 是一个轻量级的数据结构，提供了访问数组子序列（或者全部）元素的功能，而且 slice 的底层确实引用一个数组对象。 一个 slice 由三个部分构成：指针、长度和容量。 下图显示了表示一年中每个月份名字的字符串数组，还有重叠引用了该数组的两个 slice。数组这样定义： 1months := [...]string&#123;1: \"January\", /* ... */, 12: \"December\"&#125; slice 的切片操作 s[i:j]，其中 0 ≤ i≤ j≤ cap(s)，用于创建一个新的 slice，引用 s 的从第 i 个元素开始到第 j-1 个元素的子序列。 如果切片操作超出 cap(s) 的上限将导致一个 panic 异常，但是超出 len(s) 则是意味着扩展了 slice，因为新 slice 的长度会变大： 123456a := [...]int&#123;1, 2, 3, 4, 5&#125;s := a[0:2]fmt.Println(s) // [1 2]fmt.Printf(\"len: %d, cap: %d\\n\", len(s), cap(s))new_s := s[:4] // len: 2, cap: 5fmt.Printf(\"len: %d, cap: %d\\n\", len(new_s), cap(new_s)) // len: 4, cap: 5 因为 slice 值包含指向第一个 slice 元素的指针，因此向函数传递 slice 将允许在函数内部修改底层数组的元素。 要注意的是 slice 类型的变量 s 和数组类型的变量 a 的初始化语法的差异。slice 和数组的字面值语法很类似，它们都是用花括弧包含一系列的初始化元素，但是对于 slice 并没有指明序列的长度。这会隐式地创建一个合适大小的数组，然后 slice 的指针指向底层的数组。就像数组字面值一样，slice 的字面值也可以按顺序指定初始化值序列，或者是通过索引和元素值指定，或者用两种风格的混合语法初始化。 和数组不同的是，slice 之间不能比较，因此我们不能使用 == 操作符来判断两个 slice 是否含有全部相等元素。 slice 唯一合法的比较操作是和 nil 比较，如果你需要测试一个 slice 是否是空的，使用 len (s) == 0 来判断，而不应该用 s == nil 来判断。 1234var s []int // len(s) == 0, s == nils = nil // len(s) == 0, s == nils = []int(nil) // len(s) == 0, s == nils = []int&#123;&#125; // len(s) == 0, s != nil 内置的 make 函数创建一个指定元素类型、长度和容量的 slice。容量部分可以省略，在这种情况下，容量将等于长度。 12make([]T, len)make([]T, len, cap) // same as make([]T, cap)[:len] 4.2.1. append 函数内置的 append 函数用于向 slice 追加元素： 12345var runes []runefor _, r := range &quot;Hello, 世界&quot; &#123; runes = append(runes, r)&#125;fmt.Printf(&quot;%q\\n&quot;, runes) // &quot;[&apos;H&apos; &apos;e&apos; &apos;l&apos; &apos;l&apos; &apos;o&apos; &apos;,&apos; &apos; &apos; &apos;世&apos; &apos;界&apos;]&quot; 内置的 copy 函数可以方便地将一个 slice 复制另一个相同类型的 slice，copy 函数的第一个参数是要复制的目标 slice，第二个参数是源 slice，目标和源的位置顺序和 dst = src 赋值语句是一致的。 我们并不知道 append 调用是否导致了内存的重新分配，因此我们也不能确认新的 slice 和原始的 slice 是否引用的是相同的底层数组空间。同样，我们不能确认在原先的 slice 上的操作是否会影响到新的 slice。 因此，通常是将 append 返回的结果直接赋值给输入的 slice 变量： 1runes = append(runes, r) append 函数则可以追加多个元素，甚至追加一个 slice。 123456var x []intx = append(x, 1)x = append(x, 2, 3)x = append(x, 4, 5, 6)x = append(x, x...) // append the slice xfmt.Println(x) // \"[1 2 3 4 5 6 1 2 3 4 5 6]\" 4.2.2. Slice 内存技巧 给定一个字符串列表，下面的 nonempty 函数将在原有 slice 内存空间之上返回不包含空字符串的列表： 1234567891011121314151617 // Nonempty is an example of an in-place slice algorithm.package mainimport \"fmt\"// nonempty returns a slice holding only the non-empty strings.// The underlying array is modified during the call.func nonempty(strings []string) []string &#123; i := 0 for _, s := range strings &#123; if s != \"\" &#123; strings[i] = s i++ &#125; &#125; return strings[:i]&#125; 比较微妙的地方是，输入的 slice 和输出的 slice 共享一个底层数组。这可以避免分配另一个数组，不过原来的数据将可能会被覆盖，正如下面两个打印语句看到的那样： 123data := []string&#123;\"one\", \"\", \"three\"&#125;fmt.Printf(\"%q\\n\", nonempty(data)) // `[\"one\" \"three\"]`fmt.Printf(\"%q\\n\", data) // `[\"one\" \"three\" \"three\"]` 因此我们通常会这样使用 nonempty 函数：data = nonempty(data)。 4.3. Map在 Go 语言中，一个 map 就是一个哈希表的引用，map 类型可以写为 map[K]V，其中 K 和 V 分别对应 key 和 value。 内置的 make 函数可以创建一个 map： 1ages := make(map[string]int) // mapping from strings to ints 我们也可以用 map 字面值的语法创建 map，同时还可以指定一些最初的 key/value： 1234ages := map[string]int&#123; \"alice\": 31, \"charlie\": 34,&#125; 使用内置的 delete 函数可以删除元素： 1delete(ages, \"alice\") // remove element ages[\"alice\"] 在 go 的 map 上，如果一个查找失败将返回 value 类型对应的零值。 但是 map 中的元素并不是一个变量，因此我们不能对 map 的元素进行取址操作： 1_ = &amp;ages[\"bob\"] // compile error: cannot take address of map element 禁止对 map 元素取址的原因是 map 可能随着元素数量的增长而重新分配更大的内存空间，从而可能导致之前的地址无效。 要想遍历 map 中全部的 key/value 对的话，可以使用 range 风格的 for 循环实现，和之前的 slice 遍历语法类似。下面的迭代语句将在每次迭代时设置 name 和 age 变量，它们对应下一个键 / 值对： 123for name, age := range ages &#123; fmt.Printf(\"%s\\t%d\\n\", name, age)&#125; 如果要按顺序遍历 key/value 对，我们必须显式地对 key 进行排序，可以使用 sort 包的 Strings 函数对字符串 slice 进行排序。下面是常见的处理方式： 12345678910import \"sort\"var names []stringfor name := range ages &#123; names = append(names, name)&#125;sort.Strings(names)for _, name := range names &#123; fmt.Printf(\"%s\\t%d\\n\", name, ages[name])&#125; 因为我们一开始就知道 names 的最终大小，因此给 slice 分配一个合适的大小将会更有效。下面的代码创建了一个空的 slice，但是 slice 的容量刚好可以放下 map 中全部的 key： 1names := make([]string, 0, len(ages)) // type, len, cap map 类型的零值是 nil，也就是没有引用任何哈希表。 123var ages map[string]intfmt.Println(ages == nil) // \"true\"fmt.Println(len(ages) == 0) // \"true\" map 上的大部分操作，包括查找、删除、len 和 range 循环都可以安全工作在 nil 值的 map 上，它们的行为和一个空的 map 类似。但是向一个 nil 值的 map 存入元素将导致一个 panic 异常： 1ages[\"carol\"] = 21 // panic: assignment to entry in nil map 如果我们需要确认 map 取值返回的究竟是不存在的零值还是存在的零值，可以使用如下的方式： 123456age, ok := ages[\"bob\"]if !ok &#123; /* \"bob\" is not a key in this map; age == 0. */ &#125;// orif age, ok := ages[\"bob\"]; !ok &#123; /* ... */ &#125; 和 slice 一样，map 之间也不能进行相等比较；唯一的例外是和 nil 进行比较。要判断两个 map 是否包含相同的 key 和 value，我们必须通过一个循环实现。 Go 语言中并没有提供一个 set 类型，但是 map 中的 key 也是不相同的，可以用 map 实现类似 set 的功能。 有时候我们需要一个 map 或 set 的 key 是 slice 类型，但是 map 的 key 必须是可比较的类型，但是 slice 并不满足这个条件。不过，我们可以通过两个步骤绕过这个限制。第一步，定义一个辅助函数 k，将 slice 转为 map 对应的 string 类型的 key，确保只有 x 和 y 相等时 k (x) == k (y) 才成立。然后创建一个 key 为 string 类型的 map，在每次对 map 操作时先用 k 辅助函数将 slice 转化为 string 类型。 4.4. 结构体结构体是一种聚合的数据类型，是由零个或多个任意类型的值聚合成的实体。 下面两个语句声明了一个叫 Employee 的命名的结构体类型，并且声明了一个 Employee 类型的变量 dilbert： 1234567891011type Employee struct &#123; ID int Name string Address string DoB time.Time Position string Salary int ManagerID int&#125;var dilbert Employee dilbert 结构体变量的成员可以通过点操作符访问，比如 dilbert.Name 和 dilbert.DoB。 点操作符也可以和指向结构体的指针一起工作： 12var employeeOfTheMonth *Employee = &amp;dilbertemployeeOfTheMonth.Position += \" (proactive team player)\" 相当于下面语句 1(*employeeOfTheMonth).Position += \" (proactive team player)\" 下面的 EmployeeByID 函数将根据给定的员工 ID 返回对应的员工信息结构体的指针。我们可以使用点操作符来访问它里面的成员： 123456func EmployeeByID(id int) *Employee &#123; /* ... */ &#125;fmt.Println(EmployeeByID(dilbert.ManagerID).Position) // \"Pointy-haired boss\"id := dilbert.IDEmployeeByID(id).Salary = 0 // fired for... no real reason 后面的语句通过 EmployeeByID 返回的结构体指针更新了 Employee 结构体的成员。如果将 EmployeeByID 函数的返回值从 *Employee 指针类型改为 Employee 值类型，那么更新语句将不能编译通过，因为在赋值语句的左边并不确定是一个变量（译注：调用函数返回的是值，并不是一个可取地址的变量）。 通常一行对应一个结构体成员，成员的名字在前类型在后，不过如果相邻的成员类型如果相同的话可以被合并到一行，就像下面的 Name 和 Address 成员那样： 12345678type Employee struct &#123; ID int Name, Address string DoB time.Time Position string Salary int ManagerID int&#125; 如果结构体成员名字是以大写字母开头的，那么该成员就是导出的；这是 Go 语言导出规则决定的。一个结构体可能同时包含导出和未导出的成员。 结构体类型的零值是每个成员都是零值。通常会将零值作为最合理的默认值。 如果结构体没有任何成员的话就是空结构体，写作 struct{}。它的大小为 0，也不包含任何信息，但是有时候依然是有价值的。有些 Go 语言程序员用 map 来模拟 set 数据结构时，用它来代替 map 中布尔类型的 value，只是强调 key 的重要性，但是因为节约的空间有限，而且语法比较复杂，所以我们通常会避免这样的用法。 123456seen := make(map[string]struct&#123;&#125;) // set of strings// ...if _, ok := seen[s]; !ok &#123; seen[s] = struct&#123;&#125;&#123;&#125; // ...first time seeing s...&#125; 4.4.1. 结构体字面值结构体值也可以用结构体字面值表示，结构体字面值可以指定每个成员的值。 123type Point struct&#123; X, Y int &#125;p := Point&#123;1, 2&#125; 这里有两种形式的结构体字面值语法，上面的是第一种写法，要求以结构体成员定义的顺序为每个结构体成员指定一个字面值。它要求写代码和读代码的人要记住结构体的每个成员的类型和顺序，不过结构体成员有细微的调整就可能导致上述代码不能编译。 其实更常用的是第二种写法，以成员名字和相应的值来初始化： 1p := Point&#123;X: 1, Y: 2&#125; 在这种形式的结构体字面值写法中，如果成员被忽略的话将默认用零值。另外，需要注意的是，两种不同形式的写法不能混合使用。 因为结构体通常通过指针处理，可以用下面的写法来创建并初始化一个结构体变量，并返回结构体的地址： 1pp := &amp;Point&#123;1, 2&#125; 它和下面的语句是等价的 12pp := new(Point)*pp = Point&#123;1, 2&#125; 不过 &amp;Point{1, 2} 写法可以直接在表达式中使用，比如一个函数调用。 4.4.2. 结构体比较如果结构体的全部成员都是可以比较的，那么结构体也是可以比较的，那样的话两个结构体将可以使用 == 或 = 运算符进行比较。 可比较的结构体类型和其他可比较的类型一样，可以用于 map 的 key 类型。 4.4.3. 结构体嵌入和匿名成员Go 语言有一个特性让我们只声明一个成员对应的数据类型而不指名成员的名字；这类成员就叫匿名成员。匿名成员的数据类型必须是命名的类型或指向一个命名的类型的指针。下面的代码中，Circle 和 Wheel 各自都有一个匿名成员。我们可以说 Point 类型被嵌入到了 Circle 结构体，同时 Circle 类型被嵌入到了 Wheel 结构体。 123456789type Circle struct &#123; Point Radius int&#125;type Wheel struct &#123; Circle Spokes int&#125; 得益于匿名嵌入的特性，我们可以直接访问叶子属性而不需要给出完整的路径： 12345var w Wheelw.X = 8 // equivalent to w.Circle.Point.X = 8w.Y = 8 // equivalent to w.Circle.Point.Y = 8w.Radius = 5 // equivalent to w.Circle.Radius = 5w.Spokes = 20 其中匿名成员 Circle 和 Point 都有自己的名字 —— 就是命名的类型名字 —— 但是这些名字在点操作符中是可选的。我们在访问子成员的时候可以忽略任何匿名成员部分。 不幸的是，结构体字面值并没有简短表示匿名成员的语法， 因此下面的语句都不能编译通过： 12w = Wheel&#123;8, 8, 5, 20&#125; // compile error: unknown fieldsw = Wheel&#123;X: 8, Y: 8, Radius: 5, Spokes: 20&#125; // compile error: unknown fields 结构体字面值必须遵循形状类型声明时的结构，所以我们只能用下面的两种语法，它们彼此是等价的： 12345678910111213141516171819w = Wheel&#123;Circle&#123;Point&#123;8, 8&#125;, 5&#125;, 20&#125;w = Wheel&#123; Circle: Circle&#123; Point: Point&#123;X: 8, Y: 8&#125;, Radius: 5, &#125;, Spokes: 20, // NOTE: trailing comma necessary here (and at Radius)&#125;fmt.Printf(\"%#v\\n\", w)// Output:// Wheel&#123;Circle:Circle&#123;Point:Point&#123;X:8, Y:8&#125;, Radius:5&#125;, Spokes:20&#125;w.X = 42fmt.Printf(\"%#v\\n\", w)// Output:// Wheel&#123;Circle:Circle&#123;Point:Point&#123;X:42, Y:8&#125;, Radius:5&#125;, Spokes:20&#125; 需要注意的是 Printf 函数中 %v 参数包含的 # 副词，它表示用和 Go 语言类似的语法打印值。对于结构体类型来说，将包含每个成员的名字。 因为匿名成员也有一个隐式的名字，因此不能同时包含两个类型相同的匿名成员，这会导致名字冲突。同时，因为成员的名字是由其类型隐式地决定的，所以匿名成员也有可见性的规则约束。 在上面的例子中，Point 和 Circle 匿名成员都是导出的。即使它们不导出（比如改成小写字母开头的 point 和 circle），我们依然可以用简短形式访问匿名成员嵌套的成员 但是在包外部，因为 circle 和 point 没有导出，不能访问它们的成员，因此简短的匿名成员访问语法也是禁止的。 其实任何命名的类型都可以作为结构体的匿名成员。但是为什么要嵌入一个没有任何子成员类型的匿名成员类型呢？ 答案是匿名类型的方法集。简短的点运算符语法可以用于选择匿名成员嵌套的成员，也可以用于访问它们的方法。实际上，外层的结构体不仅仅是获得了匿名成员类型的所有成员，而且也获得了该类型导出的全部的方法。这个机制可以用于将一些有简单行为的对象组合成有复杂行为的对象。 组合是 Go 语言中面向对象编程的核心 4.5. JSONGo 语言对于标准格式的编码和解码都有良好的支持，由标准库中的 encoding/json、encoding/xml、encoding/asn1 等包提供支持（译注：Protocol Buffers 的支持由 github.com/golang/protobuf 包提供），并且这类包都有着相似的 API 接口。 考虑一个应用程序，该程序负责收集各种电影评论并提供反馈功能。它的 Movie 数据类型和一个典型的表示电影的值列表如下所示。 1234567891011121314151617181920212223type Movie struct &#123; Title string Year int `json:released` Color bool `json:\"color,omitempty\"` Actors []string&#125;func main() &#123; var movies = []Movie&#123; &#123;Title: \"Casablanca\", Year: 1942, Color: false, Actors: []string&#123;\"Humphrey Bogart\", \"Ingrid Bergman\"&#125;&#125;, &#123;Title: \"Cool Hand Luke\", Year: 1967, Color: true, Actors: []string&#123;\"Paul Newman\"&#125;&#125;, &#123;Title: \"Bullitt\", Year: 1968, Color: true, Actors: []string&#123;\"Steve McQueen\", \"Jacqueline Bisset\"&#125;&#125;, &#125; data, err := json.MarshalIndent(movies, \"\", \" \") if err != nil &#123; log.Fatalf(\"JSON marshaling failed: %s\", err) &#125; fmt.Printf(\"%s\\n\", data)&#125; json.MarshalIndent 函数产生整齐缩进的输出： 123456789101112131415161718192021222324252627[ &#123; \"Title\": \"Casablanca\", \"Year\": 1942, \"Actors\": [ \"Humphrey Bogart\", \"Ingrid Bergman\" ] &#125;, &#123; \"Title\": \"Cool Hand Luke\", \"Year\": 1967, \"color\": true, \"Actors\": [ \"Paul Newman\" ] &#125;, &#123; \"Title\": \"Bullitt\", \"Year\": 1968, \"color\": true, \"Actors\": [ \"Steve McQueen\", \"Jacqueline Bisset\" ] &#125;] 在编码时，默认使用 Go 语言结构体的成员名字作为 JSON 的对象（通过 reflect 反射技术），只有导出的结构体成员才会被编码。 在上述的例子中，Year 名字的成员在编码后变成了 released，还有 Color 成员编码后变成了小写字母开头的 color。这是因为结构体成员 Tag 所导致的。一个结构体成员 Tag 是和在编译阶段关联到该成员的元信息字符串： 12Year int `json:\"released\"`Color bool `json:\"color,omitempty\"` 结构体的成员 Tag 可以是任意的字符串面值，但是通常是一系列用空格分隔的 key:”value” 键值对序列；因为值中含有双引号字符，因此成员 Tag 一般用原生字符串面值的形式书写。 成员 Tag 中 json 对应值的第一部分用于指定 JSON 对象的名字，比如将 Go 语言中的 TotalCount 成员对应到 JSON 中的 total_count 对象。Color 成员的 Tag 还带了一个额外的 omitempty 选项，表示当 Go 语言结构体成员为空或零值时不生成该 JSON 对象 编码的逆操作是解码，对应将 JSON 数据解码为 Go 语言的数据结构，Go 语言中一般叫 unmarshaling，通过 json.Unmarshal 函数完成。下面的代码将 JSON 格式的电影数据解码为一个结构体 slice，结构体中只有 Title 成员。 12345var titles []struct&#123; Title string &#125;if err := json.Unmarshal(data, &amp;titles); err != nil &#123; log.Fatalf(\"JSON unmarshaling failed: %s\", err)&#125;fmt.Println(titles) // \"[&#123;Casablanca&#125; &#123;Cool Hand Luke&#125; &#123;Bullitt&#125;]\" 4.6. 文本和 HTML 模板简单的格式化，使用 Printf 是完全足够的。但是有时候会需要复杂的打印格式，这时候一般需要将格式化代码分离出来以便更安全地修改。这些功能是由 text/template 和 html/template 等模板包提供的，它们提供了一个将变量值填充到一个文本或 HTML 格式的模板的机制。 一个模板是一个字符串或一个文件，里面包含了一个或多个由双花括号包含的 {{action}} 对象。大部分的字符串只是按字面值打印，但是对于 actions 部分将触发其它的行为。每个 actions 都包含了一个用模板语言书写的表达式，一个 action 虽然简短但是可以输出复杂的打印值，模板语言包含通过选择结构体的成员、调用函数或方法、表达式控制流 if-else 语句和 range 循环语句，还有其它实例化模板等诸多特性。 下面是一个简单的模板字符串： 1234567const templ = `&#123;% raw %&#125;&#123;&#123;.TotalCount&#125;&#125;&#123;% endraw %&#125; issues:&#123;% raw %&#125;&#123;&#123;range .Items&#125;&#125;&#123;% endraw %&#125;----------------------------------------Number: &#123;% raw %&#125;&#123;&#123;.Number&#125;&#125;&#123;% endraw %&#125;User: &#123;% raw %&#125;&#123;&#123;.User.Login&#125;&#125;&#123;% endraw %&#125;Title: &#123;% raw %&#125;&#123;&#123;.Title | printf \"%.64s\"&#125;&#125;&#123;% endraw %&#125;Age: &#123;% raw %&#125;&#123;&#123;.CreatedAt | daysAgo&#125;&#125;&#123;% endraw %&#125; days&#123;% raw %&#125;&#123;&#123;end&#125;&#125;&#123;% endraw %&#125;` 这个模板先打印匹配到的 issue 总数，然后打印每个 issue 的编号、创建用户、标题还有存在的时间。对于每一个 action，都有一个当前值的概念，对应点操作符，写作 .。当前值 . 最初被初始化为调用模板时的参数，在当前例子中对应 IssuesSearchResult 类型的变量。 模板中 {{.TotalCount}} 对应 action 将展开为结构体中 TotalCount 成员以默认的方式打印的值。模板中 {{range .Items}} 和 {{end}} 对应一个循环 action，因此它们之间的内容可能会被展开多次，循环每次迭代的当前值对应当前的 Items 元素的值。 在一个 action 中，| 操作符表示将前一个表达式的结果作为后一个函数的输入，类似于 UNIX 中管道的概念。 生成模板的输出需要两个处理步骤。第一步是要分析模板并转为内部表示，然后基于指定的输入执行模板。分析模板部分一般只需要执行一次。下面的代码创建并分析上面定义的模板 templ。 123456report, err := template.New(\"report\"). Funcs(template.FuncMap&#123;\"daysAgo\": daysAgo&#125;). Parse(templ)if err != nil &#123; log.Fatal(err)&#125; 注意方法调用链的顺序：template.New 先创建并返回一个模板；Funcs 方法将 daysAgo 等自定义函数注册到模板中，并返回模板；最后调用 Parse 函数分析模板。 因为模板通常在编译时就测试好了，如果模板解析失败将是一个致命的错误。template.Must 辅助函数可以简化这个致命错误的处理：它接受一个模板和一个 error 类型的参数，检测 error 是否为 nil（如果不是 nil 则发出 panic 异常），然后返回传入的模板。 123var report = template.Must(template.New(\"issuelist\"). Funcs(template.FuncMap&#123;\"daysAgo\": daysAgo&#125;). Parse(templ)) 一旦模板创建完成，我们就能根据输入源生成我们想要的结果： 1234result, err := SearchIssues(os.Args[1:])if err := report.Execute(os.Stdout, result); err != nil &#123; log.Fatal(err)&#125;","categories":[{"name":"Go 语言圣经","slug":"Go-语言圣经","permalink":"https://homholueng.github.io/categories/Go-语言圣经/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://homholueng.github.io/tags/Go/"}]},{"title":"Go 语言圣经 - 3. 基础数据类型","slug":"gopl-basic-type","date":"2019-07-24T12:30:20.000Z","updated":"2019-07-25T05:03:52.686Z","comments":true,"path":"2019/07/24/gopl-basic-type/","link":"","permalink":"https://homholueng.github.io/2019/07/24/gopl-basic-type/","excerpt":"","text":"3.1. 整形Go 语言同时提供了有符号和无符号类型的整数运算。这里有 int8、int16、int32 和 int64 四种截然不同大小的有符号整数类型，分别对应 8、16、32、64bit 大小的有符号整数，与此对应的是 uint8、uint16、uint32 和 uint64 四种无符号整数类型。 这里还有两种一般对应特定 CPU 平台机器字大小的有符号和无符号整数 int 和 uint，其大小会根据当前运行环境来确定。 Unicode 字符 rune 类型是和 int32 等价的类型，通常用于表示一个 Unicode 码点。这两个名称可以互换使用。同样 byte 也是 uint8 类型的等价类型，byte 类型一般用于强调数值是一个原始的数据而不是一个小的整数。 最后，还有一种无符号的整数类型 uintptr，没有指定具体的 bit 大小但是足以容纳指针。 int 和 int32 也是不同的类型，即使 int 的大小也是 32bit，在需要将 int 当作 int32 类型的地方需要一个显式的类型转换操作。 任何大小的整数字面值都可以用以 0 开始的八进制格式书写，例如 0666；或用以 0x 或 0X 开头的十六进制格式书写，例如 0xdeadbeef。 fmt 使用技巧我们能够通过格式化标志的副词来告诉 fmt 要用哪一个参数进行格式化的输出： 12x := int64(0xdeadbeef)fmt.Printf(\"%d %[1]x %#[1]x %#[1]X\\n\", x) // 3735928559 deadbeef 0xdeadbeef 0XDEADBEEF 同时，打印字符时，使用 %q 能够打印带单引号的字符： 12ascii := 'a'fmt.Printf(\"%d %[1]c %[1]q\\n\", ascii) // \"97 a 'a'\" 3.2. 浮点数Go 语言提供了两种精度的浮点数，float32 和 float64。 浮点数的范围极限值可以在 math 包找到。常量 math.MaxFloat32 表示 float32 能表示的最大数值，大约是 3.4e38；对应的 math.MaxFloat64 常量大约是 1.8e308。 3.3. 复数Go 语言提供了两种精度的复数类型：complex64 和 complex128，分别对应 float32 和 float64 两种浮点数精度。内置的 complex 函数用于构建复数，内建的 real 和 imag 函数分别返回复数的实部和虚部： 12345var x complex128 = complex(1, 2) // 1+2ivar y complex128 = complex(3, 4) // 3+4ifmt.Println(x*y) // \"(-5+10i)\"fmt.Println(real(x*y)) // \"-5\"fmt.Println(imag(x*y)) // \"10\" 3.4. 布尔型布尔值并不会隐式转换为数字值 0 或 1，反之亦然。 3.5. 字符串一个字符串是一个不可改变的字节序列。字符串可以包含任意的数据，包括 byte 值 0，但是通常是用来包含人类可读的文本。文本字符串通常被解释为采用 UTF8 编码的 Unicode 码点（rune）序列。 内置的 len 函数可以返回一个字符串中的字节数目（不是 rune 字符数目），索引操作 s[i] 返回第 i 个字节的字节值。 第 i 个字节并不一定是字符串的第 i 个字符，因为对于非 ASCII 字符的 UTF8 编码会要两个或多个字节。 因为字符串是不可修改的，因此尝试修改字符串内部数据的操作也是被禁止的： 12s = \"hello world\"s[0] = 'L' // compile error: cannot assign to s[0] 不变性意味着如果两个字符串共享相同的底层数据的话也是安全的，这使得复制任何长度的字符串代价是低廉的。同样，一个字符串 s 和对应的子字符串切片 s[7:] 的操作也可以安全地共享相同的内存，因此字符串切片操作代价也是低廉的。在这两种情况下都没有必要分配新的内存。 3.5.1. 字符串面值一个原生的字符串面值形式是 `…`，使用反引号代替双引号。在原生的字符串面值中，没有转义操作；全部的内容都是字面的意思，包含退格和换行，因此一个程序中的原生字符串面值可能跨越多行 3.5.3. UTF-8UTF8 是一个将 Unicode 码点编码为字节序列的变长编码。UTF8 编码使用 1 到 4 个字节来表示每个 Unicode 码点，每个符号编码后第一个字节的高端 bit 位用于表示编码总共有多少个字节。 Go 语言的源文件采用 UTF8 编码，并且 Go 语言处理 UTF8 编码的文本也很出色。unicode 包提供了诸多处理 rune 字符相关功能的函数（比如区分字母和数字，或者是字母的大写和小写转换等），unicode/utf8 包则提供了用于 rune 字符序列的 UTF8 编码和解码的功能。 得益于 UTF8 编码优良的设计，诸多字符串操作都不需要解码操作，对于 UTF8 编码后文本的处理和原始的字节处理逻辑是一样的。 另一方面，如果我们真的关心每个 Unicode 字符，我们可以使用其它处理方式： 12345import \"unicode/utf8\"s := \"Hello, 世界\"fmt.Println(len(s)) // \"13\"fmt.Println(utf8.RuneCountInString(s)) // \"9\" Go 语言的 range 循环在处理字符串的时候，会自动隐式解码 UTF8 字符串，但是其对于非 ASCII，索引更新的步长将超过 1 个字节。 3.5.4. 字符串和 Byte 切片标准库中有四个包对字符串处理尤为重要：bytes、strings、strconv 和 unicode 包。 strings 包提供了许多如字符串的查询、替换、比较、截断、拆分和合并等功能。 bytes 包也提供了很多类似功能的函数，但是针对和字符串有着相同结构的 []byte 类型。因为字符串是只读的，因此逐步构建字符串会导致很多分配和复制。在这种情况下，使用 bytes.Buffer 类型会更有效。 strconv 包提供了布尔型、整型数、浮点数和对应字符串的相互转换，还提供了双引号转义相关的转换。 unicode 包提供了 IsDigit、IsLetter、IsUpper 和 IsLower 等类似功能，它们用于给字符分类。 bytes 包还提供了 Buffer 类型用于字节 slice 的缓存。一个 Buffer 开始是空的，但是随着 string、byte 或 []byte 等类型数据的写入可以动态增长。 3.6. 常量常量表达式的值在编译期计算，而不是在运行期。每种常量的潜在类型都是基础类型：boolean、string 或数字。 一个常量的声明语句定义了常量的名字，和变量的声明语法类似，常量的值不可修改，这样可以防止在运行期被意外或恶意的修改。 1const pi = 3.14159 // approximately; math.Pi is a better approximation 常量间的所有算术运算、逻辑运算和比较运算的结果也是常量，对常量的类型转换操作或以下函数调用都是返回常量结果：len、cap、real、imag、complex 和 unsafe.Sizeof。 3.6.1. iota 常量生成器常量声明可以使用 iota 常量生成器初始化，它用于生成一组以相似规则初始化的常量，但是不用每行都写一遍初始化表达式。在一个 const 声明语句中，在第一个声明的常量所在的行，iota 将会被置为 0，然后在每一个有常量声明的行加一。 1234567891011type Weekday intconst ( Sunday Weekday = iota Monday Tuesday Wednesday Thursday Friday Saturday) 3.6.2. 无类型常量Go 语言的常量有个不同寻常之处。虽然一个常量可以有任意一个确定的基础类型，例如 int 或 float64，或者是类似 time.Duration 这样命名的基础类型，但是许多常量并没有一个明确的基础类型。编译器为这些没有明确基础类型的数字常量提供比基础类型更高精度的算术运算；你可以认为至少有 256bit 的运算精度。这里有六种未明确类型的常量类型，分别是 无类型的布尔型 无类型的整数 无类型的字符 无类型的浮点数 无类型的复数 无类型的字符串","categories":[{"name":"Go 语言圣经","slug":"Go-语言圣经","permalink":"https://homholueng.github.io/categories/Go-语言圣经/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://homholueng.github.io/tags/Go/"}]},{"title":"Go 语言圣经 - 2. 程序结构","slug":"gopl-program-structure","date":"2019-07-24T11:29:06.000Z","updated":"2019-07-25T05:03:52.686Z","comments":true,"path":"2019/07/24/gopl-program-structure/","link":"","permalink":"https://homholueng.github.io/2019/07/24/gopl-program-structure/","excerpt":"","text":"2.3. 变量var 声明语句可以创建一个特定类型的变量，然后给变量附加一个名字，并且设置变量的初始值。变量声明的一般语法如下： 1var 变量名字 类型 = 表达式 其中 类型 或 = 表达式 两个部分可以省略其中的一个。如果省略的是类型信息，那么将根据初始化表达式来推导变量的类型信息。如果初始化表达式被省略，那么将用零值初始化该变量。 数值类型变量对应的零值是 0，布尔类型变量对应的零值是 false，字符串类型对应的零值是空字符串，接口或引用类型（包括 slice、指针、map、chan 和函数）变量对应的零值是 nil。数组或结构体等聚合类型对应的零值是每个元素或字段都是对应该类型的零值。 2.3.1. 简短变量声明在函数内部，有一种称为简短变量声明语句的形式可用于声明和初始化局部变量。它以 名字 := 表达式 形式声明变量，变量的类型根据表达式来自动推导： 123i := 0j := 1.0s := \"\" 这里有一个比较微妙的地方：简短变量声明左边的变量可能并不是全部都是刚刚声明的。如果有一些已经在相同的词法域声明过了，那么简短变量声明语句对这些已经声明过的变量就只有赋值行为了。 简短变量声明语句中必须至少要声明一个新的变量，下面的代码将不能编译通过： 123f, err := os.Open(infile)// ...f, err := os.Create(outfile) // compile error: no new variables 简短变量声明语句只有对已经在同级词法域声明过的变量才和赋值操作语句等价，如果变量是在外部词法域声明的，那么简短变量声明语句将会在当前词法域重新声明一个新的变量。 2.3.2. 指针如果用 var x int 声明语句声明一个 x 变量，那么 &amp;x 表达式（取 x 变量的内存地址）将产生一个指向该整数变量的指针，指针对应的数据类型是 *int，指针被称之为 “指向 int 类型的指针”。 12345x := 1p := &amp;x // p, of type *int, points to xfmt.Println(*p) // \"1\"*p = 2 // equivalent to x = 2fmt.Println(x) // \"2\" 任何类型的指针的零值都是 nil。如果 p 指向某个有效变量，那么 p != nil 测试为真。 2.3.3. new 函数另一个创建变量的方法是调用内建的 new 函数。表达式 new (T) 将创建一个 T 类型的匿名变量，初始化为 T 类型的零值，然后返回变量地址，返回的指针类型为 *T。 1234p := new(int) // p, *int 类型, 指向匿名的 int 变量fmt.Println(*p) // \"0\"*p = 2 // 设置 int 匿名变量的值为 2fmt.Println(*p) // \"2\" 由于 new 只是一个预定义的函数，它并不是一个关键字，因此我们可以将 new 名字重新定义为别的类型。例如下面的例子： 1func delta(old, new int) int &#123; return new - old &#125; 由于 new 被定义为 int 类型的变量名，因此在 delta 函数内部是无法使用内置的 new 函数的。 2.3.4. 变量的生命周期变量的生命周期指的是在程序运行期间变量有效存在的时间段。对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。而相比之下，局部变量的生命周期则是动态的：每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。函数的参数变量和返回值变量都是局部变量。它们在函数每次被调用的时候创建。 编译器会自动选择在栈上还是在堆上分配局部变量的存储空间，但可能令人惊讶的是，这个选择并不是由用 var 还是 new 声明变量的方式决定的。 123456789101112var global *intfunc f() &#123; var x int x = 1 global = &amp;x&#125;func g() &#123; y := new(int) *y = 1&#125; f 函数里的 x 变量必须在堆上分配，因为它在函数退出后依然可以通过包一级的 global 变量找到，虽然它是在函数内部定义的；用 Go 语言的术语说，这个 x 局部变量从函数 f 中逃逸了。相反，当 g 函数返回时，变量 *y 将是不可达的，也就是说可以马上被回收的。因此，*y 并没有从函数 g 中逃逸，编译器可以选择在栈上分配 *y 的存储空间（译注：也可以选择在堆上分配，然后由 Go 语言的 GC 回收这个变量的内存空间），虽然这里用的是 new 方式。 2.4. 赋值2.4.2. 可赋值性赋值语句是显式的赋值形式，但是程序中还有很多地方会发生隐式的赋值行为：函数调用会隐式地将调用参数的值赋值给函数的参数变量，一个返回语句会隐式地将返回操作的值赋值给结果变量，一个复合类型的字面量也会产生赋值行为。 不管是隐式还是显式地赋值，在赋值语句左边的变量和右边最终的求到的值必须有相同的数据类型。更直白地说，只有右边的值对于左边的变量是可赋值的，赋值语句才是允许的。 可赋值性的规则对于不同类型有着不同要求，对每个新类型特殊的地方我们会专门解释。对于目前我们已经讨论过的类型，它的规则是简单的：类型必须完全匹配，nil 可以赋值给任何指针或引用类型的变量。常量则有更灵活的赋值规则，因为这样可以避免不必要的显式的类型转换。 对于两个值是否可以用 == 或 != 进行相等比较的能力也和可赋值能力有关系：对于任何类型的值的相等比较，第二个值必须是对第一个值类型对应的变量是可赋值的，反之亦然。 2.5. 类型一个类型声明语句创建了一个新的类型名称，和现有类型具有相同的底层结构。新命名的类型提供了一个方法，用来分隔不同概念的类型，这样即使它们底层类型相同也是不兼容的。 1type 类型名字 底层类型 例如下面这个例子： 1234567891011121314151617// Package tempconv performs Celsius and Fahrenheit temperature computations.package tempconvimport \"fmt\"type Celsius float64 // 摄氏温度type Fahrenheit float64 // 华氏温度const ( AbsoluteZeroC Celsius = -273.15 // 绝对零度 FreezingC Celsius = 0 // 结冰点温度 BoilingC Celsius = 100 // 沸水温度)func CToF(c Celsius) Fahrenheit &#123; return Fahrenheit(c*9/5 + 32) &#125;func FToC(f Fahrenheit) Celsius &#123; return Celsius((f - 32) * 5 / 9) &#125; 我们在这个包声明了两种类型：Celsius 和 Fahrenheit 分别对应不同的温度单位。它们虽然有着相同的底层类型 float64，但是它们是不同的数据类型，因此它们不可以被相互比较或混在一个表达式运算。 刻意区分类型，可以避免一些像无意中使用不同单位的温度混合计算导致的错误，因此需要一个类似 Celsius(t) 或 Fahrenheit(t) 形式的显式转型操作才能将 float64 转为对应的类型。 比较运算符 == 和 &lt; 也可以用来比较一个命名类型的变量和另一个有相同类型的变量，或有着相同底层类型的未命名类型的值之间做比较。但是如果两个值有着不同的类型，则不能直接进行比较： 123456var c Celsiusvar f Fahrenheitfmt.Println(c == 0) // \"true\"fmt.Println(f &gt;= 0) // \"true\"fmt.Println(c == f) // compile error: type mismatchfmt.Println(c == Celsius(f)) // \"true\"! 2.6. 包和文件通常一个包所在目录路径的后缀是包的导入路径；例如包 gopl.io/ch1/helloworld 对应的目录路径是 $GOPATH/src/gopl.io/ch1/helloworld。 每个源文件的开头都是以包的声明语句开始，用来指明包的名字。 2.6.2. 包的初始化包的初始化首先是解决包级变量的依赖顺序，然后按照包级变量声明出现的顺序依次初始化： 123456var a = b + c // a 第三个初始化, 为 3var b = f() // b 第二个初始化, 为 2, 通过调用 f (依赖c)var c = 1 // c 第一个初始化, 为 1func f() int &#123; return c + 1 &#125; 如果包中含有多个 .go 源文件，它们将按照发给编译器的顺序进行初始化，Go 语言的构建工具首先会将 .go 文件根据文件名排序，然后依次调用编译器编译。 对于在包级别声明的变量，如果有初始化表达式则用表达式初始化，还有一些没有初始化表达式的，例如某些表格数据初始化并不是一个简单的赋值过程。在这种情况下，我们可以用一个特殊的 init 初始化函数来简化初始化工作。每个文件都可以包含多个 init 初始化函数 这样的 init 初始化函数除了不能被调用或引用外，其他行为和普通函数类似。在每个文件中的 init 初始化函数，在程序开始执行时按照它们声明的顺序被自动调用。 2.7. 作用域声明语句的作用域是指源代码中可以有效使用这个名字的范围。 不要将作用域和生命周期混为一谈。声明语句的作用域对应的是一个源代码的文本区域；它是一个编译时的属性。一个变量的生命周期是指程序运行时变量存在的有效时间段，在此时间区域内它可以被程序的其他部分引用；是一个运行时的概念。 句法块是由花括弧所包含的一系列语句，就像函数体或循环体花括弧包裹的内容一样。句法块内部声明的名字是无法被外部块访问的。这个块决定了内部声明的名字的作用域范围。 我们可以把块（block）的概念推广到包括其他声明的群组，这些声明在代码中并未显式地使用花括号包裹起来，我们称之为词法块。对全局的源代码来说，存在一个整体的词法块，称为全局词法块；对于每个包；每个 for、if 和 switch 语句，也都有对应词法块；每个 switch 或 select 的分支也有独立的词法块；当然也包括显式书写的词法块（花括弧包含的语句）。 在包级别，声明的顺序并不会影响作用域范围，因此一个先声明的可以引用它自身或者是引用后面的一个声明，这可以让我们定义一些相互嵌套或递归的类型或函数。","categories":[{"name":"Go 语言圣经","slug":"Go-语言圣经","permalink":"https://homholueng.github.io/categories/Go-语言圣经/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://homholueng.github.io/tags/Go/"}]},{"title":"Go 语言圣经 - 1. 入门","slug":"gopl-getting-started","date":"2019-07-18T13:59:05.000Z","updated":"2019-07-28T14:26:50.080Z","comments":true,"path":"2019/07/18/gopl-getting-started/","link":"","permalink":"https://homholueng.github.io/2019/07/18/gopl-getting-started/","excerpt":"","text":"1.2. 命令行参数通过 os.Args 切片能够拿到程序启动时得到的命令行参数： 123456789101112131415package mainimport \"fmt\" \"os\")func main() &#123; var s, sep string for _, arg := range os.Args &#123; s += sep + arg sep = \" \" &#125; fmt.Println(s)&#125; 运行以下命令会输出： 12$ go run echo1.go 1 2 3 4 5/var/folders/8v/11x3hg3x5g758rwj8kl4c42w0000gn/T/go-build732224661/b001/exe/echo1 1 2 3 4 5 1.3. 查找重复的行类似于 C 或其它语言里的 printf 函数，fmt.Printf 函数对一些表达式产生格式化输出。该函数的首个参数是个格式字符串，指定后续参数被如何格式化，Printf 有一大堆这种转换，Go 程序员称之为动词（verb）： 12345678910%d 十进制整数%x, %o, %b 十六进制，八进制，二进制整数。%f, %g, %e 浮点数： 3.141593 3.141592653589793 3.141593e+00%t 布尔：true或false%c 字符（rune） (Unicode码点)%s 字符串%q 带双引号的字符串&quot;abc&quot;或带单引号的字符&apos;c&apos;%v 变量的自然形式（natural format）%T 变量的类型%% 字面上的百分号标志（无操作数） 1.8. 本章要点在你开始写一个新程序之前，最好先去检查一下是不是已经有了现成的库可以帮助你更高效地完成这件事情。你可以在 https://golang.org/pkg 和 https://godoc.org 中找到标准库和社区写的 package。godoc 这个工具可以让你直接在本地命令行阅读标准库的文档。比如下面这个例子。 123456$ go doc http.ListenAndServepackage http // import \"net/http\"func ListenAndServe(addr string, handler Handler) error ListenAndServe listens on the TCP network address addr and then calls Serve with handler to handle requests on incoming connections....","categories":[{"name":"Go 语言圣经","slug":"Go-语言圣经","permalink":"https://homholueng.github.io/categories/Go-语言圣经/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://homholueng.github.io/tags/Go/"}]},{"title":"Chapter 12 - 继承的优缺点","slug":"fluent-python-ad-and-disad-of-inherit","date":"2019-07-13T10:05:21.000Z","updated":"2019-07-25T05:03:52.685Z","comments":true,"path":"2019/07/13/fluent-python-ad-and-disad-of-inherit/","link":"","permalink":"https://homholueng.github.io/2019/07/13/fluent-python-ad-and-disad-of-inherit/","excerpt":"","text":"12.1 子类化内置类型很麻烦使用 C 语言实现的内置类型不会调用用户顶以的类覆盖的特殊方法，如 dict 的子类覆盖的 __getitem__() 方法不会被内置类型的 get() 方法调用。 具体我们可以看看下面的例子： 1234567891011121314151617181920In [1]: class DoppelDict(dict): ...: def __setitem__(self, key, value): ...: super().__setitem__(key, [value] * 2) ...: In [2]: dd = DoppelDict(one=1) In [3]: dd Out[3]: &#123;'one': 1&#125;In [4]: dd['two'] = 2 In [5]: dd Out[5]: &#123;'one': 1, 'two': [2, 2]&#125;In [6]: dd.update(three=3) In [7]: dd Out[7]: &#123;'one': 1, 'two': [2, 2], 'three': 3&#125; 所以，直接实例化内置类型（如 dict，list 或 str）容易出错，用户自己定义的类，应该继承 collections 模块中的类，如 UserDict，UserList 和 UserString，这样就不会出现内置类型不调用子类覆盖的特殊方法的尴尬场面。 12.2 多重继承和方法的解析顺序Python 会按照特定的顺序遍历继承图来决定如何解析子类调用的方法，这个顺序叫做方法解析顺序（Method Resolution Order）。每个类都有一个名为 __mro__ 的属性，它的值是一个元组，按照方法解析顺序列出该类的各个超类，同时，MRO 还与类声明超类的顺序有关。 12.4 处理多重继承 把接口继承和实现继承区分开：继承接口是为了实现“是什么”的关系，继承实现是为了重用代码 使用抽象基类表示接口 通过混入重用代码：如果一个类的作用是为多个不相关的子类提供方法实现，从而实现重用，但不体现“是什么”关系，应该把那个类明确地定义为混入类（mixin class）。从概念上讲，混入不定义新类型，只是打包方法，便于重用。 在名称中明确指明混入：混用类的类名强烈建议使用 Mixin 作为后缀。 抽象基类可以作为混入，反过来则不成立 不要子类化多个具体类：具体类的超类中除了一个具体超类之外，其余的应该都是抽象基类或混入 为用户提供聚合类：如果抽象基类或混入的组合对客户代码非常有用，那就提供一个类，使用易于理解的方式把它们结合起来。 优先使用对象组合而不是类继承","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 11 - 函数装饰器和闭包","slug":"fluent-python-interface","date":"2019-05-04T12:23:55.000Z","updated":"2019-07-18T11:43:48.704Z","comments":true,"path":"2019/05/04/fluent-python-interface/","link":"","permalink":"https://homholueng.github.io/2019/05/04/fluent-python-interface/","excerpt":"","text":"11.1 Python 文化中的接口和协议接口在动态类型语言中是怎么运作的呢？首先，基本的事实是，Python 语言没有 interface 关键字，而且除了抽象基类，每个类都有接口： 类实现或继承的公开属性（方法或数据属性），包括特殊方法，如 __getitem__ 或 __add__。 另一方面，不要觉得把公开数据属性放入对象的接口中不妥，因为如果需要，总能实现读值方法和设值方法，把数据属性变成特性，使用 obj.attr 句法的客户代码不会受到影响。 关于接口，这里有个实用的补充定义：对象公开方法的子集，让对象在系统中扮演特定的角色。Python 文档中的“文件类对象”或“可迭代对象”就是这个意思，这种说法指的不是特定的类。接口是实现特定角色的方法集合，这样理解正是 Smalltalk 程序员所说的协议，其他动态语言社区都借鉴了这个术语。协议与继承没有关系。一个类可能会实现多个接口，从而让实例扮演多个角色。 协议是接口，但不是正式的（只由文档和约定定义），因此协议不能像正式接口那样施加限制。一个类可能只实现部分接口，这是允许的。 11.2 Python 喜欢序列下图展示了定义为抽象基类的 Sequence 正式接口： 现在让我们看看下面的代码中定义的 Foo 类，它没有继承 abc.Sequence，而且只实现了序列协议的一个方法：__getitem__： 12345678910111213141516In [1]: class Foo: ...: def __getitem__(self, pos): ...: return range(0, 30, 10)[pos] ...: In [2]: f = Foo() In [3]: f[1] Out[3]: 10In [4]: for i in f: print(i) 01020In [5]: 20 in f Out[5]: True 虽然没有实现 __iter__ 方法，但是 Foo 实例时可迭代的对象，因为发现其实现了 __getitem__ 方法时，Python 会调用它。 所以，鉴于序列协议的重要性，如果没有 __iter__ 和 __contains__ 方法，Python 会调用 __getitem__ 方法，设法让迭代和 in 运算符可用。 11.3 使用 Monkey Patch 在运行时实现协议猴子补丁：在运行时修改类或模块，而不改动源码。 猴子补丁很强大，但是打补丁的代码与要打补丁的程序耦合十分紧密，而且往往要处理隐藏和没有文档的部分。 11.4 白鹅类型（goose typing）白鹅类型指，只要 cls 是抽象基类，即 cls 的元类是 abc.ABCMeta，就可以使用 isinstance(obj, cls)。 collections.abc 中有很多有用的抽象类（Python 标准库的 numbers 模块中还有一些）。 11.5 定义抽象基类的子类在模块导入阶段（类对象初始化）时，Python 不会检查类中的抽象方法是否被实现，只有在实例化类时才会抛出异常。 11.6 标准库中的抽象基类collections.abc 模块中的抽象基类该模块中定义了 16 个抽象基类，如下图所示： Iterable、Container 和 Sized：各个集合应该继承这三个抽象基类，或者至少实现兼容的协议。 Sequence、Mapping 和 Set：这三个是主要的不可变集合类型，而且各自都有可变的子类。 MappingView：在 Python 3 中，映射方法 .items()、.keys() 和 .values() 返回的对象分别是 ItemsView、KeysView 和 ValuesView 的实例。 Callable 和 Hashable：这两个抽象基类与集合没有太大的关系，只不过因为 collections.abc 是标准库中定义抽象基类的第一个模块，而它们又太重要了，因此才把它们放到 collections.abc 模块中。 Iterator：迭代器，注意它是 Iterable 的子类。 每个类的具体说明可参考官方文档。 numbers 模块中的抽象基类numbers 包定义的是“数字塔”（即各个抽象基类的层次结构是线性的），其中 Number 是位于最顶端的超类，随后是 Complex 子类，依次往下，最底端是 Integral 类： Number Complex Real Rational Integral 与之类似，如果一个值可能是浮点数类型，可以使用 isinstance(x, numbers.Real) 检查。 11.7 定义并使用一个抽象基类抽象基类的父类自定义的抽象基类要继承 abc.ABC，在 Python3.4 之前，由于没有 abc.ABC 类，需要在 class 语句中使用 metaclas= 关键字,并将值设置为 abc.ABCMeta： 12class AbstractClass(metaclass=abc.ABCMeta): pass 而在 Python 2 中，由于没有 metaclass= 关键字，必须使用 __metaclass__ 类属性： 12class AbstractClass(object): __metaclass__ = abc.ABCMeta 抛出异常在自定义类中抛出异常是，可以考虑复用 Python 中预先定义好的异常，具体可参考异常层次结构。 抽象方法与其他方法描述符一起使用时，@abstractmethod 应该放在最里层。另外，@abstractclassmethod、@abstractstaticmethod 和 @abstractproperty 三个装饰器从 Python3.3 起就废弃掉了，所以，推荐使用以下方式声明抽象类方法： 12345class AbstractClass(abc.ABC): @classmethod @abc.abstractmethod def an_abstract_classmethod(cls): pass 虚拟子类注册虚拟子类的方式是在抽象基类上调用 register 方法。这么做之后，注册的类会变成抽象基类的虚拟子类，而且 issubclass 和 isinstance 等函数都能识别，但是注册的类不会从抽象基类中继承任何方法或属性。 像这样： 123@SuperClass.registerclass VirtualSubclass(list): pass 如果是 Python 3.3 或之前的版本，不能把 .register 当作类装饰器使用，必须使用标准的调用句法： 1234class VirtualSubclass(list): passSuperClass.register(TomboList) 虽然我们注册了虚拟子类，但是其 __mro__ 中并不会列出该子类的虚拟超类。并且，一个类的 __subclasses__ 方法只会返回该类的直接子类列表，而 _abc_registry 会返回一个包含该抽象类的所有虚拟子类的 WeakSet 对象。 鹅的行为有可能像鸭子在 Python 中，即便不注册虚拟子类，抽象基类也能把一个类识别为虚拟子类： 1234567891011class Struggle: def __len__(self): return 23In [5]: from collections import abcIn [7]: isinstance(Struggle(), abc.Sized)Out[7]: TrueIn [8]: issubclass(Struggle, abc.Sized)Out[8]: True 出现这种现象的原因是 abc.Sized 实现了一个特殊的类方法：__subclasshook__： 1234567891011121314class Sized(metaclass=ABCMeta): __slots__ = () @abstractmethod def __len__(self): return 0 @classmethod def __subclasshook__(cls, C): if cls is Sized: if any(\"__len__\" in B.__dict__ for B in C.__mro__): return True return NotImplemented 也就是说，其会检测传入类对象本身及其所继承的类中是否能够处理 __len__ 方法的调用，如果可以，即认为该类是自身的子类。 __subclasshook__ 在白鹅类型中添加了一些鸭子类型的踪迹。我们可以使用抽象基类定义正式接口，可以始终使用 isinstance 检查，也可以完全使用不相关的类，只要实现特定的方法即可（或者做些事情让 __subclasshook__ 信服）。 但是，不建议在我们自定义的抽象类中实现该方法，这可能会使得你的类可靠性变得很低。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 10 - 序列的修改、散列和切片","slug":"fluent-python-sequence","date":"2019-04-27T04:07:46.000Z","updated":"2019-07-18T11:43:48.703Z","comments":true,"path":"2019/04/27/fluent-python-sequence/","link":"","permalink":"https://homholueng.github.io/2019/04/27/fluent-python-sequence/","excerpt":"","text":"10.2 与旧的类兼容 序列类型的构造方法最好接受可迭代的对象为参数，因为所有内置的序列类型都是这样做的。 reprlib 能够为某些对象生成长度有限的表示形式: 12345678910111213141516171819In [1]: import reprlibIn [3]: aOut[3]:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ... 999]In [4]: reprlib.repr(a)Out[4]: '[0, 1, 2, 3, 4, 5, ...]' 注意，在 Python2 中，reprlib 模块的名字是 repr。 调用 repr() 函数的目的是调试，因此绝对不能抛出异常。如果 __repr__ 方法的实现有问题，那么必须处理，尽量输出有用的内容，让用户能够识别目标对象。 10.3 协议和鸭子类型在面向对象编程中，协议是非正式的接口，只在文档中定义，在代码中不定义。例如，Python 的序列协议只需要 __len__ 和 __getitem__ 两 方法。任何类（如 Spam），只要使用标准的签名和语义实现了这两个方法，就能用在任何期待序列的地方。Spam 是不是哪个类的子类无关紧要，只要提供了所需的方法即可。 因此，Spam 的行为就像是一个序列，人们称其为鸭子类型（duck typing）。 协议是非正式的，没有强制力，因此如果你知道类的具体使用场景，通常只需要实现一个协议的部分。例如，为了支持迭代，只需实现 __getitem__ 方法，没必要提供 __len__ 方法。 协议可以视为鸭子类型语言使用的非正式接口。 10.4 可切片的序列10.4.1 切片原理123456789101112131415161718192021In [5]: class MySeq: ...: def __getitem__(self, index): ...: return index ...:In [6]: s = MySeq()In [7]: s[1]Out[7]: 1In [8]: s[1:4]Out[8]: slice(1, 4, None)In [9]: s[1:4:2]Out[9]: slice(1, 4, 2)In [10]: s[1:4:2, 9]Out[10]: (slice(1, 4, 2), 9) In [11]: s[1:4:2, 7:9]Out[11]: (slice(1, 4, 2), slice(7, 9, None)) 这些步骤需要重点说明： 9：从 1 开始，到 4 结束，步幅为 2。 10：如果 [] 中有逗号，那么 __getitem__ 收到的是元组。 11：元组中可以包含多个切片对象。 这里值得一提的是，slice 内置了 indices 方法，该方法开放了内置序列实现的棘手逻辑，用于优雅地处理缺失索引和负数索引，以及长度超过目标序列的切片： 1S.indices(len) -&gt; (start, stop, stride) 给定长度为 len 的序列，计算 S 表示的扩展切片的起始（start）和结尾（stop），以及步幅（stride）。超出边界的索引会被截掉，这与常规切片的处理方式相同。我们来看看这个方法的实际效果： 12345In [12]: slice(None, 10, 2).indices(5)Out[12]: (0, 5, 2)In [13]: slice(-3, None, None).indices(5)Out[13]: (2, 5, 1) 10.4.2 能处理切片的 __getitem__ 方法下面展示了一个将 __getitem__ 委托给内部序列的实现： 1234567891011def __getitem__(self, index): cls = type(self) if isinstance(index, slice): # 1 return cls(self._components[index]) # 2 elif isinstance(index, numbers.Integral): return self._components[index] else: msg = '&#123;cls.__name__&#125; indices must be integers' raise TypeError(msg.format(cls=cls)) 这里需要说明这些细节： 1：对于切片操作，返回自身类型的实例而不是内部序列的切片。 2：测试 index 是否是整数时用的是 numbers.Integral，这是一个抽象基类。在 isinstance 中使用抽象基类做测试能让 API 更灵活且更容易更新。 10.5 动态存取属性属性查找失败后，解释器会调用 __getattr__ 方法。简单来说，对 my_obj.x 表达式，Python 会检查 my_obj 实例有没有名为 x 的属性；如果没有，到类（my_obj.__class__）中查找；如果还没有，顺着继承树继续查找。如果依旧找不到，调用 my_obj 所属类中定义的 __getattr__ 方法，传入 self 和属性名称的字符串形式（如 &#39;x&#39;）。 10.6 散列和快速等值如果要快速比较两个序列是否相同，可以使用 zip 函数来实现： 123def __eq__(self, other): return len(self) == len(other) and \\ all(a == b for a, b in zip(self, other))","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"PEP 302 - New Import Hooks","slug":"pep-302-new-imoprt-hooks-md","date":"2019-04-21T08:36:44.000Z","updated":"2019-04-23T15:34:53.076Z","comments":true,"path":"2019/04/21/pep-302-new-imoprt-hooks-md/","link":"","permalink":"https://homholueng.github.io/2019/04/21/pep-302-new-imoprt-hooks-md/","excerpt":"","text":"Abstract该提议为了使得开发者能够对 Python 的模块导入机制进行更好的自定义，提出了一种新的导入钩子，这种新型的机制能够更好注入到现有的导入机制中，并且对模块的查找和导入提供了更细粒度的控制。 Motivation在该提议出现之前，如果开发者要自定义模块的导入机制，需要重载内置的 __import__ 函数，但是重载该函数十分麻烦，需要完全实现整个 import 的机制，还需要处理各种复杂的情况。 Use Case该提议提出的方案用于加载一些与一般模块存储方式不同的模块，例如打包好的模块，pyc 文件，从网络或数据库中读取的模块等等。 Rationale较早的一个方案是在 sys.path 中添加非 str 的类来作为 hook，但这是一种十分丑陋的 hack（ugly hack），原因如下： 破坏了 sys.path 中数据类型的一致性。 与 PYTHONPATH 环境变量不兼容。 该提议提出的钩子机制设计如下：在 sys 模块中添加一个新的 path_hooks 列表，该列表中的对象会被遍历以检测其是否能够处理 sys.path 中的某一条记录，直到找到能够处理的对象为止。但是每进行一次导入操作就遍历一次 sys.path_hooks 的话对性能会有一定的影响，所以遍历 sys.path_hooks 的结果会被缓存到 sys.path_importer_cache 中，该缓存是 sys.path 中的记录到 path_hooks 返回的模块导入器的映射。 为了最小化对 import.c 的影响以及避免增加额外的性能开销。该方案默认不会为现有的文件系统导入逻辑添加显式的钩子和导入器对象，如果 sys.path_hooks 中没有任何一个对象能够处理 sys.path 中的记录的话，就会回归到内置的导入逻辑。同时，这条记录在 sys.path_importer_cache 中映射会被置为 None，避免再次进行 sys.path_hooks 的遍历。 那么问题来了：对于那些不需要处理 sys.path 的自定义导入器应该如何处理呢？ 为了解决这个问题，该提议还给出了另外一个新的机制：在 sys 模块中添加一个新的导入器列表 meta_path，这个列表中的导入器会在 sys.path 之前进行遍历并询问其是否能够处理即将要被导入的模块，该列表默认为空。 Specification part 1: The Importer ProtocolPython 解释器在遇到导入模块的语句时，会调用 __import__ 函数，解释器会向其传递四个参数，其中包括被导入的模块以及当前全局命名空间。然后 __import__ 函数会检测正在进行导入操作的模块是一个包还是包中的子模块。如果它是一个包，那么 __import__ 会先尝试从相对路径下进行导入，如果是包中的子模块，那么会尝试从该模块的父包中进行导入。而且，类似 import spam.ham 这样的导入操作会在 import spam 完成后，才会将 ham 作为 spam 的子模块进行导入。 而导入器协议（importer protocol）会在更高一层执行，当一个 importer 收到 spam.ham 的导入请求时，说明 spam 模块已经被成功导入了，而导入协议中定义了两种对象：finder 和 loader。 FinderFinder 对象必须实现以下方法： 1finder.find_module(fullname, path=None) 传入的 fullname 是模块的完全限定名（e.g. spam.ham）。而对于第二个参数 path，如果当前导入的模块是顶层模块，那么 path 的值为 None；如果当前导入的不是顶层模块，那么 path 的值是 package.__path__。 如果 Finder 能够处理当前正在导入的模块，其 find_module() 方法应该返回一个 Loader 对象；反之应该返回 None。 该方法中抛出的异常会传播给调用者，且本次导入操作会被放弃。 LoaderFinder 对象必须实现以下方法： 1loader.load_module(fullname) 传入的 fullname 是模块的完全限定名（e.g. spam.ham）。 该方法应该返回一个已经加载完成的模块或抛出 ImporteError 异常；如果要求该方法加载不能被当前 Loader 加载的模块，也应该抛出 ImporteError 异常。而且，load_module() 方法在其执行模块中的代码之前必须履行以下职责： 如果 sys.modules 中存在名为 fullname 模块对象，Loader 必须使用该现有模块（否则 reload（）将无法正常工作）。反之，如果 sys.modules 中不存在名为 fullname 的模块，则Loader 必须创建一个新模块对象并将其添加到 sys.modules 中。注意，在 Loader 执行模块代码之前，必须先将模块对象添加到 sys.modules 中，因为模块可以（直接或间接）导入自身；事先将它添加到 sys.modules 可以防止无限制的递归或是多次加载。如果模块加载失败， Loader 需要删除它可能已插入 sys.modules 的任何模块；如果模块已经位于 sys.modules 中，那么 Loader 可以不处理该模块。 必须为模块设置 __file__ 属性，且类型必须是字符串，但它可以是一个虚拟的值，例如 “&lt;frozen&gt;”。 必须为模块设置 __name__ 属性。 如果导入的是包，那么必须设置 __path__ 属性，且该类型必须是 list。 必须将模块的 __loader__ 属性设置为 Laoder 自身，用于自省和重载，也可用于获取 importer 的信息。 必须微模块设置 __package__ 属性。 以下是一个 load_module() 的一个实现模板： 12345678910111213def load_module(self, fullname): code = self.get_code(fullname) ispkg = self.is_package(fullname) mod = sys.modules.setdefault(fullname, imp.new_module(fullname)) mod.__file__ = \"&lt;%s&gt;\" % self.__class__.__name__ mod.__loader__ = self if ispkg: mod.__path__ = [] mod.__package__ = fullname else: mod.__package__ = fullname.rpartition('.')[0] exec(code, mod.__dict__) return mod Specification part 2: Registering Hooks在新的导入钩子机制下，存在两种类型的钩子： Meta hooks：在导入过程最开始的时候调用；通过将 Finder 对象添加到 sys.meta_path 中实现注册。 Path hooks：在处理 sys.path 过程中遇到相关路径的时候调用；通过将导入器工厂添加到 sys.path_hooks 中实现注册。 sys.path_hooks 是一个可调用对象（导入器工厂）的列表，这些对象会以 sys.path 中的每一条数据作为参数进行调用。如果某个对象能够处理当前传入的 path，那么其应该返回一个导入器对象；反之则应该抛出 ImportError。 如之前章节中所描述的，这些导入器对象会被缓存在 sys.path_importer_cache 中，如果在某些情况下需要重新执行一次 Path hook，可以手动删除 sys.path_importer_cache 某些映射。 Packages and the role of __path__如果一个模块拥有 __path__ 属性的话，那么 Python 的导入机制就会将该模块当成包来对待。当导入该模块下的子模块时，会使用该模块的 __path__ 属性而不是 sys.path 中的某一项，而应用在 sys.path 上的规则同样适用于 pkg.__path__，也就是说在遍历 pkg.__path__ 时，也会查询 sys.path_hooks 是否有能够处理的导入器工厂。 Optional Extensions to the Importer Protocol1loader.get_data(path) 该方法应该从底层存储中获取文件的内容并返回；如果文件不存在的话，应该抛出 IOError。 1loader.is_package(fullname) 该方法应该判断指定模块是否是一个包。 1loader.get_code(fullname) 该方法应该返回与指定模块相符的 code 对象；如果指定模块是内置或扩展模块的话，则返回 None。 1loader.get_source(fullname) 该方法应该返回指定模块的源码；如果无法获取该模块的源码，则返回 None；如果当前 Loader 无法处理该模块，则应该抛出 ImportError。 1loader.get_filename(fullname) 该方法应该返回指定模块的 __file__ 属性的值；如果模块不存在，则应该抛出 ImportError。","categories":[{"name":"Python Daily","slug":"Python-Daily","permalink":"https://homholueng.github.io/categories/Python-Daily/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 4 - 更多的 bash shell 命令","slug":"linux-cl-and-shell-more-bash-shell-cmd","date":"2019-04-20T07:06:12.000Z","updated":"2019-04-20T07:30:41.784Z","comments":true,"path":"2019/04/20/linux-cl-and-shell-more-bash-shell-cmd/","link":"","permalink":"https://homholueng.github.io/2019/04/20/linux-cl-and-shell-more-bash-shell-cmd/","excerpt":"","text":"管理进程4.1 检测程序4.1.1 探查进程Linux 系统中使用的 GNU ps 命令支持3种不同类型的命令行参数： Unix 风格的参数，前面加单破折线 BSD 风格的参数，前面不加破折线 GNU 风格的长参数，前面加双破折线 Unix 风格的参数使用 ps 命令的关键不在于记住所有可用的参数，而在于记住最有用的那些参数。 如果你想查看系统上运行的所有进程，可用 -ef 参数组合，-e 参数指定显示所有运行在系统上的进程；-f 参数则扩展了输出： 123456789$ ps -ef UID PID PPID C STIME TTY TIME CMD 0 1 0 0 29 119 ?? 4:42.80 /sbin/launchd 0 66 1 0 29 119 ?? 0:07.86 /usr/sbin/syslogd 0 67 1 0 29 119 ?? 0:04.20 /usr/libexec/UserEventAgent (System) 0 70 1 0 29 119 ?? 0:03.37 /System/Library/PrivateFrameworks/Uninstall.framework/Resources/uninstalld 0 71 1 0 29 119 ?? 0:04.53 /usr/libexec/kextd 0 72 1 0 29 119 ?? 0:49.94 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/Support/fseventsd ... UID：启动这些进程的用户 PID：进程的进程 ID PPID：父进程的进程 ID C：进程声明周期中的 CPU 利用率 STIME：进程启动时的系统时间 TTY：进程启动时的终端设备 TIME：运行进程需要的累积 CPU 时间 CMD：启动进程的程序名 如果需要获取更多信息，可以使用 -l 参数： 1234567$ ps -l UID PID PPID F CPU PRI NI SZ RSS WCHAN S ADDR TTY TIME CMD 501 10829 10827 4006 0 31 0 4318368 14308 - Ss 0 ttys001 0:00.06 /Applications/iTerm (3.1.5.beta 501 10831 10830 4006 0 31 0 4298784 1340 - S 0 ttys001 0:00.76 -zsh 501 11226 10831 4006 0 31 0 558515572 21996 - S+ 0 ttys001 0:01.33 docker run -i -t ubuntu /bin/ba 501 11228 10827 4006 0 31 0 4318368 14308 - Ss 0 ttys002 0:00.07 /Applications/iTerm (3.1.5.beta 501 11230 11229 4006 0 31 0 4297760 3804 - S 0 ttys002 0:00.62 -zsh F：内核分配给进程的系统标记 S：进程的状态（O 代表正在运行；S 代表在休眠；R 代表可运行，正等待运行；Z 代表僵化，进程已结束但父进程已不存在；T 代表停止） PRI：进程的优先级（越大的数字代表越低的优先级） NI：谦让度值用来参与决定优先级 ADDR：进程的内存地址 SZ：假如进程被换出，所需交换空间的大致大小 WCHAN：进程休眠的内核函数的地址 4.1.2 实时监测进程ps 命令虽然在收集运行在系统上的进程信息时非常有用，但也有不足之处：它只能显示 某个特定时间点的信息。如果想观察那些频繁换进换出的内存的进程趋势，用 ps 命令就不方 便了。 而 top 命令刚好适用这种情况。top 命令跟 ps 命令相似，能够显示进程信息，但它是实时显示的。 其显示内容如下 123456789top - 07:15:02 up 1 day, 8:20, 0 users, load average: 0.28, 0.38, 0.40Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.5 us, 2.5 sy, 0.0 ni, 96.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2047036 total, 188520 free, 968420 used, 890096 buff/cacheKiB Swap: 1048572 total, 1037316 free, 11256 used. 913360 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 18504 3376 2964 S 0.0 0.2 0:00.05 bash 11 root 20 0 36616 3076 2592 R 0.0 0.2 0:00.01 top part 1输出的第一部分显示的是系统的概况：第一行显示了当前时间、系统的运行时间、登录的用户数以及系统的平均负载 平均负载有3个值：最近1分钟的、最近5分钟的和最近15分钟的平均负载。值越大说明系统的负载越高。 part 2第二行显示了进程概要信息——top 命令的输出中将进程叫作任务（task）：有多少进程处在运行、休眠、停止或是僵化状态（僵化状态是指进程完成了，但父进程没有响应）。 part 3下一行显示了 CPU 的概要信息。top 根据进程的属主（用户还是系统）和进程的状态（运行、空闲还是等待）将 CPU 利用率分成几类输出。 part 4紧跟其后的两行说明了系统内存的状态。第一行说的是系统的物理内存：总共有多少内存，当前用了多少，还有多少空闲。后一行说的是同样的信息，不过是针对系统交换空间（如果分配了的话）的状态而言的。 part 5最后一部分显示了当前运行中的进程的详细列表，有些列跟 ps 命令的输出类似。 PID：进程的 ID USER：进程属主的名字 PR：进程的优先级 NI：进程的谦让度值 VIRT：进程占用的虚拟内存总量 RES：进程占用的物理内存总量 SHR：进程和其他进程共享的内存总量 S：进程的状态（D代表可中断的休眠状态，R代表在运行状态，S代表休眠状态，T代表跟踪状态或停止状态，Z代表僵化状态） %CPU：进程使用的CPU时间比例 %MEM：进程使用的内存占可用内存的比例 TIME+：自进程启动到目前为止的 CPU 时间总量 COMMAND：进程所对应的命令行名称，也就是启动的程序名 默认情况下，top 命令在启动时会按照 %CPU 值对进程排序。键入 f 允许你选择对输出进行排序的字段，键入 d 允许你修改轮询间隔。 4.1.3 结束进程在 Linux 中，进程之间通过信号来通信。进程的信号就是预定义好的一个消息，进程能识别它并决定忽略还是作出反应。进程如何处理信号是由开发人员通过编程来决定的。 在 Linux 中有两个命令可以向运行中的进程发出进程信号。 1. kill 命令kill 命令可通过进程 ID（PID）给进程发信号。默认情况下，kill 命令会向命令行中列出的全部 PID 发送一个 TERM 信号。而且要发送进程信号，你必须是进程的属主或登录为 root 用户。 使用 -s 参数能够指定其他信号（用信号名或信号值）。 1# kill -s HUP 3940 2. killall 命令killall 命令非常强大，它支持通过进程名而不是 PID 来结束进程。killall 命令也支持通配符，这在系统因负载过大而变得很慢时很有用。 1# killall http* 上例中的命令结束了所有以 http 开头的进程，比如 Apache Web 服务器的 httpd 服务。 4.2 监测磁盘空间4.2.1 挂载存储媒体如第3章中讨论的，Linux文件系统将所有的磁盘都并入一个虚拟目录下。在使用新的存储媒体之前，需要把它放到虚拟目录下。这项工作称为挂载（mounting）。 在今天的图形化桌面环境里，大多数 Linux 发行版都能自动挂载特定类型的可移动存储媒体。如果用的发行版不支持自动挂载和卸载可移动存储媒体，就必须手动完成。 1. mount 命令Linux 上用来挂载媒体的命令叫作 mount。默认情况下，mount 命令会输出当前系统上挂载的设备列表。 1234567$ mount (env: usr)/dev/disk1s1 on / (apfs, local, journaled)devfs on /dev (devfs, local, nobrowse)/dev/disk1s4 on /private/var/vm (apfs, local, noexec, journaled, noatime, nobrowse)map -hosts on /net (autofs, nosuid, automounted, nobrowse)map auto_home on /home (autofs, automounted, nobrowse)map -fstab on /Network/Servers (autofs, automounted, nobrowse) mount 命令提供如下四部分信息： 媒体的设备文件名 媒体挂载到虚拟目录的挂载点 文件系统类型 已挂载媒体的访问状态 要手动在虚拟目录中挂载设备，需要以 root 用户身份登录，或是以 root 用户身份运行 mount 命令。下面是手动挂载媒体设备的基本命令： 1mount -t type device directory type 参数指定了磁盘被格式化的文件系统类型。 2. umount 命令从 Linux 系统上移除一个可移动设备时，不能直接从系统上移除，而应该先卸载。 卸载设备的命令是 umount，其格式非常简单： 1umount [directory | device ] umount 命令支持通过设备文件或者是挂载点来指定要卸载的设备。 如果在卸载设备时，系统提示设备繁忙，无法卸载设备，通常是有进程还在访问该设备或使用该设备上的文件。这时可用 lsof 命令获得使用它的进程信息，然后在应用中停止使用该设备或停止该进程。lsof命令的用法很简单：lsof /path/to/device/node，或者 lsof /path/to/mount/point。 4.2.2 使用 df 命令有时你需要知道在某个设备上还有多少磁盘空间。df 命令可以让你很方便地查看所有已挂载磁盘的使用情况。 123456789root@bd6467bc76d1:/# dfFilesystem 1K-blocks Used Available Use% Mounted onoverlay 65792556 21643556 40777224 35% /tmpfs 65536 0 65536 0% /devtmpfs 1023516 0 1023516 0% /sys/fs/cgroup/dev/sda1 65792556 21643556 40777224 35% /etc/hostsshm 65536 0 65536 0% /dev/shmtmpfs 1023516 0 1023516 0% /proc/acpitmpfs 1023516 0 1023516 0% /sys/firmware 每一列的含义如下： 设备的设备文件位置 能容纳多少个1024字节大小的块 已用了多少个1024字节大小的块 还有多少个1024字节大小的块可用 已用空间所占的比例 设备挂载到了哪个挂载点上 一个常用的参数是 -h。它会把输出中 的磁盘空间按照用户易读的形式显示。 使用 du 命令du 命令可以显示某个特定目录（默认情况下是当前目录）的磁盘使用情况。这一方法可用来快速判断系统上某个目录下是不是有超大文件。 1234567891011121314root@bd6467bc76d1:/usr# du11272 ./bin504 ./share/info4 ./share/terminfo8 ./share/bug/init-system-helpers8 ./share/bug/apt8 ./share/bug/procps28 ./share/bug16 ./share/locale/pa/LC_MESSAGES20 ./share/locale/pa256 ./share/locale/ja/LC_MESSAGES260 ./share/locale/ja188 ./share/locale/sk/LC_MESSAGES... 每行输出左边的数值是每个文件或目录占用的磁盘块数。注意，这个列表是从目录层级的最底部开始，然后按文件、子目录、目录逐级向上。 下面是能让du命令用起来更方便的几个命令行参数： -c：显示所有已列出文件总的大小 -h：按用户易读的格式输出大小 -d n：指定显示的目录层级深度 4.3 处理数据文件4.3.1 排序数据处理大量数据时的一个常用命令是 sort 命令。默认情况下，sort 命令按照会话指定的默认语言的排序规则对文本文件中的数据行排序。 123456789101112$ cat file1onetwothreefourfive$ sort file1fivefouronethreetwo 默认情况下，sort 命令会把数字当做字符来执行标准的字符排序，产生的输出可能根本就不是你要的。解决这个问题可用 -n 参数，它会告诉 sort 命令不要把数字识别成字符，并且按值排序。 -k 和 -t 参数在对按字段分隔的数据进行排序时非常有用，例如 /etc/passwd 文件。可以用 -t 参数来指定字段分隔符，然后用 -k 参数来指定排序的字段。 12345678$ sort -t ':' -k 3 -n /etc/passwdroot:*:0:0:System Administrator:/var/root:/bin/shdaemon:*:1:1:System Services:/var/root:/usr/bin/false_uucp:*:4:4:Unix to Unix Copy Protocol:/var/spool/uucp:/usr/sbin/uucico_taskgated:*:13:13:Task Gate Daemon:/var/empty:/usr/bin/false_networkd:*:24:24:Network Services:/var/networkd:/usr/bin/false_installassistant:*:25:25:Install Assistant:/var/empty:/usr/bin/false... 下面是一个将 du 和 sort 命令组合起来使用的例子（sort 的 -r 参数将结果按降序输出）： 12345678910root@bd6467bc76d1:/usr# du -d 1 | sort -nr50612 .26148 ./lib11272 ./bin11260 ./share1872 ./sbin44 ./local4 ./src4 ./include4 ./games 4.3.2 搜索数据你会经常需要在大文件中找一行数据，而这行数据又埋藏在文件的中间。这时并不需要手动翻看整个文件，用 grep 命令来帮助查找就行了。grep 命令的命令行格式如下。默认情况下，grep 命令用基本的 Unix 风格正则表达式来匹配模式。 1grep [options] pattern [file] grep 命令会在输入或指定的文件中查找包含匹配指定模式的字符的行。grep 的输出就是包含了匹配模式的行。 如果要进行反向搜索（输出不匹配该模式的行），可加 -v 参数。 如果要显示匹配模式的行所在的行号，可加-n 参数。 如果只要知道有多少行含有匹配的模式，可用 -c 参数。 如果要指定多个匹配模式，可用 -e 参数来指定每个模式： 12345$ grep -e t -e f file1twothreefourfive egrep 命令是 grep 的一个衍生，支持 POSIX 扩展正则表达式。POSIX 扩展正则表达式含有更多的可以用来指定匹配模式的字符。 fgrep 则是另外一个版本，支持将匹配模式指定为用换行符分隔的一列固定长度的字符串。这样就可以把这列字符串放到一个文件中，然后在 fgrep 命令中用其在一个大型文件中搜索字符串了。 4.3.3 压缩数据gzip 是 Linux 上最流行的压缩工具。 其包含的工具如下： gzip：用来压缩文件 gzcat：用来查看压缩过的文本文件内容 gunzip：用来解压文件 4.3.4 归档数据目前，Unix 和 Linux 上最广泛使用的归档工具是 tar 命令。 1tar function [options] object1 object2 ... function 参数定义了 tar 命令应该做什么： -A：将一个已有 tar 归档文件追加到另一个已有 tar 归档文件中 -c：创建一个新的 tar 归档文件 -r：追加文件到已有 tar 归档文件末尾 -t：列出已有 tar 归档文件的内容 -u：将比 tar 归档文件中已有的同名文件新的文件追加到该 tar 归档文件中 -x：从已有 tar 归档文件中提取文件 上述每个功能可用选项来针对 tar 归档文件定义一个特定行为： -c：切换到指定目录 -f：输出结果到文件或设备文件 -j：将输出重定向给 bzip2 命令来压缩内容 -p：保留所有文件权限 -v：在处理文件时显示文件 -z：将输出重定向给 gzip 命令来压缩内容 下载了开源软件之后，你会经常看到文件名以 .tgz 结尾。这些是 gzip 压缩过的 tar 文件，可以用命令 tar -zxvf filename.tgz 来解压。","categories":[{"name":"Linux CL and Shell Programing","slug":"Linux-CL-and-Shell-Programing","permalink":"https://homholueng.github.io/categories/Linux-CL-and-Shell-Programing/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://homholueng.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://homholueng.github.io/tags/Shell/"}]},{"title":"Chapter 3 - 基本的 bash shell 命令","slug":"linux-cl-and-shell-basic-bash-shell-cmd","date":"2019-04-20T07:05:03.000Z","updated":"2019-04-20T07:30:41.783Z","comments":true,"path":"2019/04/20/linux-cl-and-shell-basic-bash-shell-cmd/","link":"","permalink":"https://homholueng.github.io/2019/04/20/linux-cl-and-shell-basic-bash-shell-cmd/","excerpt":"","text":"3.4 浏览文件系统3.4.1 Linux 文件系统Linux 虚拟目录中比较复杂的部分是它如何协调管理各个存储设备。在 Linux PC 上安装的第 一块硬盘称为根驱动器。根驱动器包含了虚拟目录的核心，其他目录都是从那里开始构建的。 Linux 会在根驱动器上创建一些特别的目录，我们称之为挂载点（mount point）。挂载点是虚拟目录中用于分配额外存储设备的目录。虚拟目录会让文件和目录出现在这些挂载点目录中，然 而实际上它们却存储在另外一个驱动器中。 3.6 处理文件3.6.1 创建文件使用 touch 命令不仅能够创建，文件，还能够改变文件的修改时间： 123456$ ls -l test_one -rw-rw-r-- 1 christine christine 0 May 21 14:17 test_one $ touch test_one $ ls -l test_one -rw-rw-r-- 1 christine christine 0 May 21 14:35 test_one $ 要注意的是，如果只使用 ls –l 命令，并不会显示访问时间。这是因为默 认显示的是修改时间。要想查看文件的访问时间，需要加入另外一个参数：--time=atime。有 了这个参数，就能够显示出已经更改过的文件访问时间。 3.8 查看文件内容3.8.1 查看文件类型file 命令是一个随手可得的便捷工具, 不仅能确定文件中包含的文本信息，还能确定该文本文件的字符编码。 3.8.2 查看整个文件cat 命令加上 -n 参数会在展示文件内容的同时加上行号。 如果想只给有文本的地方加上行号，可以使用 -b 选项。 3.8.3 查看部分文件使用 tail 命令会显示文件最后几行的内容，默认情况下，它会显示文件的末尾10行。 -f 参数是 tail 命令的一个突出特性。它允许你在其他进程使用该文件时查看文件的内容。tail 命令会保持活动状态，并不断显示添加到文件中的内容。 而 head 命令会现实文件开头的内容，默认情况下，它会显示文件前10行的文本。","categories":[{"name":"Linux CL and Shell Programing","slug":"Linux-CL-and-Shell-Programing","permalink":"https://homholueng.github.io/categories/Linux-CL-and-Shell-Programing/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://homholueng.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://homholueng.github.io/tags/Shell/"}]},{"title":"Chapter 2 - 走进 shell","slug":"linux-cl-and-shell-into-shell","date":"2019-04-20T07:04:15.000Z","updated":"2019-04-20T07:30:41.783Z","comments":true,"path":"2019/04/20/linux-cl-and-shell-into-shell/","link":"","permalink":"https://homholueng.github.io/2019/04/20/linux-cl-and-shell-into-shell/","excerpt":"","text":"2.1 进入命令行在图形化桌面出现之前，与 Unix 系统进行交互的唯一方式就是借助由 shell 所提供的文本命令行界面（command line interface，CLI）。CLI 只能接受文本输入，也只能显示出文本和基本的图形输出。 进入 CLI 有两种方式： 让 Linux 退出图形化桌面模式，进入文本模式，这种模式称作 Linux 控制台，因为其模拟了早起的硬接线控制台终端。Linux 系统启动后，它会自动创建出一些虚拟控制台。 使用 Linux 图形化桌面环境中终端仿真包。终端仿真包会在一个桌面图形化窗口中模拟控制台终端的使用。","categories":[{"name":"Linux CL and Shell Programing","slug":"Linux-CL-and-Shell-Programing","permalink":"https://homholueng.github.io/categories/Linux-CL-and-Shell-Programing/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://homholueng.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://homholueng.github.io/tags/Shell/"}]},{"title":"Chapter 1 - 初识 Linux shell","slug":"linux-cl-and-shell-meet-linux-shell","date":"2019-04-20T07:02:47.000Z","updated":"2019-04-20T07:30:41.783Z","comments":true,"path":"2019/04/20/linux-cl-and-shell-meet-linux-shell/","link":"","permalink":"https://homholueng.github.io/2019/04/20/linux-cl-and-shell-meet-linux-shell/","excerpt":"","text":"1.1 什么是 LinuxLinux 可以划分为四个部分： Linux 内核 GNU 工具 图形化桌面软件 应用软件 1.1.1 深入探究 Linux 内核内核主要负责以下四种功能： 系统内存管理 软件程序管理 硬件设备管理 文件系统管理 Linux 系统将硬件设备划分为三种： 字符型设备文件：每次只能处理一个字符的设备。 块设备文件：每次能处理大块数据的设备，比如硬盘。 网络设备文件：采用数据包发送和接受数据的设备，包括各种网卡和一个特殊的回环设 备。这个回环设备允许 Linux 系统使用常见的网络编程协议同自身通信。 1.1.2 GNU 工具除了由内核控制硬件设备外，操作系统还需要工具来执行一些标准功能，比如控制文件和 程序。 GNU 组织（GNU’s Not Unix）开发了一套完整的 Unix 工具，但没有可以运行它们的内核系统。 将 Linus 的 Linux 内核和 GNU 操作系统工具整合起来，就产生了一款完整的、功能丰富的免费操作系统。 尽管通常将 Linux 内核和 GNU 工具的结合体称为 Linux，但你也会在互联网上看到一些 Linux 纯粹主义者将其称为 GNU/Linux 系统，藉此向 GNU 组织所作的贡献致意。 1. 核心 GNU 工具GNU 项目的主旨在于为Unix系统管理员设计出一套类似于 Unix 的环境。这个目标促使该项目移植了很多常见的 Unix 系统命令行工具。供 Linux 系统使用的这组核心工具被称为 coreutils（core utilities）软件包。 2. shellGNU/Linux shell 是一种特殊的交互式工具。它为用户提供了启动程序、管理文件系统中的文 件以及运行在 Linux 系统上的进程的途径。shell 的核心是命令行提示符。命令行提示符是 shell 负责交互的部分。它允许你输入文本命令，然后解释命令，并在内核中执行。 1.2 Linux 发行版到此为止，你已经了解了构成完整 Linux 系统所需要的4个关键部件，那你可能在考虑要怎样 才能把它们组成一个 Linux 系统。幸运的是，已经有人为你做好这些了。 我们将完整的 Linux 系统包称为发行版。有很多不同的 Linux 发行版来满足可能存在的各种运算需求。 不同的 Linux 发行版通常归类为 3 种： 完整的核心 Linux 发行版：如 Slackware，Red Hat，Fedora… 特定用途的发行版：如 CentOS，Ubuntu，Mint… LiveCD 测试发行版：从 CD 启动的发行版。","categories":[{"name":"Linux CL and Shell Programing","slug":"Linux-CL-and-Shell-Programing","permalink":"https://homholueng.github.io/categories/Linux-CL-and-Shell-Programing/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://homholueng.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://homholueng.github.io/tags/Shell/"}]},{"title":"Docker 应用数据的管理","slug":"docker-docs-app-data","date":"2019-04-17T14:37:44.000Z","updated":"2019-04-20T07:30:41.782Z","comments":true,"path":"2019/04/17/docker-docs-app-data/","link":"","permalink":"https://homholueng.github.io/2019/04/17/docker-docs-app-data/","excerpt":"","text":"概述容器中创建的文件默认会写入可写的容器层，这意味着： 这些数据会随着容器的销毁而消失 容器的可写层与当前宿主机耦合在了一起，想要将数据转移到别的地方比较麻烦 需要借助存驱动来对文件系统进行管理才能对容器的可写层进行写入，这与使用数据卷相比性能会差一些，因为多了一层抽象层。 Docker 针对数据存储的问题提供了两种解决方案：volumn 和 bind mount，这使得我们能够放心的将容器中产生的数据存储在宿主机中。 如果你的 Docker 运行在 Linux 上，那么你还有另外一种选择： tmpfs mount。 选择正确的挂载方式不管你选择使用哪一种挂载方式，在容器内看起来都是一样的。但是从容器外的角度来看，这三种挂载方式的确存在不同，下图展示了三种挂载方式的区别： Volume 是由 Docker 管理的宿主机文件系统的一部分，非 Docker 进程一般不应该修改这部分数据。Volume 是用于存储容器中数据的最好的方式。当你创建一个 volume 时，其会被存储在 Docker 宿主机的一个目录中，当你把这个 Volume 挂载到容器中时，这个目录就会被挂载到容器中。这与 bind mount 的运作方式有些相似，但不同之处是 volume 是由 Docker 管理且与宿主机的核心功能隔离的。一个 volume 能挂载到多个容器中，且在一个 Docker 宿主机中，volume 的名称必须是唯一的，如果我们没有显式指定该名称，Docker 就会在 volume 被挂载到容器上时设定一个随机的名称。同时，volume 支持使用不同的 Volume Driver，使得我们能够将容器产生的数据存储在远程的主机或是云服务提供者处。 Bind Mount 可以存储在宿主机中的任何位置，其甚至可以是一些重要的系统文件或目录。宿主机中的非 Docker 进程和其他容器能够随时修改这些数据。这是 Docker 早期使用的功能，使用 bind mount 能够将宿主机上的文件或目录挂载到容器中。若你在开发新的 Docker 应用，建议使用具名 volume 而不是 bind mount。 tmpfs Mount 存储在宿主机的内存中，这些数据并不会写入到宿主机的文件系统中。其在容器声明周期中能够一直使用，一般用于存储一些非持久状态或是敏感信息。swarm service 就使用 tmpfs 来将 secret 挂载到 service 容器中。 最佳实践Volumes 的最佳实践 在多个正在运行的容器之间共享数据。 当宿主机不能保证提供特定的文件或目录结构时，使用 volume 来将容器运行时和宿主机进行解耦。 当你想将容器中的数据存在远程的主机或是云服务提供商处时。 当你需要在容器宿主机之间进行备份，恢复或是数据迁移时。 bind mouts 的最佳实践 在宿主机和容器之间共享某些配置文件。 在宿主机和容器的开发环境中共享源码或构建产物。 当 Docker 宿主机的文件或目录结构保证与容器 bind mount 中所需的一致时。 tmpfs 的最佳实践 用于存储某些不需要进行持久化的数据，如敏感信息。 Tips 如果你将一个空的 volume挂载到容器中的某个已经存在某些文件或目录的目录下时，这些已经存在文件或目录会被复制到 volume 中。类似的，如果你在启动容器时指定了一个不存在的 volume，那么 Docker 会帮你创建一个空的 volume。 如果你将一个 bind mount 或 非空的 volume 挂载到已经存在的某些文件或目录的下时。纳闷这些文件在 bind mount 或 volume 被卸载前都会被隐藏起来。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://homholueng.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://homholueng.github.io/tags/Docker/"},{"name":"Storage","slug":"Storage","permalink":"https://homholueng.github.io/tags/Storage/"}]},{"title":"Chapter 9 - 符合 Python 风格的对象","slug":"fluent-python-pythonic-object","date":"2019-04-14T07:51:43.000Z","updated":"2019-04-20T07:30:41.782Z","comments":true,"path":"2019/04/14/fluent-python-pythonic-object/","link":"","permalink":"https://homholueng.github.io/2019/04/14/fluent-python-pythonic-object/","excerpt":"","text":"得益于 Python 数据模型，自定义类型的行为可以像内置类型那样自然。实现如此自然的行为，靠的不是集成，而是鸭子类型（duck typing）。 对象表示形式Python 提供了两种获取对象表示形式的方法: repr()：以便于开发者理解的方式返回对象的字符串表示形式。 str()：以便于用户理解的方式返回对象的字符串表示形式。 为了支持以上两种方式，我们要实现 __repr__ 和 __str__ 两个特殊方法。还有另外两个特殊方法能够提供对象的其他表示形式：__bytes__ 和 __format__。前者在 bytes() 函数中使用，而后者会在内置的 format() 函数和 str.format() 方法中使用。 在 Python2 中，__repr__ 和 __str__ 不应该返回 Unicode 对象，而在 Python3 中，它们必须返回 Unicode 对象！ 下面是一个设计良好的对象的示例： 1234567891011121314151617181920212223242526272829303132import mathfrom array import arrayclass Vector2d: typecode = 'd' def __init__(self, x, y): self.x = float(x) self.y = float(y) def __iter__(self): return (i for i in (self.x, self.y)) def __repr__(self): class_name = type(self).__name__ return '&#123;&#125;(&#123;!r&#125;, &#123;!r&#125;)'.format(class_name, *self) def __str__(self): return str(tuple(self)) def __bytes__(self): return (bytes([ord(self.typecode)]) + bytes(array(self.typecode, self))) def __eq__(self, other): return tuple(self) == tuple(other) def __abs__(self): return math.hypot(self.x, self.y) def __bool__(self): return bool(abs(self)) 备选构造方法下面我们为上一小节定义的 Vector2d 类定义一个备选的构造方法，这个构造方法读取一个字节序列并返回一个 Vector2d 对象： 12345@classmethoddef frombytes(cls, octets): typecode = chr(octets[0]) memv = memoryview(octets[1:]).cast(typecode) return cls(*memv) classmethod 与 staticmethodclassmethod 改变了调用方法的方式，因此类方法的第一个参数是类本身，而不是实例。classmethod 最常见的用途是定义备选构造方法。 staticmethod 装饰器也会改变方法的调用方式，但是第一个参数不是特殊的值。 格式化显示内置的 format() 函数和 str.format() 方法把各个类型的格式化方式委托给相应的 .__format__(format_spec) 方法。format_spec 是格式说明符，它是： format(my_obj, format_spec) 的第二个参数，或 str.format() 方法的格式字符串，{} 里代换字段中冒号后面的部分 举个例子： 12345678910In [1]: brl = 1/2.34In [2]: brlOut[2]: 0.4273504273504274In [3]: format(brl, '0.4f') # format_spec 是 '0.4f'Out[3]: '0.4274'In [4]: '1 BRL = &#123;rate:0.2f&#125; USD'.format(rate=brl) # format_spec 是 '0.2f'Out[4]: '1 BRL = 0.43 USD' 格式说明符使用的表示法叫格式规范微语言(formatspec)。格式规范微语言是可以扩展的，各个类可以自行决定如何解释 format_spec 参数。例如 datetime 模块中的类，他们的 __format__ 方法使用的格式代码就与 strftime() 函数一样： 123456In [5]: from datetime import datetimeIn [6]: now = datetime.now()In [7]: format(now, '%H:%M:%S')Out[7]: '15:00:46' 可散列的 Vector2d要创建可散列的类型，只需要正确的实现 __hash__ 方法和 __eq__ 方法即可。实现 __hash__ 方法时，官方推荐使用位运算符异或 ^ 混合各分量的散列值： 12def __hash__(self): return hash(self.x) ^ hash(self.y) Python 的私有属性和“受保护的”属性Python 不像 Java 那样使用 private 修饰符创建私有属性，但是 Python 有个简单的机制，能避免子类意外覆盖“私有”属性。 计入我们在类 Foo 中以 __attr 的形式命名实例属性，Python 会把属性名存入实例的 __dict__ 属性中，而且会在前面加上一个下划线和类型。因此，__attr 会变成 _Foo__attr；这个语言特性叫名称改写（name mangling）。 123456789In [8]: class Foo: ...: def __init__(self): ...: self.__attr = 'attr' ...:In [9]: foo = Foo()In [10]: foo.__dict__Out[10]: &#123;'_Foo__attr': 'attr'&#125; 不是所有 Python 程序员都喜欢名称改写功能，也不是所有人都喜欢 self.__x 这种不对称的名称。有些人不喜欢这种句法，他们约定使用一个下划线前缀编写“受保护”的属性（如 self._x）。Python 解释器虽然不会对使用单个下划线的属性名做特殊处理，不过这是很多 Python 程序员严格遵守的约定，他们不会在类外部访问这种属性。 使用 slots 类属性节省空间默认情况下，Python 在各个实例中名为 __dict__ 的字典里存储实例属性。而字典会消耗大量的内存，如果要处理数百万个属性不多的实例，通过 __slots__ 类属性能够节省大量内存。 继承自父类的 __slots__ 属性没有效果。Python 只会使用各个类中定义的 __slots__ 属性。 定义 __slots__ 的方式是，创建一个类属性，使用 __slots__ 这个名字，并把它的值设为一个字符串构造的可迭代对象，其中各个元素表示各个实例属性： 12class Vector2d: __slots__ = ('x', 'y') 在类中定义 __slots__ 属性之后，实例不能再有 __slots__ 中所列名称之外的其他属性。这只是一个副作用，不是 __slots__ 存在的真正原因。不要使用 __slots__ 属性禁止类的用户新增实例属性。 此外，还需要注意的一个实例属性就是 __weakref__ 属性，为了让对象支持弱引用，必须有这个属性。用户自定义的类中默认就有该属性，可是，如果类中定义了 __slots__ 属性，而且想把实例作为弱引用的目标，那么就要把 __weakref__ 添加到 __slots__ 中。 如果你的程序不用处理数百万个实例，或许不值得费劲去创建不寻常的类，仅当权衡当下的需求并仔细搜集资料后证明确实有必要时，才应该使用 __slots__ 属性。 覆盖类属性如果父类中的方法希望获取用户在自定义子类中覆盖的类属性，那么就不应该硬编码获取属性时的类，而是通过如下方式获取： 123def method(self): Class.attr # Wrong type(self).attr 总结符合 Python 风格的对象应该正好符合所需，而不是堆砌语言特性。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 8 - 对象引用、可变性和垃圾回收","slug":"fluent-python-object-relate","date":"2019-04-07T07:52:43.000Z","updated":"2019-04-20T07:30:41.782Z","comments":true,"path":"2019/04/07/fluent-python-object-relate/","link":"","permalink":"https://homholueng.github.io/2019/04/07/fluent-python-object-relate/","excerpt":"","text":"标识、相等性和别名1234567891011121314151617In [1]: charles = &#123;'name': 'Charles L. Dodgson', 'born': 1832&#125;In [2]: lewis = charlesIn [3]: lewis == charlesOut[3]: TrueIn [4]: lewis is charlesOut[4]: TrueIn [5]: alex = &#123;'name': 'Charles L. Dodgson', 'born': 1832&#125;In [6]: alex == charlesOut[6]: TrueIn [7]: alex is charlesOut[7]: False is 运算符比较两个对象的 id()。 默认做浅复制复制列表（或多数内置的可变集合）最简单的方式是使用内置的类型构造方法。例如： 123456789101112In [8]: l1 = [3, [55, 44], (7, 8, 9)]In [9]: l2 = list(l1)In [10]: l2Out[10]: [3, [55, 44], (7, 8, 9)]In [11]: l2 == l1Out[11]: TrueIn [12]: l2 is l1Out[12]: False 然而，构造方法或者 [:] 做的是浅复制，如果集合中有可变元素，可能会导致意想不到的问题： 1234567In [13]: l1[1].append(33)In [14]: l1Out[14]: [3, [55, 44, 33], (7, 8, 9)]In [15]: l2Out[15]: [3, [55, 44, 33], (7, 8, 9)] 函数的参数作为引用时Python 唯一支持的参数传递模式是共享传参（call by sharing），指的是函数的各个形式参数获得实参中各个引用的副本，也就是说，函数内部的形参是实参的别名。 del 与垃圾回收del 语句删除名称，而不是对象。del 命令可能会导致对象被当做垃圾回收，但是仅当删除的变量保存的是对象的最后一个引用，或者无法得到对象时（孤岛）。 弱引用弱引用不会增加对象的引用数量。引用的目标对象称为所指对象（referent）。因此我们说，弱引用不会妨碍所指对象被当作垃圾回收。 12345678910111213In [1]: import weakrefIn [2]: a_set = &#123;0, 1&#125;In [3]: wref = weakref.ref(a_set)In [4]: wref() is NoneOut[4]: FalseIn [5]: a_set = &#123;2, 3, 4&#125;In [6]: wref() is NoneOut[6]: True Python 控制台会自动把 _ 变量 绑定到结果不为 None 的表达式结果上。这凸显了一个实际问题：微观管理内存时，往往会得到意外的结果，因为不明显的隐式赋值会为对象创建新引用。控制台中的 _ 变量是一例。调用跟踪对象也常导致意料之外的引用。 weakref 模块的文档（http://docs.python.org/3/library/weakref.html) 指出，weakref.ref 类其实是低层接口，供高级用途使用，多数程序最 好使用 weakref 集合和 finalize。也就是说，应该使用 WeakKeyDictionary、WeakValueDictionary、WeakSet 和 finalize（在内部使用弱引用），不要自己动手创建并处理 weakref.ref 实例。 不是每个 Python 对象都能够作为弱引用的目标。基本的 list 和 dict 实例不能作为所指出对象，但是它们的子类可以轻地解决这个问题： 12345678910111213141516In [18]: import weakrefIn [19]: weakref.ref([1, 2, 3])---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-19-7bab81ad732a&gt; in &lt;module&gt;----&gt; 1 weakref.ref([1, 2, 3])TypeError: cannot create weak reference to 'list' objectIn [20]: class MyList(list): ...: pass ...:In [21]: weakref.ref(MyList([1, 2, 3]))Out[21]: &lt;weakref at 0x10b0c5688; dead&gt; 这些局限基本上是 CPython 的实现细节，在其他 Python 解释器中情况可能不一样。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Docker 网络基础","slug":"docker-docs-network","date":"2019-04-06T10:17:19.000Z","updated":"2019-04-23T16:29:42.116Z","comments":true,"path":"2019/04/06/docker-docs-network/","link":"","permalink":"https://homholueng.github.io/2019/04/06/docker-docs-network/","excerpt":"","text":"Docker 容器和服务的功能之所以如此强大，得益于其能够通过网络与其他组件进行互联。与此同时，容器和服务也不需要关心自身和与其互联的组件是否是部署在 Docker 上的，也不需要关心 Docker 宿主机的操作系统类型，因为 Docker 已经帮我们处理了这些事情，我们能够使用一种平台无关的方式来管理这些组件和配置。 网络驱动Docker 的网络系统通过驱动来实现了可插拔的特性，并且默认提供了以下几种类型的驱动： bridge：默认使用的驱动类型，一般独立的容器需要进行网络通信时都会使用这种驱动。 host：对于独立容器来说，host 驱动去除了 Docker 宿主机与容器之间的网络隔离，直接使用宿主机网络。该模式仅仅对 17.06 之后的版本中的 swarm services 有效。 overlay：overlay 驱动将多个 Docker 连接起来使得 swarm services 之间能够进行互相通信。同时，你也可以使用 overlay 网络来让 swarm services 与独立容器、或是运行在不同的 Docker 进程上的独立容器与独立容器之间进行网络通信。 macvlan：macvlan 驱动能够为容器设置 MAC 地址，使其在网络中表现的与真实的物理设备一样。当你要处理一些需要直连物理网络的遗留应用时，macvlan 或许是比较好的选择。 none：使用的这种插件的容器将无法进行网络通信，swarm services 无法使用这种类型的插件。 除了上面列出的这些插件之外，我们还可以按需选择从 Docker Hub 处获取的第三方网络插件。 总结 当你需要让同一个 Docker 宿主机下的多个容器进行网络通信时，用户定义的 bridge 模式是最好的选择。 当你需要让运行在多个不同的 Docker 宿主机上的容器、或是多个 swarm services 进行网络通信时，overlay 模式是最好的选择。 当你希望你的容器看起来像是一个真实的物理设备时，macvlan 模式是最好的选择。 当你需要将一些特殊的网络栈集成到 Docker 中时，第三方网络插件或许能满足你的需求。 使用 bridge 网络在计算机网络的术语中，桥接网络（bridge network，后文称为 bridge 网络）指的是在网络段之间转发流量的链路层设备。网桥（bridge）既可以是硬件设备，也可以是运行在主机内核中的软件。 在 Docker 的术语中，bridge 网络通过软件层的网桥使得连接到该网络上的容器能够进行通信，并对连接到不同网络中的容器进行隔离。Docker 的 bridge 驱动会自动将规则配置到宿主机中使得不同 bridge 网络中的容器无法直接与对方进行通信。 但是，bridge 网络只能应用在运行在同一台宿主机上的容器上。当 Docker 启动时，其会自动创建一个默认的 bridge 网络，并且新启动的容器在没有指定特定网络的前提下会自动加入该默认的 bridge 网络中。但用户定义的 bridge 网络比默认创建的 bridge 网络更加强大。 用户定义的与默认的 bridge 网络的差异 用户定义的 bridge 网络相较与默认的能在容器化的应用间提供更好的隔离性和互操作性 用户定义的 bridge 网络中的容器会自动互相暴露所有的端口，并向外部屏蔽所有的端口 用户定义的 bridge 网络能够自动的在容器间进行 DNS 解析 用户定义的 bridge 网络间的容器能够通过网络中其他容器的名称来进行通信 容器能在运行中随时加入用户定义的 bridge 网络或从其中脱离出来 若要将容器从默认 bridge 网络中脱离出来，需要先将容器停止 每一个用户定义的 bridge 网络都创建了一个可配置的网桥 若要配置默认 bridge 网络，需要在 Docker 外进行配置，并且需要重启 Docker；而用户定义的 bridge 网络通过 docker network create 进行创建和配置，如果不同的应用对网络有不同的要求，只需要在创建时单独配置每个网络即可。 连接到默认 bridge 网络中的容器会共享环境变量 在用户定义的 bridge 网络中需要通过其他方式来共享环境变量，例如：挂载公共目录，使用 docker-compose 来管理容器和共享变量 当我们创建用户定义网络、将容器连接到网络或是从中断开时，容器会操作操作系统底层的网络设施，例如添加网桥设备或是配置 Linux 中的 iptables。 bridge 网络相关操作请参考官方文档：Use bridge networks bridge 网络通信实践使用默认的 bridge 网络 这种类型的网络不推荐用于于生产环境。 启动两个 alpine 容器，并运行 ash 命令，（-dit 选项使得容器带着 TTY（t）以可交互的模式（i）在后台（t）运行） 123$ docker run -dit --name alpine1 alpine ash$ docker run -dit --name alpine2 alpine ash 确认容器运行起来后探查 bridge 网络的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ docker network inspect bridge[ &#123; \"Name\": \"bridge\", \"Id\": \"ad75fa41c76af8f9fbbefb5000c347d2cebe56b0b31a730005702b42b0c00fa7\", \"Created\": \"2019-03-02T04:57:51.222477618Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": null, \"Config\": [ &#123; \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"03c4e0a99076a641ad4ce36b69621224d4aa974f008c9b3dd4b2cc587a9bdab6\": &#123; \"Name\": \"alpine1\", \"EndpointID\": \"b5b1c1020c915d2e38e824e00a465c9ecd0396e0c7e6441871a8a17cd042bcf7\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" &#125;, \"d1acd87c62615acf948153b79718ecc0f4326cfff6d55574cdcf2adef7e1e182\": &#123; \"Name\": \"alpine2\", \"EndpointID\": \"b84c1b868e29e94ab7a7f227b58a3e47f4ea11300b8bffe8e41d2d354f4b574b\", \"MacAddress\": \"02:42:ac:11:00:03\", \"IPv4Address\": \"172.17.0.3/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123; \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" &#125;, \"Labels\": &#123;&#125; &#125;] 可以看到，这个网络中目前有 alpine1 和 alpine2 两个容器，alpine1 分配到的 IP 地址为 172.17.0.2/16，而 alpine2 分配到的 IP 地址为 172.17.0.3/16。 进入 alpine1 123$ docker attach alpine1/ # 查看 alpine1 的网络设备 12345678910111213/ # ip addr show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN qlen 1 link/ipip 0.0.0.0 brd 0.0.0.03: ip6tnl0@NONE: &lt;NOARP&gt; mtu 1452 qdisc noop state DOWN qlen 1 link/tunnel6 00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00 brd 00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00205: eth0@if206: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 尝试与 alpine2 通信 1234/ # ping -c 2 172.17.0.3PING 172.17.0.3 (172.17.0.3): 56 data bytes64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.141 ms64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.217 ms 使用用户定义的 bridge 网络 先创建一个名为 alpine-net 的 bridge 网络 12$ docker network create --driver bridge alpine-net15e45a28a73cdbe96cfd3a32f0c6329871a70ced29ab7d8175a54059130d6488 确认该网络的状态 12345678910111213141516171819202122232425262728293031$ docker network inspect alpine-net[ &#123; \"Name\": \"alpine-net\", \"Id\": \"15e45a28a73cdbe96cfd3a32f0c6329871a70ced29ab7d8175a54059130d6488\", \"Created\": \"2019-04-01T15:20:45.4514219Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"172.29.0.0/16\", \"Gateway\": \"172.29.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123;&#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;] 创建四个容器，将 alpine1，alpine2 及 alpine4 加入 alpine-net 网络中，将 alpine3 和 alpine4 加入默认的 bridge 网络中（由于 docker run 命令中的 --network 选项一次只能连接一个网络，所以 alpine4 在创建好后需要通过 docker network connect 来加入默认 bridge 网络） 12345678910111213$ docker run -dit --name alpine1 --network alpine-net alpine ash645e623f184f643e3e2f9aa953a020c10df11bbfad47a15cda81a093e4019370$ docker run -dit --name alpine2 --network alpine-net alpine ash4843f601f9787d5af7453874e1645b1947fd20bd40679a46e2be9e29511e6664$ docker run -dit --name alpine3 alpine ash3d738a2a015368b4bcf427eaee052995b0e1b024a5fa17dc3fc764ca7eb9b5b8$ docker run -dit --name alpine4 --network alpine-net alpine ash6717b0b2a9acff2635a1aff411e99dfaaa0425d5873fb08ca1df11bc2ca07278$ docker network connect bridge alpine4 确认 alpine-net 及默认 bridge 网络的状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107$ docker network inspect alpine-net[ &#123; \"Name\": \"alpine-net\", \"Id\": \"15e45a28a73cdbe96cfd3a32f0c6329871a70ced29ab7d8175a54059130d6488\", \"Created\": \"2019-04-01T15:20:45.4514219Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"172.29.0.0/16\", \"Gateway\": \"172.29.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"4843f601f9787d5af7453874e1645b1947fd20bd40679a46e2be9e29511e6664\": &#123; \"Name\": \"alpine2\", \"EndpointID\": \"ca50f28a382521c055db93bc2e8e9717a0338947e2638362e79cf215d05be631\", \"MacAddress\": \"02:42:ac:1d:00:03\", \"IPv4Address\": \"172.29.0.3/16\", \"IPv6Address\": \"\" &#125;, \"645e623f184f643e3e2f9aa953a020c10df11bbfad47a15cda81a093e4019370\": &#123; \"Name\": \"alpine1\", \"EndpointID\": \"d8933f0ac8c55ed582de4fd8a03abe03872f87f487b1ded7a2788ba68a8ca266\", \"MacAddress\": \"02:42:ac:1d:00:02\", \"IPv4Address\": \"172.29.0.2/16\", \"IPv6Address\": \"\" &#125;, \"6717b0b2a9acff2635a1aff411e99dfaaa0425d5873fb08ca1df11bc2ca07278\": &#123; \"Name\": \"alpine4\", \"EndpointID\": \"a2172fb70339c71ff02ce5b4c955665a186582207ad6e59b7535607098850cfa\", \"MacAddress\": \"02:42:ac:1d:00:04\", \"IPv4Address\": \"172.29.0.4/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]$ docker network inspect bridge[ &#123; \"Name\": \"bridge\", \"Id\": \"ad75fa41c76af8f9fbbefb5000c347d2cebe56b0b31a730005702b42b0c00fa7\", \"Created\": \"2019-03-02T04:57:51.222477618Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": null, \"Config\": [ &#123; \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"3d738a2a015368b4bcf427eaee052995b0e1b024a5fa17dc3fc764ca7eb9b5b8\": &#123; \"Name\": \"alpine3\", \"EndpointID\": \"1483be13d35e9ef035d866b145d853a931fb8f9aae7f2e66003bce6f0f4fe3a1\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" &#125;, \"6717b0b2a9acff2635a1aff411e99dfaaa0425d5873fb08ca1df11bc2ca07278\": &#123; \"Name\": \"alpine4\", \"EndpointID\": \"584ae19dbe92beb6fb002b8aae909eea6ca866a9a41b3976afca86fdaacfa081\", \"MacAddress\": \"02:42:ac:11:00:03\", \"IPv4Address\": \"172.17.0.3/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123; \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" &#125;, \"Labels\": &#123;&#125; &#125;] 在用户定义的网络中，容器之间不仅能够根据彼此的 IP 地址进行通信，还能够根据容器名来进行通信，这个特性被称为服务发现，进入 alpine1 容器中，并尝试使用容器名与其他容器进行通信，由于 alpine1 与 alpine3 不在同一个网络中，所以不管通过容器名还是 IP 地址都无法与其进行通信 1234567891011121314151617181920212223242526272829303132$ docker container attach alpine1/ # ping -c 2 alpine2PING alpine2 (172.29.0.3): 56 data bytes64 bytes from 172.29.0.3: seq=0 ttl=64 time=0.296 ms64 bytes from 172.29.0.3: seq=1 ttl=64 time=0.211 ms--- alpine2 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.211/0.253/0.296 ms/ # ping -c 2 alpine1PING alpine1 (172.29.0.2): 56 data bytes64 bytes from 172.29.0.2: seq=0 ttl=64 time=0.064 ms64 bytes from 172.29.0.2: seq=1 ttl=64 time=0.160 ms--- alpine1 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.064/0.112/0.160 ms/ # ping -c 2 alpine4PING alpine4 (172.29.0.4): 56 data bytes64 bytes from 172.29.0.4: seq=0 ttl=64 time=0.428 ms64 bytes from 172.29.0.4: seq=1 ttl=64 time=0.190 ms--- alpine4 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.190/0.309/0.428 ms/ # ping -c 2 alpine3ping: bad address 'alpine3'/ # ping -c 2 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes^C--- 172.17.0.2 ping statistics ---2 packets transmitted, 0 packets received, 100% packet loss 进入 alpine4，尝试与其他容器通信 123456789101112131415161718192021222324252627$ docker container attach alpine4/ # ping -c 2 alpine1PING alpine1 (172.29.0.2): 56 data bytes64 bytes from 172.29.0.2: seq=0 ttl=64 time=0.246 ms64 bytes from 172.29.0.2: seq=1 ttl=64 time=0.241 ms--- alpine1 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.241/0.243/0.246 ms/ # ping -c 2 alpine2PING alpine2 (172.29.0.3): 56 data bytes64 bytes from 172.29.0.3: seq=0 ttl=64 time=0.281 ms64 bytes from 172.29.0.3: seq=1 ttl=64 time=0.194 ms--- alpine2 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.194/0.237/0.281 ms/ # ping -c 2 alpine3ping: bad address 'alpine3'/ # ping -c 2 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.152 ms64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.215 ms--- 172.17.0.2 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.152/0.183/0.215 ms 使用 host 网络如果某个容器使用的是 host 网络，那么该容器的网络栈是不会与 Docker 宿主机进行隔离的。举个例子，如果某个容器使用 host 网络并绑定了 80 端口，那么该绑定该端口的应用将能够在宿主机 IP 的 80 端口上访问到。 要注意的是，host 类型的驱动只在 Linux 宿主机下起作用。 host 网络实践本次实践会运行一个 nginx 容器，并将其直接绑定到宿主机的 80 端口上。从网络的层次上看，容器与宿主机的隔离程度和直接在宿主机上运行 nginx 进程是一样的。但是从存储，进程命名空间和用户命名空间上看，nginx 进程与宿主机是完全隔离的。 启动 nginx 容器（--rm 选项会移除已有的容器） 1$ docker run --rm -d --network host --name my_nginx nginx 访问 http://localhost:80/ 使用 overlay 网络overlay 网络驱动会在多个 Docker 宿主机之间建立一个分布式网络。该网络位于宿主机的网络之上，所以被称为 overlay 网络，容器能够加入到该网络中，并与该网络中的其他容器安全地进行通讯。 当你初始化一个 swarm 节点并将其加入一个已经存在的 swarm 中时，该节点的宿主机上会创建两个新的网络： 一个名为 ingress 的 overlay 网络，负责处理 swarm service 之间的通信数据。当你创建的 swarm service 没有加入自定义的 overlay 网络中时，其默认会加入 ingress 中。 一个名为 docekr_gwbridge 的 bridge 网络，其负责将 swarm 中的 Docker 进程连接起来。 通过 docker network create 命令能够创建自定义的 overlay 网络，这与创建自定义的 bridge 网络相似。一个 swarm service 或容器能够同时加入多个网络中。 使用 macvlan 网络macvlan 网络能够为容器分配一个 MAC 地址，使得容器看起来就像是物理网络中的一台真实物理设备一样。但是，这需要为 macvlan 网络分配一个 Docker 宿主机上的物理网络接口，该接口会用于该网络及其子网和网关。你甚至能够使用不同的物理接口来对不同的 macvlan 网络进行隔离，但是在使用 macvlan 网络前，必须要明确以下几点： 很可能会因为 IP 地址耗尽或是 VLAN spread 而无意中损坏你的网络。 你的网络设备需要能够处理混杂模式，即在一个物理接口上分配多个 MAC 地址。 如果你的应用在 bridge 网络或是 overlay 网络上也能够正常工作，那么从长远来看这些都是更好的解决方案。 macvlan 网络存在两种模式：bridge 模式和 802.1q trun bridge 模式，这两种模式的差异如下： 在 bridge 模式下，macvlan 网络的流量会直接经过宿主机上的网络接口。 在 802.1q trun bridge 模式下，网络流量会经过一个 Docker 创建的 802.1q 子接口。这个模式允许我们更加精细对流量进行路由和过滤。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://homholueng.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://homholueng.github.io/tags/Docker/"},{"name":"Network","slug":"Network","permalink":"https://homholueng.github.io/tags/Network/"}]},{"title":"Chapter 7 - 函数装饰器和闭包","slug":"fluent-python-decorator-and-closure","date":"2019-03-28T16:06:19.000Z","updated":"2019-04-20T07:30:41.781Z","comments":true,"path":"2019/03/29/fluent-python-decorator-and-closure/","link":"","permalink":"https://homholueng.github.io/2019/03/29/fluent-python-decorator-and-closure/","excerpt":"","text":"使用装饰器改进策略模式在 Python 策略模式中。我们可以使用装饰器来帮助我们统一管理自定义的策略函数： 1234567891011121314151617181920promos = []def promotion(promo_func): promos.append(promo_func) return promo_func@promotiondef fidelity(order): pass@promotiondef bulk_item(order): pass@promotiondef large_order(order): passdef best_promo(order): return max(promo(order) for promo in promos) 使用装饰器来管理策略函数的好处在于：策略函数能够定义在系统的任何地方，只要其使用 @promotion 来进行修饰我们就能够对其进行统一管理。 对于一些需要开放出去让开发者进行自定义，但是系统内部又需要进行统一管理的组件，使用装饰器是一个不错的选择。另外，如果我们需要进行统一管理的组件内部包含了更多的信息，此时函数可能相对来说就显得太轻量了，这时候，我们不发考虑使用元类来实现分散组件的统一管理。 闭包闭包指延伸了作用域的函数，其中包含函数定义体中引用、但是不在定义体中定义的非全局变量。 举个例子： 1234567891011121314151617181920def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averagerIn [2]: avg = make_averager()In [3]: avg(10)Out[3]: 10.0In [4]: avg(11)Out[4]: 10.5In [5]: avg(12)Out[5]: 11.0 在 averager 函数中，series 是自由变量（free variable），这是一个技术术语，指未在本地作用域中绑定的变量，如下图所示，averager 的闭包延伸到其作用域之外，并包含了自由变量 series 的绑定： 继续深入探查 avg 函数： 1234567891011In [6]: avg.__code__.co_varnamesOut[6]: ('new_value', 'total')In [7]: avg.__code__.co_freevarsOut[7]: ('series',)In [8]: avg.__closure__Out[8]: (&lt;cell at 0x104affd98: list object at 0x104bd8688&gt;,)In [9]: avg.__closure__[0].cell_contentsOut[9]: [10, 11, 12] 可以看到，series 绑定在 avg 函数的 __closure__ 属性中。avg.__closure__ 中的各个元素对应于 avg.__code__.co_freevars 中的一个名称。这些元素的 cell_contents 属性保存着该变量真正的值。 综上，闭包是一种函数，它会保留定义函数时存在的自由变量的绑定，这样调用函数时，虽然定义作用域不可用了，但是仍能使用那些绑定。 nonlocal 声明上面例子中实现的 make_averager 函数的效率其实并不高，因为我们将所有的值都存在了一个数组中，然后每次都会调用 sum 来求和并且算平均值；更加高效的做法是只记录历史总值和元素个数，然后使用这两个数计算平均值。 123456789101112131415161718192021222324252627def make_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averagerIn [2]: avg = make_averager()In [3]: avg(10)---------------------------------------------------------------------------UnboundLocalError Traceback (most recent call last)&lt;ipython-input-3-ace390caaa2e&gt; in &lt;module&gt;----&gt; 1 avg(10)&lt;ipython-input-1-97b2ca7b5c4e&gt; in averager(new_value) 4 5 def averager(new_value):----&gt; 6 count += 1 7 total += new_value 8 return total / countUnboundLocalError: local variable 'count' referenced before assignment 但是在使用的时候却跑出了 UnboundLocalError 错误，出现这个错误的原因是当 count 是数字或任何不可变类型时，count += 1 语句的作用其实与 count = count + 1 一样。因此，我们在 averager 的定义体中为 count 赋值了，这会把 count 变成局部变量，这样，count 就不是自由变量了，因此不会保存在闭包中。 为了解决这个问题，Python 3 引入了 nonlocal 声明。它的作用是把变量标记为自由变量，即使在函数中为变量赋予新值了，也会变成自由变量： 1234567891011def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager 在 python2 中，还没有引入 nonlocal 关键字，此时可以使用一个可变对象（如字典）把这些变量存储起来，并让该可变对象称为一个自由变量。 实现一个简单的装饰器我们知道，在函数上使用装饰器会使得我们丢失函数的真正信息，这个时候，可以借助 functools.wraps 装饰器来把被装饰的函数相关属性复制到装饰后的函数中： 1234567891011121314151617181920212223import timeimport functoolsdef clock(func): @functools.wraps(func) def clocked(*args, **kwargs): t0 = time.time() result = func(*args, **kwargs) elapsed = time.time() - t0 name = func.__name__ arg_lst = [] if args: arg_lst.append(', '.join(repr(arg) for arg in args)) if kwargs: pairs = ['%s=%r' % (k, w) for k, w in sorted(kwargs.items())] arg_lst.append(', '.join(pairs)) arg_str = ', '.join(arg_lst) print('[%0.8fs] %s(%s) -&gt; %r ' % (elapsed, name, arg_str, result)) return result return clocked 标准库中的装饰器使用 functools.lru_cache 做备忘functools.lru_cache 会为其装饰的函数添加基于 LRU（Least Recently Used）缓存，避免一些耗时操作的重复计算，下面我们使用斐波那契数的递归计算来作为例子： 12345678910111213141516171819202122232425262728293031323334@clockdef fibonacci(n): if n &lt; 2: return n return fibonacci(n - 2) + fibonacci(n - 1)In [9]: fibonacci(6)[0.00000000s] fibonacci(0) -&gt; 0[0.00000095s] fibonacci(1) -&gt; 1[0.00012326s] fibonacci(2) -&gt; 1[0.00000119s] fibonacci(1) -&gt; 1[0.00000119s] fibonacci(0) -&gt; 0[0.00000000s] fibonacci(1) -&gt; 1[0.00003910s] fibonacci(2) -&gt; 1[0.00008202s] fibonacci(3) -&gt; 2[0.00026703s] fibonacci(4) -&gt; 3[0.00000072s] fibonacci(1) -&gt; 1[0.00000000s] fibonacci(0) -&gt; 0[0.00000000s] fibonacci(1) -&gt; 1[0.00003386s] fibonacci(2) -&gt; 1[0.00006771s] fibonacci(3) -&gt; 2[0.00000000s] fibonacci(0) -&gt; 0[0.00000119s] fibonacci(1) -&gt; 1[0.00003695s] fibonacci(2) -&gt; 1[0.00000095s] fibonacci(1) -&gt; 1[0.00000191s] fibonacci(0) -&gt; 0[0.00000095s] fibonacci(1) -&gt; 1[0.00003672s] fibonacci(2) -&gt; 1[0.00009012s] fibonacci(3) -&gt; 2[0.00015903s] fibonacci(4) -&gt; 3[0.00026083s] fibonacci(5) -&gt; 5[0.00056100s] fibonacci(6) -&gt; 8Out[9]: 8 可以看到，对于相同参数的调用重复了很多次，让我们加上缓存试试： 123456789101112131415161718import functools@functools.lru_cache()@clockdef fibonacci(n): if n &lt; 2: return n return fibonacci(n - 2) + fibonacci(n - 1)In [11]: fibonacci(6)[0.00000000s] fibonacci(0) -&gt; 0[0.00000215s] fibonacci(1) -&gt; 1[0.00014687s] fibonacci(2) -&gt; 1[0.00000191s] fibonacci(3) -&gt; 2[0.00020289s] fibonacci(4) -&gt; 3[0.00000191s] fibonacci(5) -&gt; 5[0.00024819s] fibonacci(6) -&gt; 8Out[11]: 8 加上缓存之后，调用次数明显减少，另外，这个装饰器是会接收配置参数的： maxsize：指定最多存储多少个调用的结果 typed：将不同类型的参数分开保存（例如将 1 与 1.0 区分开），顺带一说，lru_cache 使用字典存储结果，而且键根据调用时传入的位置参数和关键字参数创建，所以被其装饰的函数的所有参数都必须是可散列的。 1functools.lru_cache(maxsize=128, typed=False) 单分派泛函数有些时候我们可能需要根据函数参数类型来确定接下来要执行的逻辑，由于 Python 中没有泛型的概念，所以这个时候一般的做法是使用 if/else 语句来判断参数类型，或者使用字典来维护每种不同类型的参数所对应的处理逻辑。 但是在 Python 3.4 之后，我们有了新的选择： 1234567891011121314151617181920212223from functools import singledispatchimport numbers@singledispatchdef process_it(obj): print('This is a object')@process_it.register(str)def _(text): print('This is a str')@process_it.register(numbers.Integral)def _(n): print('This is a interger')In [17]: process_it('str')This is a strIn [18]: process_it(1)This is a intergerIn [19]: process_it(object())This is a object PyPI 中 的 singledispatch 包（https://pypi.python.org/pypi/singledispatch） 可以向后兼容 Python 2.6 到 Python 3.3 另外，对于一些功能比较特殊的装饰器，最好通过实现 __call__ 方法的类实现。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 6 - 使用一等函数实现设计模式","slug":"fluent-python-design-pattern-with-function","date":"2019-03-24T11:52:18.000Z","updated":"2019-04-20T07:30:41.777Z","comments":true,"path":"2019/03/24/fluent-python-design-pattern-with-function/","link":"","permalink":"https://homholueng.github.io/2019/03/24/fluent-python-design-pattern-with-function/","excerpt":"","text":"1996 年，Peter Norvig 在题为“Design Patterns in Dynamic Languages”（http://norvig.com/design-patterns/) 的演讲中指出，在 Gamma 等人合著的《设计模式：可复用面向对象软件基础》一书中包含了 23 个模式，其中有 16 个在动态语言中 “不见了，或者简化了”。 Norvig 建议在有一等函数的语言中重新审视 “策略”，“命令”，“模板方法” 和 “访问者” 模式。通常，我们可以把这些模式中涉及的某些类的实例替换成简单的函数，从而减少样板代码。 策略模式策略模式的目的在于将一系列的算法封装起来，并且使他们可以相互替换。如下所示： 这个模式使得算法可以独立于使用他们的调用者而变化。 我们使用类来描述一个策略，并在计算订单总额时根据顾客类型来确定并初始化相应的策略类。 但是在有了一等函数后，我们可以不再使用策略类，而是只使用函数类代表不同的策略，并在计算总额的时候将策略函数作为参数进行传递。Python 中的 sort() 的 key 参数就是策略函数的一个例子。 使用一等函数实现策略模式时，不同策略的管理可能是个问题，此时可以使用统一的模块来管理所有的策略函数，并通过 inspect 模块来获取所有的策略： 123456789import promotions # 策略模块promos = [func for name, func in inspect.getmembers(promotions, inspect.isfunction)]def best_promo(order): \"\"\" 选择最佳折扣 \"\"\" return max(promo(order) for promo in promos) 上述方案没有检查所有策略函数的参数，如果想更进一步，可以根据函数接收的参数再次过滤，避免在使用时抛出异常。 命令模式命令模式通过把函数作为参数传递而简化，如下所示： 调用者只需要执行相应的命令，而不同的命令后有不同的接受者负责处理。该模式的目的是解耦调用操作的对象和提供实现的对象。 同样的，我们可以不为调用者提供一个 Command 实例，而是给它一个函数，此时调用者可以不用调用 command.execute()，而是直接调用 command() 即可。 小结可以看到，上面两种模式都是将设计模式中的类替换成了函数，这样做的好处在于减少运行时的开销，因为使用类时需要每次都进行类对象的实例化，而使用函数时则不会有这个问题。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 5 - 一等函数","slug":"fluent-python-first-class-function","date":"2019-03-18T14:48:59.000Z","updated":"2019-04-20T07:30:41.776Z","comments":true,"path":"2019/03/18/fluent-python-first-class-function/","link":"","permalink":"https://homholueng.github.io/2019/03/18/fluent-python-first-class-function/","excerpt":"","text":"在 Python 中，函数是一等对象。编程语言理论家把 “一等对象” 定义为满足下述条件的程序实体： 在运行时创建 能赋值给变量或数据结构中的元素 能作为参数传递给函数 能作为函数的返回结果 函数内省与用户定义的常规类一样，函数使用 __dict__ 属性存储赋予它的用户属性。但是，函数也拥有一些专有而用户定义的一般对象没有的属性： __annotations__：dict 类型，参数和返回值的注解。 __call__：method-wrapper 类型，实现 () 运算符，即可调用对象协议。 __closure__：tuple 类型，函数闭包，即自由变量的绑定。 __code__：code 类型，编译成字节码的函数元数据和函数定义体。 __defaults__：tuple 类型，形式参数的默认值。 __get__：method-wrapper 类型，实现只读描述符协议。 __globals__：dict 类型，函数所在模块中的全局变量。 __kwdefaults__：dict 类型，仅限关键字形式参数的默认值。 __name__：str 类型，函数名称。 __qualname__：str 类型，函数的限定名称，如 Random.choice。 在这里要说明一下，仅限关键字参数是 Python 3 中的新特性，仅限关键字参数只能通过关键字参数指定，它一定不会捕获未命名的定位参数： 1234567# cls 为仅限关键字参数def tag(name, *content, cls=None, **attrs): pass# b 也是仅限关键字参数def f(a, *, b): pass 获取关于参数的信息我们以在 clip.py 模块下定义的 clip 函数作为示例： 12345678910111213141516171819202122232425def clip(text, max_len=80): \"\"\" 在 max_len 前面或后面的第一个空格处截断文本 \"\"\" end = None if len(text) &gt; max_len: space_before = text.rfind(' ', 0, max_len) if space_before &gt;= 0: end = space_before else: space_after = text.rfind(' ', max_len) if space_after &gt;= 0: end = space_after if end is None: end = len(text) return text[:end].rstrip()In [25]: clip.__code__.co_varnamesOut[25]: ('text', 'max_len', 'end', 'space_before', 'space_after')In [26]: clip.__code__.co_argcountOut[26]: 2 通过 __code__ 对象来获取函数信息未免有些不方便，我们一般会使用 inspect 模块来获取函数相关信息： 123456789101112In [29]: from inspect import signatureIn [30]: sig = signature(clip)In [31]: sigOut[31]: &lt;Signature (text, max_len=80)&gt;In [32]: for name, param in sig.parameters.items(): ...: print(param.kind, ':', name, '=', param.default) ...:POSITIONAL_OR_KEYWORD : text = &lt;class 'inspect._empty'&gt;POSITIONAL_OR_KEYWORD : max_len = 80 inspect.signature 函数返回一个 inspect.Signature 对象，它有一个 parameters 属性，这是一个有序映射，把参数名 inspect.Parameter 对象对应起来。inspect.Parameter 对象的 kind 属性有以下五种值： POSITIONAL_OR_KEYWORD：可以通过定位参数和关键字参数传入的形参。 VAR_POSITIONAL：定位参数元组。 VAR_KEYWORD：关键字参数字典。 KEYWORD_ONLY：仅限关键字参数（Python 3 新增）。 POSITIONAL_ONLY：仅限定位参数；目前，Python 声明函数的句法不支持，但是有些使 用 C 语言实现且不接受关键字参数的函数（如 divmod）支持。 inspect.Signature 对象有个 bind 方法，它可以把任意个参数绑定到签名中的形参上，所用的规则与实参到形参的匹配方式一样。框架可以使用这个方法在真正调用函数前验证参数： 12345678910111213141516171819202122232425262728293031323334353637383940In [33]: import inspectIn [34]: sig = inspect.signature(clip)In [35]: my_clip = &#123;'text': '1 2 3 4 5', 'max_len': 5&#125;In [39]: bound_args = sig.bind(**my_clip)In [40]: bound_argsOut[40]: &lt;BoundArguments (text='1 2 3 4 5', max_len=5)&gt;In [41]: for name, value in bound_args.arguments.items(): ...: print(name, '=', value) ...:text = 1 2 3 4 5max_len = 5In [42]: del my_clip['text']In [43]: sig.bind(**my_clip)---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-43-1821306ba99f&gt; in &lt;module&gt;----&gt; 1 sig.bind(**my_clip)~/.pyenv/versions/3.6.4/lib/python3.6/inspect.py in bind(*args, **kwargs) 2966 if the passed arguments can not be bound. 2967 \"\"\"-&gt; 2968 return args[0]._bind(args[1:], kwargs) 2969 2970 def bind_partial(*args, **kwargs):~/.pyenv/versions/3.6.4/lib/python3.6/inspect.py in _bind(self, args, kwargs, partial) 2881 msg = 'missing a required argument: &#123;arg!r&#125;' 2882 msg = msg.format(arg=param.name)-&gt; 2883 raise TypeError(msg) from None 2884 else: 2885 # We have a positional argument to processTypeError: missing a required argument: 'text' 支持函数式编程的包operator 模块比较哟用的两个函数是 itemgetter 及 attrgetter，由于 itemgetter 使用的是 [] 运算符来取值，所以其不仅仅支持序列，还支持映射和任何实现 __getitem__ 方法的类： 12345678910111213141516171819202122232425In [1]: from operator import itemgetterIn [2]: metro_data = [ ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)), ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)), ('Mexi ...: co City', 'MX', 20.142, (19.433333, -99.133333)), ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)), ('Sao Paulo', ' ...: BR', 19.649, (-23.547778, -46.635833)), ]In [3]: for city in sorted(metro_data, key=itemgetter(1)): ...: print(city) ...:('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833))('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))('Tokyo', 'JP', 36.933, (35.689722, 139.691667))('Mexico City', 'MX', 20.142, (19.433333, -99.133333))('New York-Newark', 'US', 20.104, (40.808611, -74.020386))In [4]: cc_name = itemgetter(1, 0)In [5]: for city in metro_data: ...: print(cc_name(city)) ...:('JP', 'Tokyo')('IN', 'Delhi NCR')('MX', 'Mexico City')('US', 'New York-Newark')('BR', 'Sao Paulo') attrgetter 与 itemgetter 作用类似，它创建的函数根据名称提取对象的属性。如果把多个属性名传给 attrgetter，它也会返回提取的值构成的元组。此外，如果参数名中包含 .（点号），attrgetter 会深入嵌套对象，获取指定的属性。 在 operator 模块余下的函数中，最后介绍一下 methodcaller，其创建的函数会在对象上调用参数指定的方法： 12345678910111213In [11]: from operator import methodcallerIn [12]: s = 'The time has come'In [13]: upcase = methodcaller('upper')In [15]: upcase(s)Out[15]: 'THE TIME HAS COME'In [16]: hiphenate = methodcaller('replace', ' ', '-')In [17]: hiphenate(s)Out[17]: 'The-time-has-come' functools 模块另外，我们也可以透过 functools 中的 partial 和 partialmethod 函数来将现有函数的某些参数固定来生成新的函数，减少重复并提升代码的可读性，这两个函数在此不多做赘述。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 4 - 文本和字节序列","slug":"fluent-python-text-and-byte","date":"2019-03-17T14:36:12.000Z","updated":"2019-04-20T07:30:41.775Z","comments":true,"path":"2019/03/17/fluent-python-text-and-byte/","link":"","permalink":"https://homholueng.github.io/2019/03/17/fluent-python-text-and-byte/","excerpt":"","text":"了解编码问题如何找出字节序列的编码如果我们不知道一个文件使用的是什么编码格式，可以使用统一字符编码侦测包 Chardet 来帮助我们判断文件的编码： 12$ chardetect 04-text-byte.asciidoc04-text-byte.asciidoc: utf-8 with confidence 0.99 处理文本文件处理文本你文件的最佳实践是 “Unicode Sandwich”。即：要尽早的把输入（例如读取文本文件时）的字节序列解码成字符串，而三明治中的 “肉片” 是程序的业务逻辑，在这里只能处理字符串对象。最后，要尽可能晚的把字符串编码成字节序列。 同时，在进行文本文件的读取和写入时，最好显式地指定编解码方式，不要依赖当前环境的默认值： 123&gt;&gt;&gt; open('cafe.txt', 'w', encoding='utf-8').write('café')&gt;&gt;&gt; open('cafe.txt', encoding='utf-8').read()café 为了正确比较二规范化 Unicode 字符串因为 Unicode 有组合字符（变音符号和附加到前一个字符上的记号，打印时作为一个整体），所以字符串比较起来很复杂。 例如，“café” 这个词可以使用两种方式构成，分别有 4 个和 5 个码位，但是结果完全一样： 123456789101112In [26]: s1 = 'café'In [27]: s2 = 'cafe\\u0301'In [28]: s1, s2Out[28]: ('café', 'café')In [29]: len(s1), len(s2)Out[29]: (4, 5)In [30]: s1 == s2Out[30]: False 在 Unicode 标准中，&#39;é&#39; 和 &#39;e\\u0301&#39; 这样的序列叫“标准等价物”（canonical equivalent），应用程序应该把它们视作相同的字符。但是，Python 看到的是不同的码位序列，因此判定二者不相等。 这个问题的解决方案是使用 unicodedata.normalize 函数提供的 Unicode 规范化。这个函数的第一个参数是这 4 个字符串中的一个： &#39;NFC&#39;：使用最少的码位构成等价的字符串。 &#39;NFD&#39;：把组合字符分解成基字符和单独的组合字符。 &#39;NFKC&#39;：使用最少的码位构成等价的字符串，对兼容字符有影响。 &#39;NFKD&#39;：把组合字符分解成基字符和单独的组合字符，对兼容字符有影响。 1234567891011121314151617181920In [18]: from unicodedata import normalizeIn [19]: s1 = 'café'In [20]: s2 = 'cafe\\u0301'In [21]: len(s1), len(s2)Out[21]: (4, 5)In [22]: len(normalize('NFC', s1)), len(normalize('NFC', s2))Out[22]: (4, 4)In [23]: len(normalize('NFD', s1)), len(normalize('NFD', s2))Out[23]: (5, 5)In [24]: normalize('NFC', s1) == normalize('NFC', s2)Out[24]: TrueIn [25]: normalize('NFD', s1) == normalize('NFD', s2)Out[25]: True 西方键盘通常能输出组合字符，因此用户输入的文本默认是 NFC 形式。不过，安全起见，保存文本之前，最好使用 normalize(&#39;NFC&#39;, user_text) 清洗字符串。 兼容字符虽然 Unicode 的目标是为各个字符提供“规范的”码位，但是为了兼容现有的标准，有些字符会出现多次。例如，虽然希腊字母表中有 “μ” 这个字母（码位是 U+03BC，GREEK SMALL LETTER MU），但是 Unicode 还是加入了微符号 &#39;µ&#39;（U+00B5），以便与 latin1 相互 转换。因此，微符号是一个“兼容字符”。 在 NFKC 和 NFKD 形式中，各个兼容字符会被替换成一个或多个“兼容分解”字符。二分之一 &#39;½&#39;（U+00BD）经过兼容分解后得到的是三个字符序列 &#39;1/2&#39;；微符号 &#39;µ&#39;（U+00B5）经过兼容分解后得到的是小写字母 &#39;μ&#39;（U+03BC）。 1234567891011In [31]: from unicodedata import normalize, nameIn [32]: half = '½'In [33]: normalize('NFKC', half)Out[33]: '1⁄2'In [34]: four_squared = '4²'In [36]: normalize('NFKC', four_squared)Out[36]: '42' 从上面的例子中能够看出，NFKC 或 NFKD 可能会损失或曲解信息，在使用这两种规范化形式时要格外注意。 大小写折叠大小写折叠其实就是把所有文本变成小写，再做些其他转换。这个功能由 str.casefold() 方法（Python 3.3 新增）支持。 自 Python 3.4 起，str.casefold() 和 str.lower() 得到不同结果的有 116 个码位。Unicode 6.3 命名了 110 122 个字符，这只占 0.11%。 规范化文本匹配实用函数下面两个工具函数能够帮助我们对 Unicode 字符进行比较： 12345678910111213141516171819202122232425262728293031323334from unicodedata import normalizedef nfc_equal(str1, str2): return normalize('NFC', str1) == normalize('NFC', str2)def fold_equal(str1, str2): return (normalize('NFC', str1).casefold() == normalize('NFC', str2).casefold())In [38]: s1 = 'café'In [39]: s2 = 'cafe\\u0301'In [40]: s1 == s2Out[40]: FalseIn [41]: nfc_equal(s1, s2)Out[41]: TrueIn [42]: nfc_equal('A', 'a')Out[42]: FalseIn [43]: s3 = 'Straße'In [44]: s4 = 'strasse'In [46]: s3 == s4Out[46]: FalseIn [47]: nfc_equal(s3, s4)Out[47]: FalseIn [48]: fold_equal(s3, s4)Out[48]: True 去除变音符号12345678910111213import unicodedataimport stringdef shave_marks(txt): norm_txt = unicodedata.normalize('NFD', txt) shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c)) return unicodedata.normalize('NFC', shaved)In [56]: order = '“Herr Voß: • ½ cup of OEtker™ caffè latte • bowl of açaí.”'In [57]: shave_marks(order)Out[57]: '“Herr Voß: • ½ cup of OEtker™ caffe latte • bowl of acai.”' Unicode 文本排序在 Python 中，非 ASCII 文本的标准排序方式是使用 locale.strxfrm 函数，根据 locale 模块的文档 （https://docs.python.org/3/library/locale.html?highlight=strxfrm#locale.strxfrm)，这个函数会“把字符串转换成适合所在区域进行比较的形式”。 但是上述的排序方式需要更改当前环境下的全局区域设置，而且，还需要操作系统支持这一功能，所以，推荐使用 PyUCA 库来进行 Unicode 字符的排序： 12345678910In [1]: import pyucaIn [2]: coll = pyuca.Collator()In [3]: fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']In [4]: sorted_fruits = sorted(fruits, key=coll.sort_key)In [5]: sorted_fruitsOut[5]: ['açaí', 'acerola', 'atemoia', 'cajá', 'caju']","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 3 - 字典和集合","slug":"fluent-python-dict-and-set","date":"2019-03-16T13:50:45.000Z","updated":"2019-04-20T07:30:41.774Z","comments":true,"path":"2019/03/16/fluent-python-dict-and-set/","link":"","permalink":"https://homholueng.github.io/2019/03/16/fluent-python-dict-and-set/","excerpt":"","text":"映射的弹性键查询defaultdict：处理找不到的键的一个选择defaultdict 会在在初始化时接收一个构造方法，并在 __getietm__ 碰到找不到的键的时候调用该方法让 __getitem__ 返回某种默认值。 例： 1234567from collections import defaultdictdd = defaultdict(list)dd['key_not_exist'].append(1)dd['another_key_not_exist'].append(1) 注意，defaultdict 里的 default_factory 只会在 __getitem__ 里被调用，在其他的方法里完全不会发挥作用。比如，dd 是个 defaultdict，k 是个找不到的键，dd.get(k) 会返回 None。 特殊方法 __missing__所有的映射类型在在 __getitem__ 碰到找不到的键的时候，Python 都会调用其 __missing__ 方法，而不是抛出 KeyError。 注意，__missing__ 方法只会被 __getitem__ 调用（比如在表达式 d[k] 中）。提供 __missing__ 方法对 get 或者 __contains__（in 运算符会用到这个方法）这些方法的使用没有 影响。 像 k in my_dict.keys() 这种操作在 Python 3 中是很快的，而且即便映射类型对象很庞大也没关系。这是因为 dict.keys() 的返回值是一个“视图”。视图就像一个集合，而且跟字典类似的是，在视图里查找一个元素的速度很快（https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects）。 但 Python 2 的 dict.keys() 返回的是个列表，所以在处理庞大的映射类型对象时会比较慢。 字典的变种collections.OrderedDict这个类型在添加键的时候会保持顺序，因此键的迭代次序总是一致的。 collections.ChainMap该类型可以容纳数个不同的映射对象，然后在进行键查找操作的时候，这些对象会被当作一个整体被逐个查找，直到键被找到为止。这个功能在给有嵌套作用域的语言做解释器的时候很有用，可以用一个映射对象来代表一个作用域的上下文。 例如： 12import builtins pylookup = ChainMap(locals(), globals(), vars(builtins)) collections.Counter这个映射类型会给键准备一个整数计数器。每次更新一个键的时候都会增加这个计数器。所以这个类型可以用来给可散列表对象计数，或者是当成多重集来用——多重集合就是集合里的元素可以出现不止一 次。Counter 实现了 + 和 - 运算符用来合并记录，还有像 most_common([n]) 这类很有用的方法。 12345678&gt;&gt;&gt; ct = collections.Counter('abracadabra') &gt;&gt;&gt; ct Counter(&#123;'a': 5, 'b': 2, 'r': 2, 'c': 1, 'd': 1&#125;) &gt;&gt;&gt; ct.update('aaaaazzz') &gt;&gt;&gt; ct Counter(&#123;'a': 10, 'z': 3, 'b': 2, 'r': 2, 'c': 1, 'd': 1&#125;) &gt;&gt;&gt; ct.most_common(2) [('a', 10), ('z', 3)] colllections.UserDict这个类其实就是把标准 dict 用纯 Python 又实现了一遍，但它是专门让用户继承来写子类的。 子类化 UserDict而更倾向于从 UserDict 而不是从 dict 继承的主要原因是，后者有时 会在某些方法的实现上走一些捷径，导致我们不得不在它的子类中重写 这些方法，但是 UserDict 就不会带来这些问题。 另外一个值得注意的地方是，UserDict 并不是 dict 的子类，但是 UserDict 有一个叫作 data 的属性，是 dict 的实例，这个属性实际上 是 UserDict 最终存储数据的地方。 不可变映射类型从 Python 3.3 开始，types 模块中引入了一个封装类名叫 MappingProxyType。如果给这个类一个映射，它会返回一个只读的映 射视图。虽然是个只读视图，但是它是动态的。 集合论集合实现了许多基础的中缀运算符，给定两个集合 a 和 b： a | b 返回的是它们的合集 a &amp; b 返回的是它们的交集 a - b 返回的是它们的差集 a in b 返回 a 是否属于 b a &lt;= b 返回 a 是否是 b 的子集 a &lt; b 返回 a 是否是 b 的真子集 当我们要从集合 s 中移除一个元素时，如果我们关注该元素的存在性，则可以使用 s.remove(e) 方法，该方法在 e 不存在时会抛出 KeyError 异常；如果我们不关注该元素的存在性，则使用 s.discard(e) 即可。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 2 - 数据结构","slug":"fluent-python-index-data-structure","date":"2019-03-16T13:50:03.000Z","updated":"2019-04-20T07:30:41.774Z","comments":true,"path":"2019/03/16/fluent-python-index-data-structure/","link":"","permalink":"https://homholueng.github.io/2019/03/16/fluent-python-index-data-structure/","excerpt":"","text":"列表推导和生成器表达式用列表推到生成笛卡尔积用列表推导可以生成两个或以上的可迭代类型的笛卡儿积。 123456789colors = ['blck', 'white']sizes = ['S', 'M', 'L']tshirts = [(color, size) for color in colors for size in sizes]# [('black', 'S'), ('black', 'M'), ('black', 'L'), ('white', 'S'), ('white', 'M'), ('white', 'L')]tshirts = [(color, size) for size in sizes for color in colors]# [('black', 'S'), ('white', 'S'), ('black', 'M'), ('white', 'M'), ('black', 'L'), ('white', 'L')] 元组不仅仅是不可变的列表具名元组的一些方法 _fields(): 类方法，返回某个具名元组中的字段。 _make(iterable): 类方法，接收一个可迭代对象来生成类的实例。 _asdict(): 实例方法，返回以当前实例字段名为键，字段值为值的顺序字典。 演示如下： 1234567891011121314151617In [17]: City = collections.namedtuple(&apos;City&apos;, &apos;name country population coordinated&apos;)In [18]: LatLong = collections.namedtuple(&apos;LatLong&apos;, &apos;lat long&apos;)In [19]: delhi_data = (&apos;Delhi NCR&apos;, &apos;IN&apos;, 21.935, LatLong(28.613889, 77.208889))In [20]: delhi = City._make(delhi_data)In [21]: City._fieldsOut[21]: (&apos;name&apos;, &apos;country&apos;, &apos;population&apos;, &apos;coordinated&apos;)In [22]: delhi._asdict()Out[22]:OrderedDict([(&apos;name&apos;, &apos;Delhi NCR&apos;), (&apos;country&apos;, &apos;IN&apos;), (&apos;population&apos;, 21.935), (&apos;coordinated&apos;, LatLong(lat=28.613889, long=77.208889))]) 切片slice 对象了解切片对象能够让我们更好的了解 Python 切片操作实现的原理，并在需要进行大量且复杂的切片时提升代码的可读性。 当我们要对一个序列进行切片时，背后会调用该序列的 __getitem__ 方法，并且传入切片对象，例如 a[1:10:2] 等价于 a.__getitem__(slice(1, 10, 2) 利用这一特性，能够让切片操作可读性更强 12345DESCRIPTION = slice(6, 40)QUANTITY = slice(52, 55)for item in items: print(item[DESCRIPTION]) 对序列使用+和** 时采用的是浅拷贝需要注意的是，对序列使用 * 时得到的新序列中的元素是旧序列中元素的浅拷贝。 12345678910111213141516171819202122In [34]: a = []In [35]: b= [a]In [36]: bOut[36]: [[]]In [37]: c = b * 4In [38]: cOut[38]: [[], [], [], []]In [39]: a.append(1)In [40]: aOut[40]: [1]In [41]: bOut[41]: [[1]]In [42]: cOut[42]: [[1], [1], [1], [1]] 序列的增量赋值不可变序列的增量赋值对不可变序列进行增量赋值时，返回的是一个新的不可变序列。 123456789101112In [58]: t = (1, 2, 3)In [59]: id(t)Out[59]: 4488186256In [60]: t *= 2In [61]: tOut[61]: (1, 2, 3, 1, 2, 3)In [62]: id(t)Out[62]: 4470965640 虽然 str 也是不可变序列，但因为对 str 进行 += 操作是十分普遍的，所以 CPython 对其进行了优化，在为 str 初始化内存时，会额外分配内存空间。 三个教训 不要把可变对象放在元组中 增量赋值不是一个原子操作。 查看 Python 的字节码并不难 list.sort方法和内置函数sortedlist.sort 和 sorted 的区别list.sort 会改变原序列的顺序，而 sorted 只会返回一个新的序列，不会改变原序列的顺序。 用bisect来管理已排序的序列使用 bisect 来搜索使用 bisect 模块中的 bisect() 函数能够为我们在一个有序的序列中查找某个元素在不破坏原序列有序的情况下应该存在的位置： 123456In [48]: import bisectIn [49]: HAYSTACK = [1, 4, 5, 6, 8, 12, 15, 20, 21, 23, 23, 26, 29, 30]In [50]: bisect.bisect(HAYSTACK, 18)Out[50]: 7 用bisect.insort插入新元素使用 bisect 模块中的 insort() 函数能够在不破坏序列有序的情况下，将一个元素插入一个有序序列中。 当列表不是首选时什么时候应该使用数组如果我们需要一个只包含数字的列表，那么 array.array 比 list 更高效。 与 list 相比，array 中存储的元素类型是在创建后就固定了的，且能够存储的元素类型也是固定的： 12345678910111213141516array(typecode [, initializer]) -&gt; arrayType code C Type Minimum size in bytes&apos;b&apos; signed integer 1&apos;B&apos; unsigned integer 1&apos;u&apos; Unicode character 2 (see note)&apos;h&apos; signed integer 2&apos;H&apos; unsigned integer 2&apos;i&apos; signed integer 2&apos;I&apos; unsigned integer 2&apos;l&apos; signed integer 4&apos;L&apos; unsigned integer 4&apos;q&apos; signed integer 8 (see note)&apos;Q&apos; unsigned integer 8 (see note)&apos;f&apos; floating point 4&apos;d&apos; floating point 8 内存视图memoryview 是一个内置类，它能让用户在不复制内容的情况下操作同一个数组的不同切片。 并且 memoryview 允许我们以不同的方式读写同一块内存区域： 12345678910111213141516171819202122In [69]: numbers = array.array('h', [-2, -1, 0, 1, 2])In [70]: memv = memoryview(numbers)In [71]: memv_oct = memv.cast('B')In [72]: memv_octOut[72]: &lt;memory at 0x10b154b88&gt;In [73]: memv_oct.tolist()Out[73]: [254, 255, 255, 255, 0, 0, 1, 0, 2, 0]In [74]: memv.tolist()Out[74]: [-2, -1, 0, 1, 2]In [75]: memv_oct[5] = 4In [76]: memv_oct.tolist()Out[76]: [254, 255, 255, 255, 0, 4, 1, 0, 2, 0]In [77]: numbersOut[77]: array('h', [-2, -1, 1024, 1, 2])","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Chapter 1 - 序章","slug":"fluent-python-index-prologue","date":"2019-03-16T13:08:37.000Z","updated":"2019-04-20T07:30:41.775Z","comments":true,"path":"2019/03/16/fluent-python-index-prologue/","link":"","permalink":"https://homholueng.github.io/2019/03/16/fluent-python-index-prologue/","excerpt":"","text":"一摞 Python 风格的纸牌通过 __getitem__ 来自定义下标访问某些时候，我们需要重载自定义类的下标访问操作来实现 Python 的一致性，这个时候就需要通过实现 __getitem__ 方法来完成： 12345678910111213class FrenchDeck: ranks = [str(n) for n in range(2, 11)] + list('JQKA') suits = 'spades diamonds clubs hearts'.split() def __init__(self): self._cards = [Card(rank, suit) for suit in self.suits for rank in self.ranks] def __len__(self): return len(self._cards) def __getitem__(self, position): return self._cards[position] 使用 random 来随机选择序列中的值使用 random 模块中的 choice 函数能够随机返回一个非空序列中的某个值： 12345import randomseq = [1, 2, 3, 4, 5]random.choice(seq) 通过代理来实现 Python 语言的一致性何为一致性，即对不同类型的对象都能使用同一个方法或函数来实现特定的功能。如，无论是对于 list, str 还是 dict 对象，我们都能够使用 len() 函数来获取他们的长度。 若在自定义的类中，我们可以通过将这些一致性的方法或函数代理给类中的内部对象来实现 Python 语言的一致性，如实现 __len__ 方法将 len() 的操作代理给类中的列表对象。 如何使用特殊方法自定义布尔值默认情况下，我们自己定义的类的实例总被认为是真的，除非这个类对 __bool__ 或者 __len__ 函数有自己的实现。bool(x) 的背后是调用 x.__bool__() 的结果；如果不存在 __bool__ 方法，那么 bool(x) 会尝试调用 x.__len__()。若返回 0，则 bool 会返回 False；否则返回 True。","categories":[{"name":"Fluent Python","slug":"Fluent-Python","permalink":"https://homholueng.github.io/categories/Fluent-Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]},{"title":"Celery 使用指南","slug":"celery-introduction","date":"2019-02-09T08:03:44.000Z","updated":"2019-04-20T07:30:41.773Z","comments":true,"path":"2019/02/09/celery-introduction/","link":"","permalink":"https://homholueng.github.io/2019/02/09/celery-introduction/","excerpt":"","text":"概述Celery 是 Python 生态中一个比较有名的分布式任务队列，其具有轻量，灵活及可靠等特性。事实证明，在生产环境下，Celery 也同样值得信赖，有效的使用 Celery 能够帮助我们事半功倍的完成各种工作；另外，深入了解 Celery 的一些特性能够帮助开发者更好的使用它以及避免重复造轮子。本文会从 Celery 的若干个核心功能点出发，讲解 Celery 在日常使用中的一些基本特性及某些不常使用的深度功能，帮助开发者对 Celery 有更加全面和深入的了解。 由于 Celery 在系统中的定位是任务队列，所以其只提供了任务、Worker 的定义及一些周边的功能。Celery 本身需要依赖消息队列来进行消息的分发，即 Broker。本文中的例子所使用的语言及相关组件的版本如下： Python：3.6.1 Celery：4.2.0 Rabbitmq：3.7.8 基础对象 —— Application The Celery library must be instantiated before use, this instance is called an application (or app for short). 虽说 Celery 是一个开箱即用的框架，但是在使用它之前，我们还是需要做一些基础的配置及初始化的工作。首先，我们要先认识一下整个 Celery 库中的基础对象：Celery Application（后文简称 app）。当我们完成 app 的初始化后，Celery 就算是进入一个完全可用的状态了。可以将 app 比作整个 Celery 框架的心脏，整个框架都会围绕 app 对象来运行。 让心脏开始跳动 —— 创建一个 APP其实创建一个 app 十分简单： 1234from celery import Celeryapp = Celery()app # &lt;Celery __main__ at 0x103e93c50&gt; 只需要短短两句代码，就能够完成了一个主模块名为 __main__ 的 app 的创建。在开始下面的内容之前，需要了解一个十分重要的概念：主模块名。当 Celery 无法决定某一个任务定义函数属于哪一个模块时，其会使用 app 的主模块名作为该函数所属的模块名，如下所示： 123456789from celery import Celeryapp = Celery()@app.taskdef a_task(): returna_task # &lt;@task: __main__.a_task of __main__ at 0x11038be48&gt; 任务名为 __main__.a_task 当然，我们能够在初始化 app 的时候手动配置主模块名： 123456789from celery import Celeryapp = Celery('main_module_name')@app.taskdef a_task(): returna_task # &lt;@task: main_module_name.a_task of main_module_name at 0x110939550&gt; 此时任务的名称变成了 main_module_name.a_task 通过上面的两个例子可以看到，Celery 为了降低使用成本，在初始化 app 时会将许多配置设置为默认值。如果 Celery 设置的默认值不符合你的需求，那么你可以通过 app 对象来修改这些配置。 使用 app 配置 Celery通过改变 app 的配置，能够改变 Celery 运行的行为模式。Celery 提供了多种手段来让开发者进行 app 的配置，包括： 直接修改 app.conf 中的值 通过 config 对象进行配置 通过环境变量进行配置 举个例子，假设我们要让 Celery 在 Asia/Shanghai 时区下工作，并且使用 broker-host.com 提供的 Broker，那么我们可以使用以下几种方法来进行配置： 1. 直接修改 app.conf 中的值1234567from celery import Celeryapp = Celery()app.conf.timezone = 'Asia/Shanghai'app.conf.enable_utc = Trueapp.conf.broker_url = 'amqp://guest@broker-host.com//' 2. 通过 config 对象进行配置此处的 config 对象可以是一个定义了配置属性的模块，也可以是一个拥有配置属性的对象。如果你喜欢将配置集中放在某个模块下，可以这样进行配置： 1234567891011121314# ./conf.pytimezone = 'Asia/Shanghai'enable_utc = Truebroker_url = 'amqp://guest@broker-host.com//'# ./app.pyfrom celery import Celeryimport confapp = Celery()app.config_from_object(conf) 或者你也可以选择使用类来管理这些配置： 12345678910from celery import Celeryapp = Celery()class Config: timezone = 'Asia/Shanghai' enable_utc = True broker_url = 'amqp://guest@broker-host.com//'app.config_from_object(Config) 3. 通过环境变量进行配置如果我们能够预置好若干个配置模块，然后在 Celery 启动的时候再根据当前的环境选择特定的配置模块，就会让我们的代码更加的灵活。好在 Celery 允许开发者通过环境变量来传递配置模块名，使得我们得以实现这一功能： 12345678import osfrom celery import Celery# 设置默认的配置模块名os.environ.setdefault('CELERY_CONFIG_MODULE', 'celeryconfig')app = Celery()app.config_from_envvar('CELERY_CONFIG_MODULE') 上述的若干个例子中仅仅展示了 Celery 配置选项中的冰山一角，如果想要了解完整的配置选项，请移步配置手册。 任务队列中的任务 —— Task如果说 app 是 Celery 的心脏，那么任务就是 Celery 的灵魂。在 Celery 中，任务能过通过任意可调用对象创建出来，如下所示： 1234567from celery import Celeryapp = Celery()@app.task()def add(x, y): return x + y 这里要引入一个绑定任务（bound task）的概念，如果我们将某个可调用对象设定为绑定任务，那么在任务执行时，我们就能够拿到当前任务对象（Task）： 1234567from celery import Celeryapp = Celery()@task(bind=True)def add(self, x, y): return x + y 当某个可调用对象被设置成绑定任务后，该对象被调用后第一个参数一定是当前被执行的任务对象实例（Task），与 Python 中的绑定方法（bound method）相似。在该任务实例中，我们能够拿到一些比较有用的信息： 123456789101112from celery import Celeryapp = Celery()@task(bind=True)def add(self, x, y): logger.info(self.request.id) # 获取该次任务的唯一 ID logger.info(self.request.args) # 获取该次任务请求的位置参数 logger.info(self.request.kwargs) # 获取该次任务请求的关键字参数 logger.info(self.acks_late) # 该任务是否是延迟确认任务 logger.info(self.backend) # 该任务的结果存储后端 ... 任务的命名 Every task must have a unique name. 为什么每个任务都需要拥有一个唯一的名字呢？因为 Celery 内部会通过 name -&gt; task 的映射关系来维护用户注册的任务。当我们向 Celery 发起一个执行任务的请求时，Celery 并不会将整段任务定义的代码放到队列中，而是将任务的名字放入执行请求中，当 worker 收到该请求后，则会根据任务名字来找到任务的定义。所以任务的名字必须是唯一的。 但是在上述的一些例子中，我们在定义任务时并没有显示的为任务设置名称，那么 Celery 是如何找到这些任务的呢？ 在我们没有显式的设置任务名称时，Celery 会为我们定义的任务设置默认名，这个默认名会根据任务函数所在的模块及任务函数的命名来确定。也就是说，如果我们在 tasks.py 文件下定义了一个名为 add 的任务，那么 Celery 默认会将其名字设置为 tasks.add： 1234567# tasks.py@app.taskdef add(x, y): return x + yprint(add.name) # 'tasks.add' 当然，你也可以在定义任务时显式的为这些任务命名： 1234567# tasks.py@app.task(name='task.tasks.add')def add(x, y): return x + yprint(add.name) # 'task.tasks.add' 请在使用自动命名时确保一致的导入方式Celery 为任务设置默认名的行为的确方便了开发者，但是也带来了一些潜在的问题，假如我们有下面这样的一个目录结构： 123task├── __init__.py└── tasks.py 然后在 task/tasks.py 文件中定义了这样的一个任务： 123@app.taskdef add(x, y): return x + y 如果我们尝试在不同的目录下导入该任务，就会出现任务名不一致的情况： 1234567&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; add.name'tasks.add'&gt;&gt;&gt; from task.tasks import add&gt;&gt;&gt; add.name'task.tasks.add' 也就是说，如果 worker 与主程序导入任务的方式不同，那么在主程序发送执行任务的请求时 worker 就会抛出 NotRegistered 错误，因为主程序看到的任务名与 worker 看到的任务名是不同的。除了保持一致的任务导入方式外，我们还可以通过显式的为任务命名来规避这个问题，当我们显式的为某个任务设置名称后，该任务的名字就不会因导入的方式变化而变化了： 1234567891011@app.task(name='task.tasks.add')def add(x, y): return x + y&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; add.name'task.tasks.add'&gt;&gt;&gt; from task.tasks import add&gt;&gt;&gt; add.name'task.tasks.add' 任务注册表在上面一个小节中，我们不止一次提到任务注册表这个概念，其实在 Celery 中，要获取任务注册表非常简单，app 对象中的 tasks 属性就是 Celery 的任务注册表： 12345678&gt;&gt;&gt; from proj.celery import app&gt;&gt;&gt; app.tasks&#123;'celery.chord_unlock': &lt;@task: celery.chord_unlock&gt;, 'celery.backend_cleanup': &lt;@task: celery.backend_cleanup&gt;, 'celery.chord': &lt;@task: celery.chord&gt;&#125; 需要注意的是，只有当定义任务的模块在代码中被导入了，这些任务才会出现在注册表中。所以，Celery worker 和主程序中都维护了这样的一个字典，并通过唯一的任务名来映射到具体的任务。 Make some noise —— Calling Tasks任务队列最迷人的地方不是它对任务的定义，而是其执行任务的能力。在我们定义好各种各样的任务之后，我们还需要去调用他们，发挥其真正的价值。但是在 Celery 中，调用任务的姿势（方法）也是一门小学问。比如对于一个简单的 add 任务，就能够通过以下几种方式去执行： 123456from tasks import addadd.delay(1, 2)add.apply_async(args=(1, 2))add.apply_async((1, 2), countdown=10)add(1, 2) 看似复杂，但其实 Celery 调用任务的方式能够总结为以下三种： apply_async(args[, kwargs[, …]])：调用任务的标准 API，调用者能够按需传入各种执行选项，上述例子中的 add.apply_async((1, 2), countdown=10) 调用即传入了 countdown 选项，这使得该任务会在 10 秒后被执行。 delay(*args, **kwargs)：apply_async 的捷径方法，不支持传入执行选项。 直接调用（__call__）：直接调用该任务，如上述代码中的 add(1, 2)。若使用这种调用方式，则任务不会被发送到 Worker 中，而是在当前线程中执行。 当我们不需要使用执行选项时，笔者推荐各位尽量使用 delay 来执行任务，使用 delay 会让发起任务的代码看起来更加自然： 12task.delay(arg1, arg2, kwarg1='x', kwarg2='y') # 使用 delaytask.apply_async(args=[arg1, arg2], kwargs=&#123;'kwarg1': 'x', 'kwarg2': 'y'&#125;) # 使用 apply_async 执行选项当我们对任务的某次执行有一些特殊的要求时，就需要用到 apply_async 接口提供的各种执行选项了，例如你希望某个任务在 30 秒后再开始执行： 12345from datetime import datetime, timedelta# 以下两个调用是等价的add.apply_async((2, 2), countdown=30)add.apply_async((2, 2), eta=datetime.utcnow() + timedelta(seconds=30)) 或者是你希望为某个任务设置一个过期时间（当 worker 收到一个过期的任务时，会放弃执行该任务）： 12345from datetime import datetime, timedelta# 以下两个调用是等价的add.apply_async((10, 10), expires=5)add.apply_async((10, 10), expires=datetime.utcnow() + timedelta(seconds=5)) 该方法支持的执行选项还有很多，就不在此一一赘述，感兴趣的读者可以到 API 手册中了解。笔者推荐各位了解一下各个执行选项的作用，这样能够使你在遇到一些新的需求时能够更加快速的找到解决方案，避免重复造轮子。 周期任务 —— Periodic TasksCelery 自带了一个名为 celery beat 的调度器，这个调度器能够让我们周期性的执行一些我们预先定义好的任务。 注册周期任务只要将任务加入到调度列表中，调度器就能够周期性的执行这些任务： 12345678910111213141516171819202122from celery import Celeryfrom celery.schedules import crontabapp = Celery()@app.on_after_configure.connectdef setup_periodic_tasks(sender, **kwargs): # 每 10s 调用一次 test('hello') sender.add_periodic_task(10.0, test.s('hello'), name='add every 10') # 每 30s 调用一次 test('hello') sender.add_periodic_task(30.0, test.s('world'), expires=10) # 在每周一的早上 7:30 调用一次 test('Happy Mondays') sender.add_periodic_task( crontab(hour=7, minute=30, day_of_week=1), test.s('Happy Mondays!'), )@app.taskdef test(arg): print(arg) 上述代码中，我们使用 on_after_configure 信号在 app 完成配置后执行周期任务设置的函数，并通过 add_periodic_task 方法将定义好的任务添加到调度器的调度列表中。我们添加到调度列表的周期任务，其实也维护在 app 内部的一个字典中： 12345678910111213141516&gt;&gt;&gt; app.conf.beat_schedule&#123;'add every 10': &#123;'schedule': 10.0, 'task': 'periodic_tasks.test', 'args': ('hello',), 'kwargs': &#123;&#125;, 'options': &#123;&#125;&#125;, \"periodic_tasks.test('world')\": &#123;'schedule': 30.0, 'task': 'periodic_tasks.test', 'args': ('world',), 'kwargs': &#123;&#125;, 'options': &#123;'expires': 10&#125;&#125;, \"periodic_tasks.test('Happy Mondays!')\": &#123;'schedule': &lt;crontab: 30 7 1 * * (m/h/d/dM/MY)&gt;, 'task': 'periodic_tasks.test', 'args': ('Happy Mondays!',), 'kwargs': &#123;&#125;, 'options': &#123;&#125;&#125;&#125; 不难发现，周期任务的注册表也是维护在 app.conf 下的，这就意味着，如果你不喜欢通过在代码中调用 API 来注册周期任务，可以选择通过配置的方式来设定周期任务： 12345678910111213141516171819202122232425262728293031# conf.pyfrom celery.schedules import crontabbeat_schedule = &#123;'add every 10': &#123;'schedule': 10.0, 'task': 'periodic_tasks.test','args': ('hello',),'kwargs': &#123;&#125;,'options': &#123;&#125;&#125;,\"periodic_tasks.test('world')\": &#123;'schedule': 30.0,'task': 'periodic_tasks.test','args': ('world',),'kwargs': &#123;&#125;,'options': &#123;'expires': 10&#125;&#125;,\"periodic_tasks.test('Happy Mondays!')\": &#123;'schedule': crontab(hour=7, minute=30, day_of_week=1),'task': 'periodic_tasks.test','args': ('Happy Mondays!',),'kwargs': &#123;&#125;,'options': &#123;&#125;&#125;&#125;# tasks.pyfrom celery import Celeryimport confapp = Celery()app.config_from_object(conf)@app.taskdef test(arg): print(arg) 调度策略如果我们仅仅希望某个任务在固定的时间间隔后被执行，那么只需要在注册周期任务时传递固定的调度间隔即可： 12# 每 10s 调用一次 test('hello')sender.add_periodic_task(10.0, test.s('hello'), name='add every 10') 但在某些场景下， 可能需要更加复杂的调度规则，如在某些天的某个时间点进行调度，这个时候就可以考虑使用 Celery 内置的一些调度策略。其中一个比较常用的调度策略为 crontab： 12345678910from celery.schedules import crontabapp.conf.beat_schedule = &#123; # 在每周一的早上 7:30 调用一次 test('Happy Mondays') 'add-every-monday-morning': &#123; 'task': 'tasks.add', 'schedule': crontab(hour=7, minute=30, day_of_week=1), 'args': (16, 16), &#125;,&#125; crontab 本身还支持十分灵活的表达式，关于其更详细的用法，在此不做赘述，有兴趣的读者请移步 crontab API 手册。另外一个比较有意思的调度策略是 Celery 提供的 solar schedule，其会根据地球上某个地点的日出日落或是黄昏黎明时间来触发任务执行。 启动调度器完成了周期任务的注册和策略的选择后，只需要把调度器启动起来，就大功告成了: 1$ celery -A tasks beat Django 用户看这里如果你是 Django 用户，在使用 celery beat 之前不妨先了解一下 django-celery-beat 这个扩展，其通过扩展 celery beat 的 Scheduler 类，提供了以下功能： 将周期任务配置持久到数据库中 通过 Django admin 来管理周期任务 为每个 crontab 策略分配不同的时区 职责划分 —— Routing Tasks在我们没有手动设置任务路由的情况下，Celery 会为我们在 Rabbitmq 创建一个默认队列，所有的任务执行请求都会被发送到该队列中，我们启动的 worker 也只会默认从该队列中获取消息。这种行为在一些较为简单的场景下并不会出现问题，但假设我们的任务平均耗时相差很大，那么一些平均耗时较长的任务很可能会阻塞后面到来的平均耗时较短的任务，导致系统中的任务平均等待时间较长。 这时候可以考虑设置多个队列，将不同类型的任务路由到不同的队列中，每个队列都分配一定数量的 worker 来处理。其实在实际场景下，并不一定是根据任务的耗时来进行分配，也可以选择根据任务的重要程度，性质（计算密集或 IO 密集），甚至是内部实现细节来进行任务路由的配置。而换句话说，设置任务的路由实际上就是对 worker 进行职责划分，如果两个 worker 处理的队列不同，那么我们可以认为其在系统中所属的角色不同，即职责不同，通过更细化的配置来更好地利用计算机资源。 下面的例子就展示了如何将两个任务分配到不同的队列中： 12345678910111213141516from celery import Celeryapp = Celery()app.conf.task_routes = &#123; 'task.tasks.task_1': &#123;'queue': 'queue_1'&#125;, 'task.tasks.task_2': &#123;'queue': 'queue_2'&#125;&#125;@app.task(name='task.tasks.task_1')def task_1(): return@app.task(name='task.tasks.task_2')def task_2(): return 当然，也可以使用通配符来配置任务的路由： 123456789101112131415from celery import Celeryapp = Celery()app.conf.task_routes = &#123; 'task.tasks.task_*': &#123;'queue': 'queue_1'&#125;&#125;@app.task(name='task.tasks.task_1')def task_1(): return@app.task(name='task.tasks.task_2')def task_2(): return 在完成了任务路由的配置后，我们需要在启动 worker 时指定其监听的队列： 12$ celery worker -A tasks -Q queue_1 # 监听 queue_1$ celery worker -A tasks -Q queue_1,celery # 同时监听 queue_1 及 celery 简单的工作流 —— Work-flow除了提供单个任务的定义和执行能力之外，Celery 还允许我们将多个任务组合串联起来，形成一整个工作流，在学习如何使用这些能力之前，必须要先了解任务签名这一概念。 任务签名在 Celery 中，任务签名代表了单个任务的一次调用及相关的参数和执行选项，换句话说，任务签名表示一次任务调用的行为。任务签名的存在能够使得开发者将任务调用这一行为进行序列化，或在参数中进行传递。创建一个任务签名十分简单： 12345678910from celery import signaturefrom tasks import add# 以下两种创建方式是等价的s1 = add.signature((2, 2), countdown=10)s2 = signature('tasks.add', args=(2, 2), countdown=10)# 捷径方法，无法设置执行选项s3 = add.s(2, 2)s4 = add.s(2, 2).set(countdown=10).set(retry=False) # 通过 set 方法来设置执行选项 既然任务签名代表了一次调用的命令，我们自然能够使用这个命令来完成任务的调用，但是需要注意的是，任务签名中的执行选项会被调用执行函数时传入的执行选项覆盖： 12345678910from tasks import adds = add.s(2, 2)s() # 在当前进程调用s.apply_async() # 发送到 worker 执行s.delay() # 发送到 worker 执行的捷径方式s_with_option = add.s(2, 2).set(countdown=1)s_with_option.apply_async(countdown=10) # 任务会在 10s 后开始执行，而不是 1s 相信大家都知道 Python 中 functools 模块下的 partial 函数能预置已有函数的某些参数以创建出一个新的函数。而在 Celery 中，任务签名也支持这一特性，在创建任务签名时我们不必将所有的参数都预先设置好，可以预留一些参数在真正执行时再传入： 1234567from tasks import addpartial_s = add.s(2)# 以下两个调用是等价的partial_s.delay(4)partial_s.apply_async((2,)) 有了这个特性后，我们就能够在任务的连接和编排中将父任务的结果作为参数传递到子任务中。 了解了任务签名这个概念后，让我们来看一个使用任务签名来实现任务回调的简单例子，在这个例子中，我们要通过 add 任务来实现 (4 + 4) + 8 这一工作，由于 add 任务单次只能完成两个操作数的相加，所以我们需要组合两个 add 任务来完成这项工作，第一个 add 计算 4 + 4，并把结果传递给第二个 add，第二个 add 则负责完成 8 + 8 的计算，代码如下： 12345678910from celery import Celeryapp = Celery()@app.taskdef add(x, y): return x + ys = add.s(8)add.apply_async((4, 4), link=s) # 使用 link 选项来指定 add 的回调任务 可以看到，通过使用任务签名和回调，我们能够将多个任务串联起来执行，并在任务间传递执行结果。这里需要注意的是：回调只会在当前任务执行成功的情况下才会发生，并且，父任务的返回值会作为参数传递给回调任务（子任务）。 执行原语在了解了任务签名及回调的概念后，就可以正式开始学习工作流的编排了。Celery 通过各种执行原语的组合来实现工作流的编排，而这些原语本身也是任务签名，所以开发者能够通过将原语进行再组合来形成更加复杂的任务流。 groupgroup 原语能够让一组任务并行执行： 123456789from celery import groupfrom tasks import addg = group(add.s(2, 2), add.s(4, 4))# 以下三个调用是等价的g()g.delay()g.apply_async() 执行 group 后会返回一个 GroupResult 对象，这个对象中记录了这一组任务的执行结果和一些执行信息： 12345678910111213141516171819&gt;&gt;&gt; from celery import group&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; job = group([... add.s(2, 2),... add.s(4, 4),... add.s(8, 8),... add.s(16, 16),... add.s(32, 32),... ])&gt;&gt;&gt; result = job.apply_async()&gt;&gt;&gt; result.ready() # 是否所有任务都完成了True&gt;&gt;&gt; result.successful() # 是否所有任务都成功了True&gt;&gt;&gt; result.get()[4, 8, 16, 32, 64] chainchain 原语允许我们将任务像链表一样串起来，一个接着一个的执行： 123456from celery import chainfrom tasks import add# 以下两个执行是等价的c = chain(add.s(2, 2), add.s(4), add.s(8))c = add.s(2, 2) | add.s(4) | add.s(8) 在这里，我们要先了解一下 Celery 中对于父任务和子任务的定义，我们在上述代码中创建的任务链如下图所示： 在 Celery 中，某个任务的前置任务被称为其父任务，也就是说，上图中的 task1 是 task2 的父任务，而 task2 是 task3 的父任务。 了解了这个概念之后，我们再来看看 Celery 为我们提供的一些 API。我们在调用 chain 之后，会拿到一个 AsyncResult 对象，通过调用其 get() 方法我们能够获取任务链的调用结果，但是这个结果是任务链最后的一个任务返回的结果，如果你需要获取链条中其他任务的执行结果，可以通过 parent 属性向前获取其父任务的执行结果： 123456789from celery import chainfrom tasks import addc = add.s(2, 2) | add.s(4) | add.s(8)r = c.apply_async()r.get() # return 16r.parent.get() # return 8 r.parent.parent.get() # return 4r.parent.parent.parent # None 不仅如此，我们还能为任务链添加错误处理函数，当链条中的某一个任务出错时，就会调用这个处理函数： 12345678910111213141516171819202122import loggingfrom celery import Celerylogger = logging.getLogger('celery')app = Celery('tasks')@app.task(name='task.tasks.task_1')def task_1(fail=False): if fail: raise Exception()@app.task(name='task.tasks.task_2')def task_2(fail=False): if fail: raise Exception()@app.task(name='task.tasks.log_error')def log_error(request, exc, traceback): logger.error('bad thing happened!') logger.error('--\\n\\n&#123;0&#125; &#123;1&#125; &#123;2&#125;'.format(request.id, exc, traceback)) 让我们来试一下： 12345678910111213&gt;&gt;&gt; c = task_1.s(True) | task_2.s()&gt;&gt;&gt; c.apply_async(link_error=log_error.s())&lt;AsyncResult: 2339608b-b735-4510-85f8-cad6abf7cbc4&gt;# 观察 worker 的输出，正确打印出了出错任务 id，异常和堆栈信息&gt;&gt;&gt; c.on_error(log_error.s()).delay()&lt;AsyncResult: 871e2a58-d9b1-49be-84d4-be7514eeb6f6&gt;# 观察 worker 的输出，正确打印出了出错任务 id，异常和堆栈信息&gt;&gt;&gt; c = task_1.s() | task_2.s(True)&gt;&gt;&gt; c.on_error(log_error.s()).delay()...TypeError: task_2() takes from 0 to 1 positional arguments but 2 were given 可以看到链条中的任何一个任务出错后，处理函数都会被调用，但是，当我们尝试让第二个任务出错时，貌似抛出的异常和预期设置的不一样。 这是因为 Celery 会把父任务的返回值向子任务传递，但是在 c = task_1.s() | task_2.s(True) 语句中我们却给 task_2 手动设置了 fail 的值，这样 task_2 在调用时就会收到两个参数，自然会抛出位置参数过多的异常。要解决这个问题，只需要使用不可变签名即可： 123# 以下两个调用是等价的si = task_2.signature(args=(True,), immutable=True)si = task_2.si(True) 不可变签名在创建后就不会再接收其他的参数，所以父任务传过来的参数也自然会被忽略： 123&gt;&gt;&gt; c = task_1.s() | task_2.si(True)&gt;&gt;&gt; c.on_error(log_error.s()).delay()# 这次即可看到正确的异常被抛出 如果你在任务链条中，不希望某个任务接收父任务的返回值，请务必使用不可变签名。 chord使用 chord 原语能够让某个任务在一组任务完成执行后再开始执行。 例如我们要计算 1 + 1 + 2 + 2 + ... + n + n 的结果，我们可以将这个任务分为两个步骤（这是一个十分刻意的例子，只是为了说明 chord 的使用方法，最佳解决方案是直接计算 sum(i + i for i in range(n))）： 完成 n 个 i + i 的计算 将第 1 步的所有结果进行求和 那么首先我们需要将任务定义好： 1234567891011from celery import Celeryapp = Celery()@app.taskdef add(x, y): return x + y@app.taskdef tsum(numbers): return sum(numbers) 然后，动手： 12345&gt;&gt;&gt; from celery import chord&gt;&gt;&gt; from tasks import add, tsum&gt;&gt;&gt; chord(add.s(i, i) for i in range(100))(tsum.s()).get()9900 首先 chord(add.s(i, i) for i in range(100)) 返回了一个可调用对象 chord，然后我们调用了这个对象并将 tsum 的签名传了进去，最后调用 get() 取得我们想要的结果。上述的例子也能被拆解为以下的方式来调用： 12345&gt;&gt;&gt; callback = tsum.s()&gt;&gt;&gt; header = [add.s(i, i) for i in range(100)]&gt;&gt;&gt; result = chord(header)(callback)&gt;&gt;&gt; result.get()9900 也就是说，chord 操作分为两个主要的部分：header 和 callback，callback 会在 header 中所有任务都返回后才开始执行，且 header 中的任务的返回值会作为参数传递到 callback 中。但是要注意的是，如果 header 中的任意一个任务执行失败了，那么整个 chord 就会进入失败状态，不能再继续往下执行，对 chord 的失败处理与 chain 类似，可以参考 chord error-handling。 map &amp; starmapmap 和 startmap 都是在某个序列上重复的执行一个任务，但是其与 group 不同之处在： 只会执行一次任务 执行是串行的 map 的使用如下： 12&gt;&gt;&gt; tsum.map([range(10), range(100), range(1000)])()[45, 4950, 499500] 等价于执行了以下任务： 123@app.taskdef temp(): return [tsum(range(10)), tsum(range(100)), tsum(range(1000))] 而 startmap 的使用如下： 12&gt;&gt;&gt; add.starmap(zip(range(10), range(10)))()[0, 2, 4, 6, 8, 10, 12, 14, 16, 18] 等价于执行了以下任务： 123@app.taskdef temp(): return [add(i, i) for i in range(10)] 那么 map 和 startmap 到底有什么区别呢？如果你要调用的任务只接受一个参数，那请使用 map，如果该任务接收多个参数，请使用 starmap。 chunk最后让我们来看看 chunk ，chunk 能够让我们对任务进行分块，假如你有一个任务需要处理 100 个对象，那么你可以使用 chunk 将其分成 10 个任务，每个任务处理 10 个对象。如下所示： 1234567891011&gt;&gt;&gt; add.chunks(zip(range(100), range(100)), 10)().get()[[0, 2, 4, 6, 8, 10, 12, 14, 16, 18], [20, 22, 24, 26, 28, 30, 32, 34, 36, 38], [40, 42, 44, 46, 48, 50, 52, 54, 56, 58], [60, 62, 64, 66, 68, 70, 72, 74, 76, 78], [80, 82, 84, 86, 88, 90, 92, 94, 96, 98], [100, 102, 104, 106, 108, 110, 112, 114, 116, 118], [120, 122, 124, 126, 128, 130, 132, 134, 136, 138], [140, 142, 144, 146, 148, 150, 152, 154, 156, 158], [160, 162, 164, 166, 168, 170, 172, 174, 176, 178], [180, 182, 184, 186, 188, 190, 192, 194, 196, 198]] 观察 worker 的输出，不难发现，chunk 背后使用了 startmap 来实现： 123456789[2019-02-09 17:42:42,901: INFO/MainProcess] Received task: celery.starmap[c1a74285-4029-4389-829d-ba4a224343bc][2019-02-09 17:42:42,904: INFO/ForkPoolWorker-2] Task celery.starmap[c1a74285-4029-4389-829d-ba4a224343bc] succeeded in 0.0017125179874710739s: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18][2019-02-09 17:42:42,904: INFO/MainProcess] Received task: celery.starmap[d42ce391-f6c2-4c2d-ac53-ffe0b12f6bdf][2019-02-09 17:42:42,906: INFO/MainProcess] Received task: celery.starmap[6f1688fa-6339-41dc-8060-5111aa961427][2019-02-09 17:42:42,908: INFO/MainProcess] Received task: celery.starmap[9ebe51a4-de65-46ba-8df8-5de02641bf12][2019-02-09 17:42:42,908: INFO/ForkPoolWorker-1] Task celery.starmap[d42ce391-f6c2-4c2d-ac53-ffe0b12f6bdf] succeeded in 0.002243652008473873s: [20, 22, 24, 26, 28, 30, 32, 34, 36, 38][2019-02-09 17:42:42,908: INFO/ForkPoolWorker-2] Task celery.starmap[6f1688fa-6339-41dc-8060-5111aa961427] succeeded in 0.0008158769924193621s: [40, 42, 44, 46, 48, 50, 52, 54, 56, 58][2019-02-09 17:42:42,909: INFO/MainProcess] Received task: celery.starmap[949611d3-7a49-4231-8c62-5c04d2e4bde2].... 使用 group 方法，我们能够将一个 chunk 转成 group，这样就能够将 chunk 和 chord 结合起来使用： 123456789@app.task(name='task.tasks.lsum')def lsum(chunks): return sum([sum(l) for l in chunks])&gt;&gt;&gt; chord(group(add.s(i, i) for i in range(100)))(tsum.s()).get() # chord19900&gt;&gt;&gt; chord(add.chunks(zip(range(100), range(100)), 10).group())(lsum.s()).get() # chord29900 虽然上面两个 chord 的结果是相同的，但是执行的过程是不一样的： chord1 会先启动 100 个 add 任务计算 1 - 100 的合，完成后再启动 tsum 求和。 chord2 会先启动 10 个 add starmap 任务计算 1 - 100 的合，完成后再启动 lsum 求和。 所以，根据实际使用场景，合理的使用 chunk 和 group 能够让我们的任务流更加清晰，也更加的高效。 小建议Celery 虽然提供了任务工作流的功能，但是笔者并不推荐各位用这些原语来构造十分复杂和冗长的任务流，因为 Celery 只是提供了任务的串联和编排功能，对整个任务流程在执行过程中的一些控制（如暂停，失败重试，跳过等）能力却几乎为0，这意味着如果你构造出了一个十分复杂的流程，那么你就要去对这个流程执行中各种可能的情况进行预测和处理。如果你需要复杂流程编排、执行及控制的能力，请尝试使用工作流引擎来完成。 关于 worker关于 Celery，还有另外一个十分重要的组件就是 worker 了，可以说 worker 为 Celery 提供了无穷无尽的计算能力，在整个框架中承担着十分重要的职责。那么，深入的了解 worker 的一些功能，能够在我们日常开发和生产中解决问题时提供更多的思路。下面会重点讲解 worker 的探测和远程控制，并提及一些注意事项。 探测器通过使用探测器，我们能够及时的获取当前所有可用 worker 及其状态，探测器同样从 app 实例中获取： 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; from celery import current_app as app&gt;&gt;&gt; i = app.control.inspect() # 探测所有 worker&gt;&gt;&gt; app.control.inspect().ping() # 对所有 worker 广播 ping 消息&#123;'worker1.example.com': &#123;'ok': 'pong'&#125;&#125;&gt;&gt;&gt; i.registered() # 获取所有 worker 中注册的任务[&#123;'worker1.example.com': ['tasks.add', 'tasks.sleeptask']&#125;]&gt;&gt;&gt; i.active() # 获取所有 worker 当前正在执行的任务[&#123;'worker1.example.com': [&#123;'name': 'tasks.sleeptask', 'id': '32666e9b-809c-41fa-8e93-5ae0c80afbbf', 'args': '(8,)', 'kwargs': '&#123;&#125;'&#125;]&#125;]&gt;&gt;&gt; i.scheduled() # 获取所有 worker 的调度任务信息[&#123;'worker1.example.com': [&#123;'eta': '2010-06-07 09:07:52', 'priority': 0, 'request': &#123; 'name': 'tasks.sleeptask', 'id': '1a7980ea-8b19-413e-91d2-0b74f3844c4d', 'args': '[1]', 'kwargs': '&#123;&#125;'&#125;&#125;, &#123;'eta': '2010-06-07 09:07:53', 'priority': 0, 'request': &#123; 'name': 'tasks.sleeptask', 'id': '49661b9a-aa22-4120-94b7-9ee8031d219d', 'args': '[2]', 'kwargs': '&#123;&#125;'&#125;&#125;]&#125;] 上面列出的只是探测器的部分功能，有兴趣的读者可以看一看探测器 API 手册。 远程控制Celery 能够通过 broker 中的一个高优先级队列来向所有的 worker 广播或是向某一些 worker 发送消息，从而实现远程控制的功能，上一小节的探测器功能就是通过远程控制来实现的。同时，worker 也能够响应这些消息，类似上一小节中的 ping 命令，但是由于我们无法得知集群中到底有多少个 worker，所以在使用一些响应式命令时，我们需要配置超时时间。如果某些 worker 在超时前始终没有回复我们发送的消息，并不代表这个 worker 不可用了，有可能因为网络延迟或是该 worker 正忙于处理其他命令而导致超时。 广播函数Celery 中的 broadcast 函数就是用于向所有的 worker 广播消息的，在远程控制的所有接口中，有一些（ping，ratelimit）则是广播函数的高阶实现，下面让我们看一下广播函数的使用示例，下面的代码向所有的 worker 发送了一条 rate_limit 消息，让所有的 worker 对名为 myapp.mytask 的任务进行限速： 1234&gt;&gt;&gt; from celery import current_app as app&gt;&gt;&gt; app.control.broadcast('rate_limit',... arguments=&#123;'task_name': 'myapp.mytask',... 'rate_limit': '200/m'&#125;) 想要深入了解广播函数，请移步 broadcast 函数参考。 撤销任务有些时候，我们可能想要撤销一些已经发送出去的任务，这个时候 revoke 函数就派上用场了： 123456&gt;&gt;&gt; from celery import current_app as app&gt;&gt;&gt; res = add.s(2, 2).apply_async(countdown=30)&gt;&gt;&gt; res.revoke()&gt;&gt;&gt; app.control.revoke(res.id) 当我们调用 revoke 之后，Celery 会向 worker 发送一条消息，告知 worker 某个 ID 的任务执行请求已经被撤销了，当 worker 取出相应 ID 的请求后，就会放弃执行这条请求。但是如果在我们调用 revoke 之前，任务已经被执行了，那么这次调用相当于是无效的。但是，Celery 还给了我们另一个选择，我们能够终止正在执行某个任务的 worker 子进程： 12345&gt;&gt;&gt; app.control.revoke('d9078da5-9915-40a0-bfa1-392c7bde42ed',... terminate=True)&gt;&gt;&gt; app.control.revoke('d9078da5-9915-40a0-bfa1-392c7bde42ed',... terminate=True, signal='SIGKILL') # 自定义向 worker 子进程发送的信号 注意！上述操作是十分危险的！执行到一半的任务被强制中断可能会导致系统中出现数据不一致的情况。 Celery 提供的远程控制能力不只上面提到的这些，感兴趣的读者可以移步远程控制用户指南。 注意事项在笔者的日常使用中发现，Celery 的 worker 在 prefork 模式下子进程存在内存泄漏的问题，并且在 github 上也有相关的一些 issue，如果读者们也遇到了 worker 子进程内存泄漏的问题的话，不妨考虑在启动 worker 进程时加上下面两个配置： max task per child max memory per child 如果你配置了 max task per child，那么 worker 进程会在任意一个 worker 子进程执行的任务达到一定数量后用一个新的进程来替换该子进程，若配置了 max memory per child，则子进程会在占用内存达到一定的值后被新的进程替换掉。 使用这两个选项来缓解 Celery worker 内存占用过高的问题，从而避免 worker 因内存占用过多被操作系统强制杀死。 总结我们首先了解了如何启动和配置 Celery，在有了 Celery 的核心 app 之后，我们又学习了如何定义和调用任务。而通过任务签名我们能够将一些简单的任务进行连接和编排而形成一个任务流。其次，Celery 本身也提供了一些周边的功能，如周期任务，探测器和远程控制等。希望各位在读完本文后对 Celery 有了更加深刻的理解，并在日常使用中能够玩的开心。 总的来说，Celery 虽然不是一个完美的异步任务队列，但它一定是一个值得信赖的异步任务队列。","categories":[{"name":"Python Daily","slug":"Python-Daily","permalink":"https://homholueng.github.io/categories/Python-Daily/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"},{"name":"Celery","slug":"Celery","permalink":"https://homholueng.github.io/tags/Celery/"}]},{"title":"celery 与 django 的致命组合","slug":"celery-django-drug","date":"2018-11-04T04:44:47.000Z","updated":"2019-04-20T07:30:41.772Z","comments":true,"path":"2018/11/04/celery-django-drug/","link":"","permalink":"https://homholueng.github.io/2018/11/04/celery-django-drug/","excerpt":"","text":"1. 背景说明​ 笔者在某个项目中中采用了 event-driven 的架构，并使用 Celery 来作为架构模式中的 worker 角色。Celery 在收到 master 发出的任务执行信号后，即会从数据库中读取任务信息并开始执行，于此同时，在执行过程中还会将必要的信息持久化到数据库中，而在这整个流程中，所有的数据库操作都通过 Django orm 框架来完成。 笔者的开发环境如下： mac OS 10.13.4 Python 2.7.9 Django 1.8.11 Celery 3.1.18 Django-celery 3.1.16 Mysql 5.7.22 2. 出现问题​ Celery worker 在执行的过程中，会偶发性的出现一些由 PyMySQL 抛出的异常，如（出现过的错误包括但不仅限于以下错误）： error: Error -3 while decompressing data: incorrect header check：读取数据库中数据出错 OperationalError: (2006, &quot;MySQL server has gone away (error(32, &#39;Broken pipe&#39;))&quot;)：数据库连接中断报错 error: Command Out Of Sync：多条进程占用同一条链接出错 …. ​ 通过多次测试和排查，发现问题是由于多个 Celery worker 使用了同一个数据库连接而导致的，由于多个 worker 使用了同一条连接，所以在进行数据库读写的时候，就很有可能会因为读到了由另一个 Worker 写入的脏数据或者是因为事务隔离而导致的数据不一致而引发异常，部分不可恢复的异常抛出后，由于连接被断开，就会导致其他 worker 在使用同一条链接时引发 MySQL server has gone away 的错误，而且由于并行程序存调度的不确定性，导致很难精确的复现某个错误，使得问题的根源具有一定的隐蔽性。那么，究竟是什么原因导致多个 worker 使用了同一条数据库连接呢？ 3. 追根溯源3.1 Celery 多进程（prefork）模式 By default multiprocessing is used to perform concurrent execution of tasks, but you can also use Eventlet. Celery 的官方文档中有说明，Celery 默认使用 multiprocessing 库来实现任务的并行，而 multiprocessing 是通过 fork 来实现进程的创建操作的，那么 Celery 的 worker 进程会不会都是由一个父进程通过 fork 创建出来的呢？ 在 celery worker 启动后，通过 ps 查看子进程的状态，结果如下: 123456$ ps -ef | grep celery501 46229 54985 0 4:50下午 ttys004 0:01.41 python manage.py celery worker -c 4501 46251 46229 0 4:50下午 ttys004 0:00.02 python manage.py celery worker -c 4501 46252 46229 0 4:50下午 ttys004 0:00.02 python manage.py celery worker -c 4501 46253 46229 0 4:50下午 ttys004 0:00.02 python manage.py celery worker -c 4501 46254 46229 0 4:50下午 ttys004 0:00.02 python manage.py celery worker -c 4 通过进程状态查看工具，可以发现进程 46251，46252，46253 ，46254 都是由进程 46299 fork 出来的，而在 fork 操作时子进程会复制父进程内存空间中的所有数据，所以在这个时候，所有的子进程都拿到了父进程中的数据库连接，当多个子进程操作该连接时，就会引发上述问题。 至于为什么子进程会拿到和父进程一样的连接，还得从 Django ORM 连接管理部分的实现说起。 3.2 Django ORM 连接管理既然问题是由于共享数据库连接导致的，Django ORM 中数据库连接管理的实现就是问题的根源所在。 首先，Django ORM 中所有的数据库操作都会通过 django.db 模块下的 connections 全局变量来获取特定数据库的连接，源码如下： 123456789# django/db/models/sql/query.pyfrom django.db import connectionsdef get_compiler(self, using=None, connection=None): if using is None and connection is None: raise ValueError(\"Need either using or connection\") if using: connection = connections[using] # 关注点，在这里获取数据库连接 return connection.ops.compiler(self.compiler)(self, connection, using) 而connections 声明的源码如下： 1234567# django/db/__init__.pyfrom django.db.utils import ConnectionHandler# ...connections = ConnectionHandler()# ... 原来 connections 这个全局变量是一个 ConnectionHandler 类，那么这个 ConnectionHandler 又是何方神圣呢，ConnectionHandler 实现部分源码如下： 1234567891011121314151617181920212223242526272829# django/db/utils.pyfrom threading import localclass ConnectionHandler(object): def __init__(self, databases=None): self._databases = databases self._connections = local() # django 通过 ThreadLocal 来管理 [线程 -&gt; 数据库连接] 的映射关系 # .... def __getitem__(self, alias): # 关注点 if hasattr(self._connections, alias): return getattr(self._connections, alias) self.ensure_defaults(alias) self.prepare_test_settings(alias) db = self.databases[alias] backend = load_backend(db['ENGINE']) # 加载特定的数据库后端 conn = backend.DatabaseWrapper(db, alias) # 初始化 DatabaseWrapper setattr(self._connections, alias, conn) # 存到 ThreadLocal 中 return conn def __setitem__(self, key, value): setattr(self._connections, key, value) def __delitem__(self, key): delattr(self._connections, key) # ... 当我们通过 connections[usings] 获取数据库连接时，就会调用 ConnectionHandler 的 __getitem__() 方法，此时 ConnectionHandler 会先通过我们在 settings.py 中的配置的数据库信息加载相应的数据库后端，同时初始化该后端实现的 DatabaseWrapper 对象并返回。 DatabaseWrapper 是 Django 用于表示数据库连接的一个包装类，由数据库后端自行实现，并且该必须继承自 BaseDatabaseWrapper，该类的部分实现如下： 1234567891011121314151617181920212223242526272829303132333435# django/db/backends/base/base.pyclass BaseDatabaseWrapper(object): \"\"\" Represents a database connection. \"\"\" # ... def __init__(self, settings_dict, alias=DEFAULT_DB_ALIAS, allow_thread_sharing=False): self.connection = None # 真正的数据库连接对象 self.close_at = None # 关注点，连接到期时间，下文会有详细讲解 self.allow_thread_sharing = allow_thread_sharing # ... def get_connection_params(self): raise NotImplementedError('subclasses of BaseDatabaseWrapper may require a get_connection_params() method') def get_new_connection(self, conn_params): raise NotImplementedError('subclasses of BaseDatabaseWrapper may require a get_new_connection() method') # ... def ensure_connection(self): # 在每次使用之前都会确保当前连接是否有效 if self.connection is None: with self.wrap_database_errors: self.connect() def connect(self): conn_params = self.get_connection_params() self.connection = self.get_new_connection(conn_params) # 在这里真正建立数据库连接 # ... 绕来绕去，笔者终于找到了真正的数据库连接的藏身之处，Django 通过 BaseDatabaseWrapper 来实现对单个数据库连接的封装和管理，而获取数据库连接的方法则交给子类来实现。笔者本地的数据库后端配置如下： 1234567891011DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': '', 'USER': '', 'PASSWORD': '', 'HOST': 'localhost', 'PORT': '3306', 'CONN_MAX_AGE': 3600, &#125;,&#125; 顺着声明，找到相应后端的 DatabaseWrapper 的实现： 123456789101112131415# django/db/backends/mysqltry: import MySQLdb as Databaseexcept ImportError as e: from django.core.exceptions import ImproperlyConfigured raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e)class DatabaseWrapper(BaseDatabaseWrapper): def get_new_connection(self, conn_params): conn = Database.connect(**conn_params) # 真正的数据库连接在此处建立 conn.encoders[SafeText] = conn.encoders[six.text_type] conn.encoders[SafeBytes] = conn.encoders[bytes] return conn 至此，Django ORM 中对数据库连接的管理已经非常清晰了：Django 通过 ConnectionHandler 来管理整个 Django APP 中的所有数据库连接，并且使用 ThreadLocal 来隔离不同线程所能获取到的数据库连接，同时，通过 DatabaseWrapper 封装了对真正的数据库连接的操作，整个结构关系如下图所示： 3.3 验证猜想在了解了 Django ORM 对数据库连接的管理方式之后，我们就能够来探讨导致多个 worker 共享同一个数据库连接的真正原因了。为了验证不同的子进程确实使用了同一个数据库连接，笔者监听了 Celery worker 进程初始化的信号，并在 worker 进程初始化完成后查看当前进程的相关信息 12345678910111213141516171819202122232425262728293031import logging import threading import osfrom celery import signalsfrom django.db import connections@signals.worker_process_init.connectdef worker_init(**kwargs): import threading import os for conn in connections.all(): # 这里会读取 settings 中所有数据库配置对应的连接 logger.error( '\\n &lt;pid&gt;: %s\\n ' '---------------------------------- \\n ' '&lt;connection&gt;: %s\\n ' '&lt;wrapper&gt;: %s\\n ' '---------------------------------- \\n ' '&lt;thread&gt;: %s\\n ' '&lt;local&gt;: %s\\n ' '&lt;handler&gt;: %s\\n ' '&lt;parent pid&gt;: %s \\n ' '&lt;allow_thread_sharing&gt;: %s\\n' % ( os.getpid(), # 获取当前进程 ID conn.connection, # 获取 DatabaseWrapper 中存储的真正的数据库连接对象 conn, # 当前取得的 DatabaseWrapper threading.currentThread(), threading.local(), connections, # 当前取得的 ConnectionHandler 对象 os.getppid(), # 获取父进程的 ID conn.allow_thread_sharing)) # 查看当前 DatabaseWrapper 是否是线程安全的 ConnectionHandler 的 all() 方法实现如下： 12345678910111213141516171819202122232425# django/db/utils.pyDEFAULT_DB_ALIAS = 'default'class ConnectionHandler(object): @cached_property def databases(self): if self._databases is None: self._databases = settings.DATABASES if self._databases == &#123;&#125;: self._databases = &#123; DEFAULT_DB_ALIAS: &#123; 'ENGINE': 'django.db.backends.dummy', &#125;, &#125; if DEFAULT_DB_ALIAS not in self._databases: raise ImproperlyConfigured(\"You must define a '%s' database\" % DEFAULT_DB_ALIAS) return self._databases def __iter__(self): return iter(self.databases) def all(self): return [self[alias] for alias in self] 输出如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[2018-07-20 09:50:05,601: ERROR/Worker-3] &lt;pid&gt;: 41293 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x107b6e350&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10739d250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x107df54d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x1061edd50&gt; &lt;parent pid&gt;: 41281 &lt;allow_thread_sharing&gt;: False[2018-07-20 09:50:05,601: ERROR/Worker-2] &lt;pid&gt;: 41292 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x107b6e350&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10739d250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x107df44d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x1061edd50&gt; &lt;parent pid&gt;: 41281 &lt;allow_thread_sharing&gt;: False[2018-07-20 09:50:05,604: ERROR/Worker-1] &lt;pid&gt;: 41291 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x107b6e350&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10739d250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x107df44d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x1061edd50&gt; &lt;parent pid&gt;: 41281 &lt;allow_thread_sharing&gt;: False[2018-07-20 09:50:05,604: ERROR/Worker-4] &lt;pid&gt;: 41294 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x107b6e350&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10739d250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x107df54d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x1061edd50&gt; &lt;parent pid&gt;: 41281 &lt;allow_thread_sharing&gt;: False 可以看到，4个 worker 进程 41291, 41292, 41293, 41294 被启动了，随后通过 lsof 查看这些进程打开的 mysql 连接： 12345$ lsof -p 41291,41292,41293,41294 | grep :mysqlpython2.7 41291 lianghonghao 10u IPv6 0xb37e546feaec2979 0t0 TCP localhost:62502-&gt;localhost:mysql (ESTABLISHED)python2.7 41292 lianghonghao 10u IPv6 0xb37e546feaec2979 0t0 TCP localhost:62502-&gt;localhost:mysql (ESTABLISHED)python2.7 41293 lianghonghao 10u IPv6 0xb37e546feaec2979 0t0 TCP localhost:62502-&gt;localhost:mysql (ESTABLISHED)python2.7 41294 lianghonghao 10u IPv6 0xb37e546feaec2979 0t0 TCP localhost:62502-&gt;localhost:mysql (ESTABLISHED) 果然，四个进程同时使用了 localhost:62502-&gt;localhost:mysql 这条 TCP 连接。 如果我们在 Worker 进程初始化之后手动关闭 DatabseWrapper 中的连接，然后重新建立呢？修改代码： 1234567891011121314151617181920212223242526@signals.worker_process_init.connectdef worker_init(**kwargs): import threading import os for conn in connections.all(): conn.close() # 关闭连接 conn.connect() # 重新建立 logger.error( '\\n &lt;pid&gt;: %s\\n ' '---------------------------------- \\n ' '&lt;connection&gt;: %s\\n ' '&lt;wrapper&gt;: %s\\n ' '---------------------------------- \\n ' '&lt;thread&gt;: %s\\n ' '&lt;local&gt;: %s\\n ' '&lt;handler&gt;: %s\\n ' '&lt;parent pid&gt;: %s \\n ' '&lt;allow_thread_sharing&gt;: %s\\n' % ( os.getpid(), # 获取当前进程 ID conn.connection, # 获取 DatabaseWrapper 中存储的真正的数据库连接对象 conn, # 当前取得的 DatabaseWrapper threading.currentThread(), threading.local(), connections, # 当前取得的 ConnectionHandler 对象 os.getppid(), # 获取父进程的 ID conn.allow_thread_sharing)) # 查看当前 DatabaseWrapper 是否是线程安全的 运行程序，输出如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[2018-07-20 09:55:49,137: ERROR/Worker-2] &lt;pid&gt;: 42715 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x10fb87b50&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10f33a250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x10fd904d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x10e18ad50&gt; &lt;parent pid&gt;: 42705 &lt;allow_thread_sharing&gt;: False[2018-07-20 09:55:49,138: ERROR/Worker-3] &lt;pid&gt;: 42716 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x10fb87b50&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10f33a250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x10fd914d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x10e18ad50&gt; &lt;parent pid&gt;: 42705 &lt;allow_thread_sharing&gt;: False[2018-07-20 09:55:49,140: ERROR/Worker-1] &lt;pid&gt;: 42714 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x10fb87b50&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10f33a250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x10fd904d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x10e18ad50&gt; &lt;parent pid&gt;: 42705 &lt;allow_thread_sharing&gt;: False[2018-07-20 09:55:49,141: ERROR/Worker-4] &lt;pid&gt;: 42717 ---------------------------------- &lt;connection&gt;: &lt;pymysql.connections.Connection object at 0x10fb87b50&gt; &lt;wrapper&gt;: &lt;django.db.backends.mysql.base.DatabaseWrapper object at 0x10f33a250&gt; ---------------------------------- &lt;thread&gt;: &lt;_MainThread(MainThread, started 140735510033280)&gt; &lt;local&gt;: &lt;thread._local object at 0x10fd914d0&gt; &lt;handler&gt;: &lt;django.db.utils.ConnectionHandler object at 0x10e18ad50&gt; &lt;parent pid&gt;: 42705 &lt;allow_thread_sharing&gt;: False 查看 worker 进程打开的连接： 12345$ lsof -p 42714,42715,42716,42717 | grep :mysqlpython2.7 42714 lianghonghao 3u IPv6 0xb37e546fd9bde279 0t0 TCP localhost:62813-&gt;localhost:mysql (ESTABLISHED)python2.7 42715 lianghonghao 3u IPv6 0xb37e546feaec34f9 0t0 TCP localhost:62811-&gt;localhost:mysql (ESTABLISHED)python2.7 42716 lianghonghao 3u IPv6 0xb37e546feaec4079 0t0 TCP localhost:62812-&gt;localhost:mysql (ESTABLISHED)python2.7 42717 lianghonghao 3u IPv6 0xb37e546fd9bdff39 0t0 TCP localhost:62814-&gt;localhost:mysql (ESTABLISHED) 可以看到，在子进程初始化后断开并重新建立之后，就不会出现多个 worker 同时占用一个连接的情况了。 那么，到底是什么原因导致了子进程拿到了和父进程一样的连接呢？ 3.4 致命组合首先，普及一个知识，Django 中是没有数据库连接池的，这就意味着每次请求都需要经历一次连接建立和连接断开的过程。 在 Django 1.6 版本之后，加入了一个新的功能：persistent connections，开发者能够通过在数据库配置中设置 CONN_MAX_AGE 变量的值来指定一个数据库从建立到销毁前能够被保留的时间。至于 Django 为什么不采用数据库连接池而是使用连接持久化的方案，可以参考一下这个帖子中的讨论：Database pooling vs. persistent connections。 Django 建立每个连接时都会将该连接到期的时间设置到 DatabaseWrapper 的 close_at 属性中： 1234567891011# django/db/backends/base/base.pyclass BaseDatabaseWrapper(object): def connect(self): # ... max_age = self.settings_dict['CONN_MAX_AGE'] self.close_at = None if max_age is None else time.time() + max_age # 上文提到的连接到期时间 # ... 同时，在每次请求开始前和结束后，检测当前内存中数据库连接的可用性，并关闭过期和不可用的连接： Persistent connections avoid the overhead of re-establishing a connection to the database in each request. They’re controlled by the CONN_MAX_AGE parameter which defines the maximum lifetime of a connection. It can be set independently for each database. 实现如下： 12345678910# django/db/__init__.pyfrom django.core import signalsdef close_old_connections(**kwargs): for conn in connections.all(): conn.close_if_unusable_or_obsolete()# 监听请求开始和结束的事件，并在触发时检测连接的有效性signals.request_started.connect(close_old_connections)signals.request_finished.connect(close_old_connections) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# django/db/backends/base/base.pyclass BaseDatabaseWrapper(object): # ... def close_if_unusable_or_obsolete(self): if self.connection is not None: if self.get_autocommit() != self.settings_dict['AUTOCOMMIT']: self.close() return # 如果连接在上次使用过程中遇到了无法恢复的错误也会被关闭 # If an exception other than DataError or IntegrityError occurred # since the last commit / rollback, check if the connection works. if self.errors_occurred: if self.is_usable(): self.errors_occurred = False else: self.close() return # 重点在这里 # 如果连接没过期就不会被关闭 # persistent connections 的使用影响了这段逻辑 if self.close_at is not None and time.time() &gt;= self.close_at: self.close() return def close(self): self.validate_thread_sharing() if self.closed_in_transaction or self.connection is None: return # 关闭数据库连接并置空 try: self._close() finally: if self.in_atomic_block: self.closed_in_transaction = True self.needs_rollback = True else: self.connection = None def _close(self): if self.connection is not None: with self.wrap_database_errors: return self.connection.close() # 真正的数据库连接在这里关闭 其实这个功能的出发点是好的，但是和 multiprocessing 结合使用，就有可能导致致命的错误，通过查阅资料和阅读 Celery 和 djcelery 的源码后，笔者发现两者都在某些时刻尝试去清理子进程从父进程处获得的数据库连接以保证正常运行，如果没有使用 persistent connections 功能，就能够在子进程初始化完成后和任务开始执行前成功关闭从父进程处继承来的连接，当子进程需要进行数据库操作时，就会重新建立新的连接，也就不会出现多个进程同时使用一条连接的情况。以下为 celery 中的部分源码： 1234567891011121314151617181920212223242526272829303132333435363738# celery.fixups.django.py# ...class DjangoWorkerFixup(object): def __init__(self, app): # ... try: self._close_old_connections = symbol_by_name( 'django.db:close_old_connections', # 重点 ) except (ImportError, AttributeError): self._close_old_connections = None def install(self): # ... signals.task_prerun.connect(self.on_task_prerun) # 在任务开始前执行 signals.task_postrun.connect(self.on_task_postrun) # 在任务结束后执行 # ... self.close_database() self.close_cache() return self def on_task_prerun(self, sender, **kwargs): \"\"\"Called before every task.\"\"\" if not getattr(sender.request, 'is_eager', False): self.close_database() def close_database(self, **kwargs): if self._close_old_connections: return self._close_old_connections() # Django 1.6 # ... 以下为 djcelery 的部分源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# djcelery.loaders.pyclass DjangoLoader(BaseLoader): # ... def on_process_cleanup(self): \"\"\"Does everything necessary for Django to work in a long-living, multiprocessing environment. \"\"\" # See http://groups.google.com/group/django-users/ # browse_thread/thread/78200863d0c07c6d/ self.close_database() self.close_cache() # 在每个任务开始前执行 def on_task_init(self, task_id, task): \"\"\"Called before every task.\"\"\" try: is_eager = task.request.is_eager except AttributeError: is_eager = False if not is_eager: self.close_database() def _close_database(self): try: funs = [conn.close for conn in db.connections] except AttributeError: if hasattr(db, 'close_old_connections'): # Django 1.6+ funs = [db.close_old_connections] # 在这里获取到 django.db:close_old_connections else: funs = [db.close_connection] # pre multidb for close in funs: try: close() except DATABASE_ERRORS as exc: str_exc = str(exc) if 'closed' not in str_exc and 'not connected' not in str_exc: raise def close_database(self, **kwargs): db_reuse_max = self.conf.get('CELERY_DB_REUSE_MAX', None) if not db_reuse_max: return self._close_database() if self._db_reuse &gt;= db_reuse_max * 2: self._db_reuse = 0 self._close_database() self._db_reuse += 1 而在笔者的数据库配置中，已经设置了 CONN_MAX_AGE 为 3600（一个连接从建立成功后有 3600s 的有效期），而 djcelery 和 celery 清理都是通过调用 django.db.close_old_connections() 来清理数据库连接的，正是因为该方法在检查没有出现异常的连接时，若该连接没有过期则不会进行关闭，才导致了 djcelery 和 celery 的准备工作没有起作用，关键代码如下： 12345# django/db/backends/base/base.pyif self.close_at is not None and time.time() &gt;= self.close_at: self.close() return 所以一个完美的 BUG 就这么出现了： 父进程在进行子进程初始化工作之前调用了 Django ORM 进行数据库操作，导致父进程中的 DatabaseWrapper 中的连接被初始化。 子进程被 fork 出来后，由于完全复制了父进程的内存数据，导致所有 worker 共享了同一个 MySQL 连接（同一个 socket file）。 djcelery 和 Celery 虽然在子进程初始化和任务开始时对子进程中的连接进行了清理，但是由于 persistent connections 功能的存在，导致数据库连接没有被关闭。 当多个子进程同时使用时，极有可能原地爆炸。 3.5 结论及解决方案所以，Django 的 persistent connections 在与 multiprocessing 同时使用时，要多加注意，一定要在把父进程中的数据库连接关闭之后再创建子进程。 3.5.1 关闭 persistent connections 功能关闭 persistent connections 功能，这样在子进程初始化完成后和任务开始前会把从父进程中继承过来的连接关闭。 3.5.2 手动关闭 Django ORM 中维护的连接不关闭 persistent connections 功能，监听子进程初始化完成和任务开始的信号，并在收到信号时手动强制关闭当前进程中 Django ORM 的连接，实现如下： 1234567891011from django.db import connections@signals.task_prerun.connectdef task_prerun(**kwargs): for conn in connections.all(): conn.close() # 这里会调用 DatabaseWrapper 的 close() 方法@signals.worker_process_init.connectdef worker_init(**kwargs): for conn in connections.all(): conn.close() 3.5.4 使用其他的并发实现方式目前，只有在多进程模式下使用 Django ORM 会出现上述问题，Django 通过 ThreadLocal 来管理不同线程所对应的数据库连接，如果不使用多进程模式就不会出现内存数据相同的问题，也就不会出现共享文件描述符的问题，而 Celery 提供了若干种并发的模式：eventlet,gevent, solo, threads，在能够满足功能性需求和非功能性需求的前提下，不妨考虑其他的并发模式。 4. 后记这个 BUG 是两种框架的特性冲突而导致，但是 Django 官方文档在介绍 persistent connections 功能时并没有提醒开发者注意该事项，而出现错误后又很难定位问题，感觉在一定程度上对开发者有些不友好。 希望本次总结能够给各位读者打预防针，使得读者在日后的使用中尽量避免遇到这样的问题，或对读者在解决其他问题时提供一定的参考。","categories":[{"name":"Python Daily","slug":"Python-Daily","permalink":"https://homholueng.github.io/categories/Python-Daily/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"},{"name":"Celery","slug":"Celery","permalink":"https://homholueng.github.io/tags/Celery/"},{"name":"Django","slug":"Django","permalink":"https://homholueng.github.io/tags/Django/"}]},{"title":"Redis 各部署场景下 python client 的使用","slug":"redis-connection-with-py-client","date":"2018-11-03T04:05:37.000Z","updated":"2019-04-20T07:30:41.771Z","comments":true,"path":"2018/11/03/redis-connection-with-py-client/","link":"","permalink":"https://homholueng.github.io/2018/11/03/redis-connection-with-py-client/","excerpt":"","text":"1. Single Node1.1. 介绍 这是我们比较常用的一种部署方式，该模式下只包含一个 Redis 实例，该实例存储所有的数据，client 的所有读写操作都在该实例上完成。 1.2. 如何使用 client在该模式下，我们只需要连接该 Redis 实例并对其进行相应的操作即可： 123456789import redishost = ''port = 6379db = 0r = redis.StrictRedis(host=host, port=port, db=db)r.set('foo', 'bar') # return Truer.get('foo') # return 'bar' 2. Replication2.1. 介绍 即使我们开启了 Redis 的本地持久功能，也无法保证绝对的安全，因为数据仍有可能因为机器的物理损坏而丢失，所以，我们需要在系统中保存 Redis 数据的多个备份。Redis 的复制（Replication ）功能能够解决这个问题。 在该部署模式下，Redis 其实是以集群的方式来运行的，集群中会包含多个 Redis 实例，这些实例中只有一个是主（master）节点，其余的都是从（slave）节点，从节点会作为主节点的复制（replication），定期的将主节点中的数据复制过来进行备份。 在该模式下，需要注意下面这些问题： Redis 使用的是异步复制策略。 不仅主节点能够有从节点，从节点也能拥有自己的从节点。 复制功能既可以单纯的作为数据冗余用，也能够通过让多个从服务器处理只读命令来提升 Redis 服务的扩展性（如将繁重的 SORT 命令交给从节点去执行）。 该模式下一般会将从节点配置为只读节点，当然你也可以将从节点配置为可写节点，但因为从节点会定期从主节点中同步数据，所以写到从节点中的数据会在数据同步完成后丢失。 2.2. 如何使用 client由于该模式下一般只有主节点能够进行写操作，所以需要在应用中维护主节点和从节点的地址列表，按需操作： 123456789101112131415161718import redismaster_host = ''master_port = 6379master_db = 0slave_1_host = ''slave_1_port = 6379slave_1_db = 0mr = redis.StrictRedis(host=master_host, port=master_port, db=master_db)mr.set('foo', 'bar') # return Truemr.get('foo') # return 'bar'sr = redis.StrictRedis(host=slave_1_host, port=slave_1_port, db=slave_1_db)sr.get('foo') # return 'bar'sr.set('foo', 'from slave 1') # will raise ReadOnlyError 3. Replication With Sentinel3.1. 介绍 在 Replication 的部署方式下，由于从节点对主节点的数据做了备份，那么在主节点宕机时，从节点完全有能力晋升为主节点，以保证 Redis 的可用性，但是，随之而来的问题是：一旦主节点发生了变化，那么用户端需要修改主节点的地址，同时还要告诉集群中的其他从节点去备份新的主节点的数据。 而 Redis Sentinel（哨兵） 为我们解决了这个问题，Redis Sentinel 支持自动故障转移（Automatic failover)：当集群中的主节点宕机时，Sentinel 会在其从节点中从新选举出一个主节点，并重新配置其他的从节点，使得这些从节点能够从新的主节点中复制数据，同时，Sentinel 还会通知后续要使用 Redis 的 client 新的主节点的地址。与此同时，为了保证 Sentinel 不会成为系统中的单点，你还可以在一个架构中运行多个 Sentinel 进程。 除了自动故障转移，Sentinel 还会提供以下功能： 监控（Monitoring）：Sentinel 会定时检测主节点和从节点的运行状态。 通知（Notification）：Sentinel 能够在其检测的 Redis 实例出现错误时通过 API 来通知系统管理员或是其他程序。 配置提供（Configuration provider）：Sentinel 能够为 client 提供服务发现的功能，client 能够从 Sentinel 处获取某个服务当前所对应的主节点的地址，并且 Sentinel 会在自动故障转移发生后返回新的主节点的地址。 需要注意的是，因为 Redis 使用的是异步复制的策略，所以 Redis 集群无法保证数据的强一致性。考虑以下场景： 假设一个 Redis 集群中包含三个主节点 A，B，C，且每个主节点都拥有一个从节点 A1，B1，C1。 你向 master B 中写入了一条数据。 master B 响应了你并告诉你已经写成功了。 master B 将这次写操作广播到其所有的从节点 B1 B2 B3…. 若 master B 在第二步完成后和第三步开始前宕机了，那么其从节点 B1 就会被推举为主节点，但是你对 master B 的这次写入操作就会永远丢失。 至于 Redis 为什么不采用同步复制的策略，主要原因还是处于性能考虑。 3.2. 如何使用 client由于 Sentinel 提供了服务发现的功能，我们能够向 Sentinel 获取当前 Redis 集群中某个服务的主节点和从节点的信息，并进行相应的操作： 12345678910111213import redisfrom redis.sentinel import Sentinelsentinel_host = ''sentinel_port = 16379rs = Sentinel([(sentinel_host, sentinel_port), ], password='') # 在此传入 Redis 配置的密码mr = rs.master_for(service_name='myservice') # 获取 myservice 服务的 master 节点，服务名在 Redis 集群配置中设置；该方法默认返回 StrictRedis 实例mr.set('foo', 'bar') # return Truemr.get('foo') # return 'bar'sr = rs.slave_for(service_name='myservice')mr.get('foo') # return 'bar' 4. Data Sharding4.1. 介绍 在上面介绍的几种模式中，都只有一个 Redis 主节点，而应用的写操作又都是由主节点来完成的，这就会导致主节点的写能力和存储能力受到单机性能的限制。 而 Redis 的数据分片（data sharding）功能能够为我们解决这个问题，当我们开启的数据分片后，集群中会包含多个主节点，我们写入 Redis 中的数据会被分配并存储到不同的主节点中，一个 Redis 集群会包含 16384 个哈希槽（hash slot），集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽。 假设一个 Redis 集群中包含三个节点：A，B，C，那么： 节点 A 会管理 0 - 5500 号哈希槽 节点 B 会管理 5501 - 11000 号哈希槽 节点 C 会管理 11001 - 16383 号哈希槽 需要注意的是，Redis 集群中的节点并不能代理命令请求，若我们向集群中的某个实例请求读取或写入一个不属于该实例所管理的键，那么其会返回 -MOVED 或 -ASK 重定向错误，同时，负责管理该键的节点地址和端口的信息会一并返回。 Redis 官方的建议是客户端应该记录每个节点所管理的槽的信息，当集群处于稳定状态时， 所有客户端最终都会保存有一个哈希槽至节点的映射记录， 使得集群非常高效。 4.2. 如何使用 client由于 Redis 集群中的节点不能为我们代理命令请求，而 Python 中的 redis 包并不会处理，这就意味着我们需要在应用中自己维护节点所管理的哈希槽信息。但是好在有第三方的包能够处理这个问题： redis-py-cluster 会帮我们管理节点信息，我们只需要像使用普通的 client 一样使用它就好。 注意：在这种情况下我们不需要再通过连接 sentinel 来获取集群中的节点信息, redis-py-cluster 会自动帮我们发现集群中的节点。 12345678import redis-py-clusterstartup_nodes = [&#123;\"host\": \"127.0.0.1\", \"port\": \"7000\"&#125;] # 集群中的任一非 sentinel 节点rc = StrictRedisCluster(startup_nodes=startup_nodes, password='pwd')rc.set(\"foo\", \"bar\") # Truerc.get(\"foo\") # 'bar' 5.总结总的来说，Redis 的部署方式及其特点如下： 部署方式 数据备份 数据分片 高可用 py-cli module 备注 Single Node × × × redis 可开启本地数据持久 Replication √ × × redis Replication with Sentinel √ × √ redis Sharding (with Replication) √ √ × redis-py-cluster 通常都会与 Replication 一起使用 Sharding with Sentinel (with Replication) √ √ √ redis-py-cluster 通常都会与 Replication 一起使用","categories":[{"name":"Python Daily","slug":"Python-Daily","permalink":"https://homholueng.github.io/categories/Python-Daily/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://homholueng.github.io/tags/Redis/"},{"name":"Python","slug":"Python","permalink":"https://homholueng.github.io/tags/Python/"}]}]}